\section{Montague Semantics II}
\index{Montague Semantics}%%
\label{kap6-2}
%
%
%
This section deals with the problem of providing a language with a 
compositional semantics. The problem is to say, which languages that 
are weakly context free are also strongly context free. The principal 
result of this section is that if a language is strongly context free, 
it can be given a compositional interpretation based on an AB--grammar. 
Recall that there are three kinds of languages: languages as sets of 
strings, interpreted languages, and finally, systems of signs. A linear 
system of signs is a subset of $A^{\ast} \times C \times M$, where $C$ 
is a set of categories.
%%%
\begin{defn}
\label{defn:cfsg}
%%%
\index{sign grammar!context free}%%
\index{sign system!context free}%%
\index{sign grammar!quasi context free}%%
%%%
A linear sign grammar is \textbf{context free} if (a) $C$
is finite, (b) if $f$ is a mode of arity $n > 0$ then 
$f^{\varepsilon}(x_0,\dotsc,x_{n-1}) := \prod_{i< n} x_i$, 
(c) $f^{\mu}(m_0, \dotsc, m_{n-1})$ is defined if there exist
derivable signs $\sigma_i= \auf e_i, c_i, m_i\zu$, $i < n$, such
that $f^{\gamma}(\vec{c})$ is defined and (d) if 
$f \neq g$ then $f^{\gamma} \neq g^{\gamma}$. If only (a) --- 
(c) are satisfied, the grammar is \textbf{quasi context free}. 
$\Sigma$ is (\textbf{quasi}) \textbf{context free} if it is 
generated by a (quasi) context free linear sign grammar.
\end{defn}
%%%
This definition is somewhat involved. (a) says that if $f$
is an $n$--ary mode, $f^{\gamma}$ can be represented by a list of
$n$--ary immediate dominance rules. It conjunction with (b)
we get that we have a finite list of context free rules. Condition
(c) says that the semantics does not add any complexity to this
by introducing partiality. Finally, (d) ensures that the rules
of the CFG uniquely define the modes. (For we could in principle 
have two modes which reduce to the same phrase structure rule.) 
The reader may verify the following simple fact.
%%
\begin{prop}
Suppose that $\Sigma$ is a context free linear system of signs.
Then the string language of $\Sigma$ is context free.
\end{prop}
%%%
An interpreted string language is a subset $\CI$ of $A^{\ast} 
\times M$ where $M$ is the set of (possible) (sentence) meanings. 
The corresponding string language is $S(\CI)$. An interpreted 
language is \textbf{weakly context free} iff the string language 
is. 
%%%
\index{interpreted language!strongly context free}%%
\index{interpreted language!weakly context free}%%
%%%%
\begin{defn}
An interpreted language $\CI$ is \textbf{strongly context free}
if there is a context free linear system of signs $\Sigma$ and
a category {\tt S} such that $\mbox{\tt S}(\Sigma) = \CI$.
\end{defn}
%%%
For example, let $L$ be the set of declarative sentences of English.
$M$ is arbitrary. We take the meanings of declarative sentences to be
truth--values, here $0$ or $1$ (but see Section~\ref{kap6}.\ref{kap4x7}). 
A somewhat more refined 
approach is to let the meanings be functions from contexts to truth 
values. Next, we shall also specify what it means for a system of
signs to be context free.

Obviously, a linear context free system of signs defines a strongly
context free interpreted language. The converse does not hold,
however. A counterexample is provided by the following grammar,
which generates simple equality statements.
%%
\begin{equation}
\begin{split}
\mbox{\tt E} & \pf \mbox{\tt C=C} \\
\mbox{\tt C} & \pf \mbox{\tt D} \mid \mbox{\tt D+D} \\
\mbox{\tt D} & \pf \mbox{\tt 1} \mid \mbox{\tt 2}
\end{split}
\end{equation}
%%
Expressions of category {\tt E} are called equations, and they have
as their meaning either $\mathsf{true}$ or $\mathsf{false}$. Now, assign 
the following meanings to the strings.  {\tt 1} has as its {\tt D}-- 
and {\tt C}--meaning the number 1, {\tt 2} the number 2, and {\tt 1+1} 
as its {\tt C}--meaning the number 2. The {\tt E}--meanings are as follows.
%%
\newcommand{\sE}{\mbox{\smtt E}}
\begin{equation}
\begin{array}{ll@{\qquad}ll}
\mbox{}[\mbox{\tt 2=2}]^{\sE} & = \{\mathsf{true}\} 
	& [\mbox{\tt 1+1=2}]^{\sE} & = \{\mathsf{false}\} \\
\mbox{}[\mbox{\tt 1=2}]^{\sE} & = \{\mathsf{false}\} 
	& [\mbox{\tt 1=1+1}]^{\sE} & = \{\mathsf{true}\} \\
\mbox{}[\mbox{\tt 2=1}]^{\sE} & = \{\mathsf{false}\}
	& [\mbox{\tt 1+1=1}]^{\sE} & = \{\mathsf{true}\} \\
\mbox{}[\mbox{\tt 1=1}]^{\sE} & = \{\mathsf{true}\} 
	& [\mbox{\tt 2=1+1}]^{\sE} & = \{\mathsf{false}\} \\
                   &          & [\mbox{\tt 1+1=1+1}]^E & = \{\mathsf{true}\}
\end{array}
\end{equation}
%%
This grammar is unambiguous; and every string of category $X$ has 
exactly one $X$--meaning for $X \in \{\mbox{\tt C}, \mbox{\tt D}, 
\mbox{\tt E}\}$. Yet, there is no CFG of signs for this language. 
For the string {\tt 1+1} has the same {\tt T}--meaning as {\tt 2}, 
while substituting one for the other in an equation changes the 
truth value.

We shall show below that `weakly context free' and `strongly context 
free' coincide for interpreted languages. This means that the notion 
of an interpreted language is not a very useful one, since adding 
meanings to sentences does not help in establishing the structure of 
sentences. The idea of the proof is very simple. Consider an 
arbitrary linear sign grammar $\GA$ and a start symbol 
{\tt S}. We replace {\tt S} throughout by $\mbox{\tt S}^{\circ}$, 
where $\mbox{\tt S}^{\circ} \not\in C$. Now replace the algebra of 
meanings by the partial algebra of definite structure terms. This 
defines $\GB$.  Then for $c \in C - \{\mbox{\tt S}\}$, 
$\auf \vec{x}, c, \Gs\zu$ is a sign generated by $\GB$ iff 
$\Gs$ is a definite structure term such that $\Gs^{\varepsilon} = 
\vec{x}$ and $\Gs^{\gamma} = c$; 
and $\auf \vec{x}, \mbox{\tt S}^{\circ}, \Gs\zu$ is generated 
by $\GB$ iff $\Gs$ is a definite structure term such that 
$\Gs^{\varepsilon} = \vec{x}$, and $\Gs^{\gamma} = \mbox{\tt S}$. 
Finally, we introduce the following unary mode {\tt F}.
%%%
\begin{equation}
\mbox{\tt F}(\auf \vec{x}, \mbox{\tt S}^{\circ}, \Gs\zu) := 
	\auf \vec{x}, \mbox{\tt S}, \Gs^{\mu}\zu
\end{equation}
%%%
This new grammar, call it $\GA^{\circ}$, defines the same interpreted 
language with respect to {\tt S}. So, if an interpreted language is 
strongly context free, it has a context free sign grammar of this 
type. Now, suppose that the interpreted language $\CI$ is weakly 
context free. So there is a CFG $G$ generating $\mbox{\tt S}(\CI)$. 
At the first step we take the trivial semantics: everything is mapped 
to $0$. This is a strongly context free sign system, and 
we can perform the construction above. This yields a context free 
sign system where each $\vec{x}$ has as its $C$--denotations the 
set of structure terms that define a $C$--constituent with string 
$\vec{x}$. Finally, we have to deal with the semantics. Let $\vec{x}$ 
be an {\tt S}--string and let $|M_{\vec{x}}|$ be the set of meanings 
of $\vec{x}$ and $\mbox{\tt S}_{\vec{x}}$ the set of structure terms for 
$\vec{x}$ as an {\tt S}. If $|\mbox{\tt S}_{\vec{x}}| < |M_{\vec{x}}|$, 
there is no grammar for this language based on $G$. If, however, 
$|\mbox{\tt S}_{\vec{x}}| \geq |M_{\vec{x}}|$ there is a function 
$f_{\vec{x}} \colon \mbox{\tt S}_{\vec{x}} \pf M_{\vec{x}}$. Finally, 
put 
%%
\begin{equation}
\begin{split}
f^{\ast} & := \bigcup \auf f_{\vec{x}} : \vec{x} \in \mbox{\tt S}(\GI)\zu \\
\mbox{\tt F}^{\ast}(\auf \vec{x}, \mbox{\tt S}^{\circ}, \Gs\zu) & := 
	\auf \vec{x}, \mbox{\tt S}, f^{\ast}(\Gs)\zu
\end{split}
\end{equation}
%%%
This defines the sign grammar $\GA^{\ast}$. It is context free 
and its interpreted language with respect to the symbol {\tt S} is 
exactly $\CI$.
%%%
\begin{thm}
\label{thm:schwachstark}
Let $\CI$ be a countable interpreted language.
%%%
\begin{dingautolist}{192}
\item
If the string language of $\CI$ is context free then $\CI$ is
strongly context free.
\item
For every CFG $G$ for $\mbox{\tt S}(\CI)$ there exists a
context free system of signs $\Sigma$ and with a category 
{\tt S} such that
%%%%
\begin{enumerate}
\item $\mbox{\tt S}(\Sigma) = \CI$,
\item for every nonterminal symbol $A$ 
	$$\{\vec{x} : \mbox{ for some } 
    m \in M: \auf \vec{x}, A, m\zu \in \Sigma\} = \{\vec{x} :
    A \vdash_G \vec{x}\}$$
\end{enumerate}
\end{dingautolist}
\end{thm}
%%%
\proofbeg
\ding{193} has been established. For \ding{192} it suffices to 
observe that for every CFL there exists a CFG in which every 
sentence is infinitely ambigous. Just replace {\tt S} by 
$\mbox{\tt S}^{\bullet}$ and add the rules 
$\mbox{\tt S}^{\bullet} \pf \mbox{\tt S}^{\bullet} 
\mid \mbox{\tt S}$.
\proofend

Notice that the use of unary rules is essential. If there are no 
unary rules, a given string can have only exponentially many 
analyses. 
%%
\begin{lem}
\label{lem:cschaetz}
Let $L$ be a CFL and $d > 0$. Then there is a CFG $G$ and such 
that for all $\vec{x} \in L$ the set of nonisomorphic $G$--trees 
for $\vec{x}$ has at least $d^{|\vec{x}|}$ members.
\end{lem}
%%%
\proofbeg
Notice that it is sufficient that the result be proved for almost
all $\vec{x}$. For finitely many words we can provide as many
analyses as we wish. First of all, there is a grammar in Chomsky
normal form that generates $L$. Take two rules that can be used in
succession.
%%
\begin{equation}
A\quad \pf\quad BC, \qquad C\quad \pf\quad DE 
\end{equation}
%%
Add the rules
%%
\begin{equation}
A\quad \pf\quad  XE, \qquad X\quad\pf\quad AD
\end{equation}
%%
Then the string {\it ABC} has two analyses: $[A\; [B\; C]]$
and $[[A\; B]\; C]$. We proceed similarly if we have a pair of rules
%%
\begin{equation}
A\quad\pf\quad XE, \qquad X\quad\pf\quad AB
\end{equation}
%%
This grammar assigns exponentially many parses to a given
string. To see this notice that any given string $\vec{x}$
of length $n$ contains $n$ distinct constituents. For $n \leq 3$,
we use the `almost all' clause. Now, let $n > 3$. Then $\vec{x}$
has a decomposition $\vec{x} = \vec{y}_0\vec{y}_1\vec{y}_2$
into constituents. By inductive hypothesis, for $\vec{y}_i$
we have $d^{|\vec{y}_i|}$ many analyses. Thus $\vec{x}$ has at least
$2d^{|\vec{y}_0|}d^{|\vec{y}_1|}d^{|\vec{y}_2|} = 2d^{|\vec{x}|}$
analyses.
%%
\proofend

The previous proof actually assumes exponentially many 
different structures to a string. We can also give a simpler 
proof of this fact. Simply replace $N$ by $N \times d$ and 
replace in each rule every nonterminal $X$ by any one of the 
$\auf X, k\zu$, $k < d$.
%%%
\begin{thm}
\label{thm:bound}
Let $\CI$ be a countable interpreted language. Then $\CI$ is strongly
context free for a CFG without unary rules iff $\mbox{\tt S}(\CI)$
is context free and there is some constant $c > 0$ such that for almost 
all strings of $A^{\ast}$: the number of meanings of $\vec{x}$ is 
bounded by $c^{|\vec{x}|}$.
\end{thm}
%%
We give an example.  Let $A := \{\mbox{\tt Paul}, \mbox{\tt Marcus}, 
\mbox{\tt sees}\}$ and
%%
\begin{align}
\begin{split}
L := \{ & \mbox{\tt Paul sees Paul}, \mbox{\tt Paul sees Marcus}, \\
    & \mbox{\tt Marcus sees Paul}, \mbox{\tt Marcus sees Marcus}\}
\end{split}
\end{align}
%%
We associate the following truth values to the sentences.
%%
\begin{align}
\begin{split}
\CI = \{  & \auf \mbox{\tt Paul sees Paul}, 0\zu, \\
          & \auf \mbox{\tt Paul sees Marcus}, 1\zu, \\
          & \auf \mbox{\tt Marcus sees Paul}, 0\zu, \\
          & \auf \mbox{\tt Marcus sees Marcus},1\zu\}
\end{split}
\end{align}
%%
Furthermore, we fix a CFG that generates $L$:
%%
\begin{equation}
\begin{array}{lll@{\qquad}lll}
\rho_0 & := \mbox{\tt S} & \pf \mbox{\tt NP VP} 
	& \rho_1 & := \mbox{\tt VP} & \pf \mbox{\tt V NP} \\
\rho_2 & := \mbox{\tt NP} & \pf \mbox{\tt Paul} 
	& \rho_3 & := \mbox{\tt V} & \pf \mbox{\tt sees} \\
\rho_4 & := \mbox{\tt NP} & \pf \mbox{\tt Marcus} & &  
\end{array}
\end{equation}
%%
We construct a context free system of signs $\Sigma$ with
$\mbox{\tt S}(\Sigma) = \CI$. For every rule $\rho$ of arity 
$n > 0$ we introduce a symbol $\mbox{\tt N}_{\rho}$ of arity $n$. 
In the first step the interpretation is simply given by the 
structure term. For example, 
%%
\begin{equation}
\mbox{\tt N}_{\seins}(\auf \vec{x},\mbox{\tt V},\Gs\zu, 
\auf\vec{y}, \mbox{\tt NP},\Gt\zu) = 
\auf \vec{x}\oconc\vec{y}, \mbox{\tt VP}, \mbox{\tt N}_{\seins}\conc 
\Gs\conc\Gt\zu
\end{equation}
%%
(To be exact, on the left hand side we find the unfolding of 
$\mbox{\tt N}_{\seins}$ rather than the symbol itself.) Only 
the definition of 
$\mbox{\tt N}_{\snull}^{\mu}$ is somewhat different. Notice 
that the meaning of sentences is fixed. Hence the following 
must hold. 
%%
\begin{equation}
\label{eq:46a}
\begin{split}
(\mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier})^{\mu} & = 1 \\
(\mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei})^{\mu} & = 0 \\
(\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier})^{\mu} & = 1 \\
(\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei})^{\mu} & = 0
\end{split}
\end{equation}
%%
We can now do two things: we can redefine the action of the function 
$\mbox{\tt N}_{\snull}^{\mu}$. Or we can leave the action as given and 
factor out the congruence defined by \eqref{eq:46a} in the algebra of 
the structure terms enriched by the symbols $0$ and $1$. 
If we choose the latter option, we have
%%
\begin{align}
\begin{split}
M :=  \{ & 0, 1, \mbox{\tt N}_{\szwei}, \mbox{\tt N}_{\sdrei}, 
\mbox{\tt N}_{\svier}, \mbox{\tt N}_{\seins}\mbox{\tt N}_{\sdrei}%
\mbox{\tt N}_{\svier},  
\mbox{\tt N}_{\seins}\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei},  
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier}, 
\\ 
	&
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei}, 
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier},
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei}\}
\end{split}
\end{align}
%%
Now let $\Theta$ be the congruence defined by \eqref{eq:46a}.
%%
\begin{align}
\begin{split}
M/\Theta :=  \{ & \{0, 
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei},  
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei}\},  
\{\mbox{\tt N}_{\szwei}\}, \{\mbox{\tt N}_{\sdrei}\}, 
\\
&
\{\mbox{\tt N}_{\svier}\}, 
\{1, \mbox{\tt N}_{\snull}\mbox{\tt N}_{\svier}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier}, 
\mbox{\tt N}_{\snull}\mbox{\tt N}_{\szwei}\mbox{\tt N}_{\seins}%
\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier}\}, 
\\ 
&
\{\mbox{\tt N}_{\seins}\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\svier}\},  
\{\mbox{\tt N}_{\seins}\mbox{\tt N}_{\sdrei}\mbox{\tt N}_{\szwei}\}
\}
\end{split}
\end{align}

Next we define the action on the categories. Let 
$\rho = B \pf A_0\dotsb A_{n-1}$. Then
%%
\begin{equation}
\mbox{\tt N}_{\rho}^{\gamma}(\gamma_0, \dotsc, \gamma_n)
    :=
    \begin{cases}
    B & \text{ if for all $i < n: \gamma_i = A_i$,} \\
        \star & \text{ otherwise.}
    \end{cases}
\end{equation}
%%
The functions on the exponents are fixed. 

Let us pause here and look at the problem of reversibility. Say 
that $f \colon S \pf \wp(T)$ is \textbf{finite} 
%%%%
\index{function!finite}%%
\index{function!bounded}%%
%%%%
if $|f(x)| < \omega$ for every $x \in S$. Likewise, $f$ is 
\textbf{bounded} if there is a number $k < \omega$ such that 
$|f(x)| < k$ for all $x$.
%%%
\begin{defn}
%%%
\label{defn:reverse}
\index{interpreted language!finitely reversible}%%
\index{interpreted language!boundedly reversible}%%
\index{$x^{\GI}$, $x_{\GI}$}%%
%%%
Let $\CI \subseteq E \times M$ be an interpreted language. 
$\CI$ is \textbf{finitely reversible} (\textbf{boundedly 
reversible}) if for every $x \in E$, $x^{\GI} := \{y : 
\auf x,y\zu \in \CI\}$ is finite (bounded), and for every 
$y \in M$, $y_{\GI} := \{x : \auf x,y\zu \in \CI\}$ is finite 
(bounded), and moreover, the functions $x \mapsto x^{\GI}$ and 
$y \mapsto y_{\GI}$ are computable.
\end{defn}
%%
The conditions on $x^{\GI}$ are independent 
from the conditions on $x_{\GI}$ 
(see \cite{dymetman:reversible,dymetman:thesis}).
%%
\begin{thm}[Dymetman]
There are interpreted languages $\CI$ and $\GK$ such that 
(a) $x \mapsto x^{\GI}$ is finite and computable but 
$y \mapsto y_{\GI}$ is not, 
(b) $y \mapsto y_{\GK}$ is finite and computable but 
$x \mapsto x^{\GI}$ is not.
\end{thm}
%%%
In the present context, the problem of enumerating the analyses 
is trivial. So, given a context free sign grammar, the function 
$\vec{x} \mapsto \vec{x}^{\GI}$ is always computable, although 
not always finite. If we insist on branching, $\vec{x}^{\GI}$ 
grows at most exponentially in the length of $\vec{x}$. We have 
established nothing about the maps $m \mapsto m_{\GI}$.

Let us now be given a sign system $\Sigma$ whose projection to 
$E \times C$ is context free. What conditions must be impose 
so that there exists a context free sign grammar for $\Sigma$?
Let $\vec{x} \in A^{\ast}$
%%%
\index{$[\vec{x}]_{\Sigma}^A$}%%
\index{$A$--meaning}%%
\index{meaning!$A$--\faul}%%
%%%
and $A \in C$. We write $[\vec{x}]_{\Sigma}^A :=
\{m : \auf \vec{x}, A, m\zu \in \Sigma\}$ and call this set
the set of $A$--\textbf{meanings of} $\vec{x}$ \textbf{in} $\Sigma$.
If $\Sigma$ is given by the context, we omit it. If a system of
has a CFG $\GA$ then it satisfies the following equations for 
every $A$ and every $\vec{x}$.
%%
\begin{equation}
\label{eq:numbmean}
\bigcup_{Z \in F} Z^{\mu}[[\vec{x}_0]^{B_0}\times \dotsb\times
    [\vec{x}_{\Omega(Z)-1}]^{B_{\Omega(Z)-1}}] =
    [\vec{x}_0\conc \dotsb \conc \vec{x}_{\Omega(Z)-1}]^A
\end{equation}
%%
where $Z^{\tau}(B_0, \dotsb, B_{n-1}) = A$. This means simply
put that the $A$--meanings of $\vec{x}$ can be computed
directly from the meanings of the immediate subconstituents.
From \eqref{eq:numbmean} we can derive the following estimate.
%%
\begin{equation}
\label{eq:nn2}
\sum_{Z \in F} \left(\prod_{i < \Omega(Z)} |[\vec{x}_i]^{B_i}|\right)
    \geq
    |[\vec{x}_0\conc \dotsb \conc \vec{x}_{\Omega(Z)-1}]^A|
\end{equation}
%%
This means that a string cannot have more meanings than it has 
readings (= structure terms). We call the condition \eqref{eq:nn2}
the \textbf{count condition}. In particular, it implies that 
there is a constant $c$ such that for almost all $\vec{x}$ and all 
$A$:
%%
\begin{equation}
|[\vec{x}]^A| \leq c^{|\vec{x}|}
\end{equation}
%%
Even if the count condition is satisfied it need not be 
possible to construct a context free system of signs. Here is an 
example. 
%%%
\begin{align}
\Sigma := & 
	 \{\auf \mbox{\tt 0}^n, \mbox{\tt T}, 0\zu : n \in \omega\} 
	\cup \{\auf \mbox{\tt a}\mbox{\tt 0}^n\mbox{\tt a}, \mbox{\tt S}, 
	n\zu : n \in \omega\}
\end{align}
%%%
A CFG for $\Sigma$ is 
%%%
\begin{align}
\mbox{\tt S} \pf & \mbox{\tt aTa} &
\mbox{\tt T} \pf & \varepsilon \mid \mbox{\tt T0} 
\end{align}
%%%
The condition \eqref{eq:numbmean} is satisfied. But there is no 
context free sign grammar for $\Sigma$. The reason is that no matter 
how the functions are defined, they must produce an infinite 
set of numbers from just one input, 0. Notice that we can even 
define a boundedly reversible system for which no CFG exists.
It consists of the signs $\auf \mbox{\tt 0}^n, \mbox{\tt T}, n\zu$, 
$n \in \omega$, the signs $\auf \mbox{\tt a0$^n$a}, \mbox{\tt S}, 2^n\zu$ 
and the signs $\auf \mbox{\tt a0$^n$a}, \mbox{\tt S}, 3^n\zu$ 
where $n$ is prime. We have $\mbox{\tt a0$^n$a}^{\GI} \subseteq 
\{2^n, 3^n\}$, whence $|\vec{x}^{\GI}| \leq 2$, and $|k_{\GI}| \leq 2$.
However, suppose we allow the functions $f^{\varepsilon}$ 
to be partial, but if defined $f^{\varepsilon}(\vec{x}_0, 
\dotsc, \vec{x}_{\Omega(f)-1}) = \prod_{i < \Omega(f)} \vec{x}_i$.
Then a partial context free grammar exists if the interpreted 
language defined by $\Sigma$ is boundedly reversible. (Basically, 
the partiality allows any degree of sensitivity to the string of 
which the expression is composed. Each meaning is represented by 
a bounded number of expressions.)

Let $\rho = A \pf B_0 \dotsb B_{n-1}$. Notice that  the function
$\mbox{\tt F}_{\rho}$ makes $A$--meanings from certain
$B_i$--meanings of the constituents of the string. However,
often linguists use their intuition to say what an $A$--string
means under a certain analysis, that is to say, structure term.
This --- as is easy to see --- is tantamount to knowing the
functions themselves, not only their domains and ranges. For
let us assume we have a function which assigns to every structure
term $\Gt$ of an $A$--string $\vec{x}$ an $A$--meaning. Then
the functions $\mbox{\tt F}_{\rho}$ are uniquely determined.
For let a derivation of $\vec{x}$ be given. This derivation
determines derivations of its immediate constituents, which
are now unique by assumption. For the tuple of meanings of
the subconstituents we know what the function does. Hence,
it is clear that for any given tuple of meanings we can
say what the function does. (Well, not quite. We do not know
what it does on meanings that are not expressed by a $B_i$--string,
$i < \Omega(f)$. However, on any account we have as much knowledge
as we need.)

Let us return to Montague Grammar. Let $\Sigma
%%%
\index{Montague Semantics}%%
%%%
\subseteq A^{\ast} \times C \times M$
be strongly context free, with $F$ the set of modes. We want to
show that there is an AB--grammar which generates
$\Sigma$. We have to precisify in what sense we want to understand
this. We cannot expect that $\Sigma$ is any context free system,
since AB--grammars are always binary branching. This, however,
means that we have to postulate other constituents than those of
$\Sigma$. Therefore we shall only aim to have the same sentence
meanings. In what way we can get more, we shall see afterwards.
To start, there is a trivial solution of our problem.
If $\rho = A \pf B_0 B_1$ is a rule we add a 0--ary mode
%%
\begin{equation}
\mbox{\tt N}_{\rho} = \auf \varepsilon, A/B_0/B_1,
\lambda x_{B_0}.\lambda x_{B_1}.F_{\rho}(x_{B_0}, x_{B_1})\zu
\end{equation}
%%
This allows us to keep our constituents. However, postulating
empty elements does have its drawbacks. It increases the costs 
of parsing, for example. We shall therefore ask whether one
can do without empty categories. This is possible. For, as we
have seen, with the help of combinators one can liberate oneself
from the straightjacket of syntactic structure. Recall from 
Section~\ref{kap2}.\ref{kap2-2} the transformation of a CFG into Greibach 
Normal Form. This uses essentially the tool of skipping a rule 
and of eliminating left recursion. We leave it to the reader to 
formulate (and prove) an analogon of the skipping of rules for 
context free sign grammars.  This allows us to concentrate on the 
elimination of left recursion. We look again at the construction of
Lemma~\ref{lem:linksrek}. Choose a nonterminal $X$.  Assume
that we have the following $X$--productions, where
$\vec{\alpha}_j$, $j < m$, and $\vec{\beta}_i$, $i < n$,
do not contain $X$.
%%
\begin{equation}
\rho_j := X \pf X\conc \vec{\alpha}_i, \quad i < m,
\qquad \sigma_i := X \pf \beta_i, \quad i < n
\end{equation}
%%
Further let $\mbox{\tt F}^{\mu}_{\rho_j}$, $j < m$, and
$\mbox{\tt F}^{\mu}_{\sigma_i}$, $i < n$, be given. To keep
the proof legible we assume that $\vec{\beta}_j = Y_j$,
$\vec{\alpha}_i = U_i$, are nonterminal symbols. (Evidently,
this can be achieved by introducing some more nonterminal
symbols.) We we have now these rules.
%%
\begin{equation}
\rho_j = X \pf X\conc U_i, \quad i < m,
\qquad \sigma_i = X \pf Y_i, \quad i < n
\end{equation}
%%
So, we generate the following structures.
%%
\begin{equation}
[Y' \; [U_{i_0}\; [U_{i_1}\dotsb [U_{i_{n-2}}\; U_{i_{n-1}}]\dotsb]]]
\end{equation}
%%
We want to replace them by these structures instead:
%%
\begin{equation}
[[\dotsb[[Y'\; U_{i_0}] \; U_{i_1}]\dotsb U_{i_{n-2}}]\;
U_{i_{n-1}}]
\end{equation}
%%
Proceed as in the proof of Lemma~\ref{lem:linksrek}. Choose a new
symbol $Z$ and replace the rules by the following ones.
%%
\begin{subequations}
\begin{align}
\lambda_j & := X \pf U_j, & j < m, &&
\nu_i & := Z \pf Y_i, & i < n, \\
\mu_j & := X \pf Y_j \conc Z, & j < m, &&
\xi   & := Z \pf Y_i \conc Z, & i < n.
\end{align}
\end{subequations}
%%
Now define the following functions.
%%
\begin{align}
\begin{split}
\mbox{\tt G}^{\mu}_{\lambda_i} & := \mbox{\tt F}^{\mu}_{\sigma_i} \\
\mbox{\tt G}^{\mu}_{\mu_i}(x_0,x_1) & := x_1(x_0) \\
\mbox{\tt G}^{\mu}_{\nu_i}(x_0,x_1) & :=
    \lambda x_2.\mbox{\tt F}^{\mu}_{\rho_i}(x_1(x_2), x_0)
    \\
\mbox{\tt G}^{\mu}_{\xi_i}(x_0) & :=
    \lambda x_0.\mbox{\tt F}^{\mu}_{\rho_i}(x_0, x_1)
\end{split}
\end{align}
%%
Now we have eliminated all left recursion on $X$. We only have to 
show that we have not changed the set of $X$--meanings for any 
string. To this end, let $\vec{x}$ be an $X$--string, say 
$\vec{x} = \vec{y} \conc \prod_{i < k} \vec{z}_i$, where $\vec{y}$ 
is a $Y_j$--string and $\vec{z}_i$ a $U_{j_i}$--string. Then in the 
transformed grammar we have the $Z$--strings
%%
\begin{equation}
\vec{u}_p := \prod_{p \leq i < k} \vec{z}_i
\end{equation}
%%
and $\vec{x}$ is an $X$--string. Now we still have to determine
the meanings. Let $\Gm$ be a meaning of $\vec{y}$ as a
$Y_i$--string and $\Gn_i$, $i < k$, a meaning of $\vec{z}_i$ as
a $U_{j_i}$--string. The meaning of $\vec{x}$ as an $X$--string
under this analysis is then
%%
\begin{equation}
\mbox{\tt F}^{\mu}_{\rho_{j_{n-1}}}(%
    \mbox{\tt F}^{\mu}_{\rho_{j_{n-2}}}(\dotsb (%
    \mbox{\tt F}^{\mu}_{\rho_{j_0}}(\Gm,\Gn_0),\Gn_1),
    \dotsc,\Gn_{n-2}),\Gn_{n-1})
\end{equation}
%%
As a $Z$--string $\vec{u}_{n-1}$ has the meaning
%%
\begin{equation}
\Gu_{n-1} :=
    \mbox{\tt G}^{\mu}_{\nu_{i_{n-1}}}(\Gn_{n-1}) =
    \lambda x_0.\mbox{\tt F}^{\mu}_{\rho_i}(x_0, \Gn_{n-1}) 
\end{equation}
%%
Then $\vec{u}_{n-2}$ has the meaning
%%
\begin{align}
\begin{split}
\Gu_{n-2} & := \mbox{\tt G}^{\mu}_{\nu_{i_{n-2}}}(\Gn_{n-2},\Gu_{n-1}) \\
    & = \lambda x_2.\mbox{\tt F}^{\mu}_{\rho_{n-2}}(\Gu_{n-1}(x_2),
        \Gn_{n-2}) \\
    & = \lambda x_2.\mbox{\tt F}^{\mu}_{\rho_{n-2}}(%
    \mbox{\tt F}^{\mu}_{\rho_i}(x_2, \Gn_{n-1}),\Gn_{n-2}).
\end{split}
\end{align}
%%
Inductively we get
%%
\begin{equation}
\Gu_{n-j} =
    \lambda x_0.\mbox{\tt F}^{\mu}_{\rho_{n-1}}(%
    \mbox{\tt F}^{\mu}_{\rho_{n-2}}(\dotsb
    \mbox{\tt F}^{\mu}_{\rho_{n-j}}(
    x_0, \Gn_{n-j}), \dotsc, \Gn_{n-2}),\Gn_{n-1})
\end{equation}
%%
If we put $j = n$, and if we apply at last the function
$\mbox{\tt F}^{\mu}_{\mu_j}$ on $\Gm$ and the result
we finally get that $\vec{x}$ has the same $X$--meaning
under this analysis. The converse shows likewise that
every $X$--analysis of $\vec{x}$ in the transformed
grammar can be transformed back into an $X$--analysis
in the old grammar, and the $X$--meanings of the two
are the same.

The reader may actually notice the analogy with the semantics
of the Geach rule. There we needed to get new constituent
structures by bracketing $[A [B\; C]]$ into $[[A\; B]\; C]$.
Supposing that $A$ and $B$ are heads, the semantics of the
rule forming $[A\; B]$ must be function composition. This is
what the definitions achieve here. Notice, however, that we
have no categorial grammar to start with, so the proof given
here is not fully analogous. Part of the semantics of the
construction is still in the modes themselves, while categorial
grammar requires that it be in the meaning of the lexical
items.

After some more steps, consisting in more recursion
elimination and skipping of rules we are finally done.
Then the grammar is in Greibach normal form. The latter
can be transformed into an AB--grammar, as we have already 
seen.
%%
\begin{thm}
Let $\Sigma$ be a context free linear system of signs.
Then there exists an AB--grammar that
generates $\Sigma$.
\end{thm}
%%
The moral to be drawn is that Montague grammmar is actually
%%%
\index{Montague Semantics}%%%
%%%
quite powerful from the point of view of semantics. If the string
languages are already context
free, then if any context free analysis succeeds, so does
an analysis in terms of Montague grammar (supposing here
that nothing except linear concatenation is allowed in the
exponents). We shall extend this result later to
\textbf{PTIME}--languages.

{\it Notes on this section.} With suitable conditions (such as 
nondeletion) the set $x^{\GI}$ becomes enumerable for every 
given $x$, simply because the number of parses of a string is 
finite (and has an a priori bound based on the length of 
$x$). Yet, $x^{\GI}$ is usually infinite (as we discussed in 
Section~\ref{kap4}.\ref{kap4-1}), and the sets $y_{\GI}$ need not be 
recursively enumerable for certain $y$. \cite{pogodalla:reseaux} 
%%%
\index{Pogodalla, Sylvain}%%%
%%%
studies how this changes for categorial grammars if semantic 
representations are not formulae but linear formulae. In that 
case, the interpreted grammar becomes reversible, and generation 
is polynomial time computable.
%%
\vplatz
\exercise
Let $G$ be a quasi context free sign grammar. Construct a 
context free sign grammar which generates the same interpreted 
language.
%%
\vplatz
\exercise
Let $G$ be determined by the two rules
$\mbox{\tt S} \pf \mbox{\tt SS} \mid \mbox{\tt a}$.
Show that the set of constituent structures of $G$ cannot be
generated by an AB--grammar. {\it Hint.} Let $d(\alpha)$  
be the number of occurrences of slashes ({\mtt{\tb}} or 
{\mtt{\tf}}) in $\alpha$. If $\alpha$ is the mother of $\beta$ 
and $\gamma$ then either $d(\beta) > d(\alpha)$ or $d(\gamma) > 
d(\alpha)$.
%%
\vplatz
\exercise
Let $\Sigma$ be strongly context free with respect to a
2--standard CFG $G$ with the following property:
there exists a $k \in \omega$ such that for every $G$--tree
$\auf T, <, \sqsubset, \ell\zu$ and every node $x \in T$ there 
is a terminal node $y \in T$ with $[y,x] \leq k$. Then there 
exists an AB--grammar for $\Sigma$ which generates the 
same constituent structures as $G$.
%%
\vplatz
\exercise
As we have seen above, left recursion can be eliminated from
a CFG $G$. Show that there exists a $\CCG(\mathsf{B})$ 
grammar which generates for every nonterminal $X$ the same set 
of $X$--strings. Derive from this that we can write an AB--grammar 
which for every $X$ generates the same $X$--strings as $G$. Why 
does it not follow that $L_B(G)$ can be generated by some AB--grammar?
{\it Hint.} For the first part of the exercise consider
Exercise~\ref{ueb:ab}.
%%
\vplatz
\exercise
%%%%
\index{$\oli{X}$--syntax}%%
%%%%
Let $\auf \mbox{\tt S}, C, A, \zeta\zu$ be an AB--grammar. Put
$\CC^0 := \bigcup_{a \in A} \zeta(a)$. These are the 0th projections.
Inductively we put for $\beta^i = \alpha/\gamma \in \CC^i$,
$\gamma \neq \alpha$, $\beta^{i+1} := \alpha$. In this way we
define the projections of the symbols from $\CC^0$. Show that
by these definitions we get a grammar which satisfies the
principles of $\oli{X}$--syntax.  {\it Remark.} The maximal
projections are not necessarily in $\CC^2$.
