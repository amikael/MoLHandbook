\section{Ambiguity, Transparency and Parsing Strategies}
\label{kap2-4}
%
%
%
In this section we will deal with the relationship between strings
and trees. As we have explained in Section~\ref{kap1}.\ref{einsvier}, 
there is a bijective correspondence between derivations in $G$ and
derivations in the corresponding graph grammar $\gamma G$.
Moreover, every derivation $\Delta = \auf A_i : i < p\zu$
of $G$ defines an exhaustively ordered tree $\GB$ with labels in
$N \cup A$ whose associated string is exactly $\vec{\alpha}_{p}$, 
where $A_{p-1} = \auf \vec{\alpha}_{p-1}, C_{p-1}, \vec{\alpha}_p\zu$.
If $\vec{\alpha}_p$ is not a terminal string, the labels of the 
leaves are also not all terminal. We call such a tree a 
\textbf{partial} $G$--\textbf{tree}.
%%%
\index{tree!partial $G$--\faul}%%%
%%
\begin{defn}
%%%
\index{constituent!$G$--\faul}%%%
\index{constituent!accidental}%%
%%%
Let $G$ be a CFG. $\vec{\alpha}$ is called a
$G$--\textbf{constituent of category} $A$ if $A \vdash_G \vec{\alpha}$.
Let $\GB$ be a $G$--tree with associated string $\vec{x}$ and
$\vec{y}$ a substring of $\vec{x}$. Assume further that $\vec{y}$
is a $G$--constituent of category $A$ and $\vec{x} = D(\vec{y})$.
The occurrence $D$ of $\vec{y}$ in $\vec{x}$ is called an
\textbf{accidental G--constituent of category} $A$ 
\textbf{in} $\GB$ if it is not a $G$--constituent of category 
$A$ in $\GB$.
\end{defn}
%%
We shall illustrate this terminology with an example. Let $G$
be the following grammar.
%%
\begin{equation}
\begin{array}{l@{\quad\pf\quad}l}
\mbox{\tt S} & \mbox{\tt SS} \mid \mbox{\tt AB} \mid \mbox{\tt BA} \\
\mbox{\tt A} & \mbox{\tt AS} \mid \mbox{\tt SA} \mid \mbox{\tt a} \\
\mbox{\tt B} & \mbox{\tt BS} \mid \mbox{\tt SB} \mid \mbox{\tt b}
\end{array}
\end{equation}
%%
The string $\vec{x} = \mbox{\tt abaabb}$ has several derivations,
which generate among other the following bracketing analyses.
%%
\begin{equation}
(\mbox{\tt a}(\mbox{\tt b}(\mbox{\tt a}((%
\mbox{\tt ab})\mbox{\tt b})))), \quad
((\mbox{\tt ab})(((\mbox{\tt a}(\mbox{\tt ab}))\mbox{\tt b})))
\end{equation}
%%
We now list all $G$--constituents which occur in $\vec{x}$:
%%
\begin{align}\notag
\mbox{\tt A} & : \mbox{\tt a}, \mbox{\tt aab}, \mbox{\tt aba},
    \mbox{\tt baa}, \mbox{\tt abaab} \\
\mbox{\tt B} & : \mbox{\tt b}, \mbox{\tt abb} \\\notag
\mbox{\tt S} & : \mbox{\tt ab}, \mbox{\tt aabb}, \mbox{\tt abaabb}
\end{align}
%%
Some constituents occur several times, for example
{\tt ab} in $\auf \varepsilon, \mbox{\tt aabb}\zu$ and also in
$\auf \mbox{\tt aba}, \mbox{\tt b}\zu$.
Now we look at the first bracketing, $(\mbox{\tt a}(\mbox{\tt b}%
(\mbox{\tt a}((\mbox{\tt ab})\mbox{\tt b}))))$.
The constituents are {\tt a} (contexts:
$\auf \varepsilon, \mbox{\tt baabb}\zu$, $\auf \mbox{\tt ab},
\mbox{\tt abb}\zu$, $\auf \mbox{\tt aba}, \mbox{\tt bb}\zu$),
{\tt b}, {\tt ab} (for example in the context: $\auf \mbox{\tt aba}, %
\mbox{\tt b}\zu$), {\tt abb} in the context $\auf \mbox{\tt aba}, %
\varepsilon\zu$, {\tt aabb}, {\tt baabb} and {\tt abaabb}. These
are the constituents of the tree. The occurrence $\auf \varepsilon, %
\mbox{\tt aabb}\zu$ of {\tt ab} in {\tt ababb} is therefore an accidental
occurrence of a $G$--constituent of category {\tt S} in that tree.
For although {\tt ab} is a $G$--constituent, this occurrence in the 
tree is not a constituent occurrence of it. Notice that it may happen 
that $\vec{y}$ is a constituent of the tree $\GB$ but that as a 
$G$--constituent of category $C$ it occurs accidentally since its 
category in $\GB$ is $D \neq C$.
%%
\begin{defn}
%%%
\index{grammar!transparent}%%
\index{transparency}%%
\index{grammar!inherently opaque}%%
%%%
A grammar $G$ is called \textbf{transparent} if no $G$--constituent
occurs accidentally in a $G$--string. A grammar which is not
transparent will be called \textbf{opaque}. A language for which
no transparent grammar exists will be called \textbf{inherently
opaque}.
\end{defn}
%%
An example shall illustrate this. For any given signature 
$\Omega$, Polish Notation 
%%%
\index{Polish Notation}%%%
%%%
can be generated by a transparent grammar. 
%%
\begin{equation}
\mbox{\tt S} \pf \mbox{\tt F}_{\Omega(f)} \mbox{\tt S}^{\Omega(f)}
\qquad
\mbox{\tt F}_{\Omega(f)} \pf f 
\end{equation}
%%
\index{$\Pi_{\Omega}$}%%
%%%
This defines the grammar $\Pi_{\Omega}$ for $\PN_{\Omega}$.
Moreover, given a string $\vec{x}$ generated by this grammar, 
the subterm occurrences of $\vec{x}$ under a given analysis are 
in one to one correspondence with the subcontituents of category 
{\tt S}. An occurrence of an $n$--ary function symbol is a 
constituent of type $\mbox{\tt F}_n$. We shall show that this 
grammar is not only unambiguous, it is transparent. 

Let $\vec{x} = x_0 x_1 \dotsb x_{n-1}$ be a string. Then let
$\gamma(\vec{x}) := \sum_{i < n} \gamma(x_i)$,
where for every $f \in F$, $\gamma(f) := \Omega(f) - 1$.
(So, if $\Omega(f) = 0$, $\gamma(f) = -1$.)
The proof of the following is left as an exercise. 
%%
\begin{lem}
\label{lem:zahl}%% 
$\vec{x} \in \PN_{\Omega}$ iff (a) $\gamma(\vec{x}) = -1$ 
and (b) for every proper prefix $\vec{y}$ of $\vec{x}$ we have 
$\gamma(\vec{y}) \geq 0$.
\end{lem}
%%
It follows from this theorem that no proper prefix of a
term is a term. (However, a suffix of a term may again be a term.) 
The constituents are therefore all the substrings that have the 
properties (a) and (b). We show that the grammar is transparent.
Now suppose that $\vec{x}$ contains an accidental occurrence of a 
term $\vec{y}$. Then this occurrence overlaps properly with a 
constituent $\vec{z}$. Without loss of generality $\vec{y} = 
\vec{u} \conc \vec{v}$ and $\vec{z} = \vec{v} \conc \vec{w}$ 
(with $\vec{u}, \vec{w} \neq \varepsilon$). It follows that
$\gamma(\vec{v}) = \gamma(\vec{y}) - \gamma(\vec{u})
< 0$ since $\gamma(\vec{u}) \geq 0$. Hence there exists a
proper prefix  $\vec{u}_1$ of $\vec{u}$ such that
$\vec{u}_1 = -1$. (In order to show this one must first conclude
that the set $P(\vec{x}) := \{\gamma(\vec{p}) : 
\vec{p} \mbox{ is a prefix of } \vec{x}\}$ is a convex set
for every term $\vec{x}$. See Exercise~\ref{ex:PN}.)
%%
\begin{thm}
\label{thm:pn}
The grammar $\Pi_{\Omega}$ is transparent.
\proofend
\end{thm}
%%
Now look at the languages $\mbox{\tt a}^+\mbox{\tt b}$ and
$\mbox{\tt a}^+$. Both are regular. There is a transparent 
regular grammar for $\mbox{\tt a}^+\mbox{\tt b}$. It has the rules 
$\mbox{\tt S} \pf \mbox{\tt aB}$, $\mbox{\tt B} \pf \mbox{\tt AB} 
\mid \mbox{\tt b}$. $\mbox{\tt a}^+$ is on the other hand inherently 
opaque.  For any CFG must generate at least two constituents of the  
form $\mbox{\tt a}^p$ and $\mbox{\tt a}^q$, $q > p$. 
Now there exist two occurrences of $\mbox{\tt a}^p$ in 
$\mbox{\tt a}^q$ which properly overlap. One of them must be
accidental.
%%
\begin{prop}
$\mbox{\tt a}^+$ is inherently opaque.
\proofend
\end{prop}
%%
It can easily be seen that if $L$ is transparent and
$\varepsilon \in L$, then $L = \{\varepsilon\}$.
Also, a language over an alphabet consisting of a single letter
can only be transparent if it contains no more than
a single string. Many properties of CFGs
are undecidable. Transparency is different in this respect.
%%
\nocite{fine:transparency}
\index{Fine, Kit}%%%
%%
\begin{thm}[Fine]
Let $G$ be a CFG. It is decidable whether or not
$G$ is transparent.
\end{thm}
%%
\proofbeg
Let $k$ be the constant from the Pumping Lemma (\ref{thm:pumplemma}). 
This constant can effectively be determined. By Lemma~\ref{lem:opakred} 
there is an accidental occurrence of a constituent iff there is
an accidental occurrence of a right hand side of a production.
These are of the length $p + 1$ where $p$ is the maximum
productivity of a rule from $G$. Further, because of Lemma~\ref{lem:akz}
we only need to check those constituents for accidental occurrences
whose length does not exceed $p^2 + p$. This can be done in
finite amount of time.
\proofend
%%
\begin{lem}
\label{lem:opakred}
$G$ is opaque iff there is a production $\rho =
A \pf \vec{\alpha}$ such that $\vec{\alpha}$ has an accidental
occurrence in a partial $G$--tree.
\end{lem}
%%
\proofbeg
Let $\vec{\phi}$ be a string of minimal length which occurs
accidentally. And let $C$ be an accidental occurrence of
$\vec{\phi}$. Further, let $\vec{\phi} = \vec{\gamma}_1 %
\vec{\alpha} \vec{\gamma}_2$, and let  $A \pf \vec{\alpha}$
be a rule. Then two cases may occur.
(A) The occurrence of $\vec{\alpha}$ is accidental.
Then we have a contradiction to the minimality of
$\vec{\phi}$. (B) The occurrence of $\vec{\alpha}$ is not
accidental. Then $\vec{\eta} := \vec{\gamma}_1 A \vec{\gamma}_2$
also occurs accidentally in $C(\vec{\eta})$! (We can undo
the replacement $A \pf \vec{\alpha}$ in the string
$C(\vec{\varphi})$ since $\vec{\alpha}$ is a constituent.)
Also this contradicts the minimality of $\vec{\phi}$. So,
$\vec{\phi}$ is the right hand side of a production.
\proofend
%%
\begin{lem}
Let $G$ be a CFG without rules of productivity
$-1$ and let $\vec{\alpha}$, $\vec{\gamma}$ be strings. Further, 
assume that $\vec{\gamma}$ is a $G$--constituent of category $A$ 
in which $\vec{\alpha}$ occurs accidentally and in which 
$\vec{\gamma}$ is minimal in the following sense: there is no 
$\vec{\eta}$ of category $A$ with (1) $|\vec{\eta}| < |\vec{\gamma}|$ 
and (2) $\vec{\eta} \vdash_G \vec{\gamma}$ and (3) $\vec{\alpha}$ 
occurs accidentally in $\vec{\eta}$. Then every constituent of 
length $> 1$ overlaps with the accidental occurrence of $\vec{\alpha}$.
\end{lem}
%%
\proofbeg
Let $\vec{\gamma} = \vec{\sigma}_1 \, \vec{\eta}\, \vec{\sigma}_2$,
$|\vec{\eta}| > 1$, and assume that the occurrence of $\vec{\eta}$
is a constituent of category $A$ which does not overlap with
$\vec{\alpha}$. Then $\vec{\alpha}$ occurs accidentally in
$\vec{\delta} := \vec{\sigma}_1\, A\, \vec{\sigma}_2$. Further,
$|\vec{\delta}| < |\vec{\gamma}|$, contradicting the minimality of
$\vec{\gamma}$.
\proofend
%%
\begin{lem}
\label{lem:akz}
Let $G$ be a CFG where the productivity of rules
is at least 0 and at most $p$, and let $\vec{\alpha}$
be a string of length $n$ which occurs accidentally.
Then there exists a constituent $\vec{\gamma}$
of length $\leq n p$ in which $\vec{\alpha}$ occurs accidentally.
\end{lem}
%%
\proofbeg
Let $A \vdash_G \vec{\gamma}$ be minimal in the sense of the previous
lemma. Then we have that every constituent of $\vec{\gamma}$
of length $> 1$ overlaps properly with $\vec{\alpha}$. Hence
$\vec{\gamma}$ has been obtained by at most $n$ applications
of rules of productivity $> 0$. Hence $|\vec{\gamma}| \leq n p$.
\proofend

The property of transparency is stronger than that of unique
readability, also known as unambiguity, which is defined as follows.
%%
\begin{defn}
%%%
\index{grammar!ambiguous}%%
\index{language!inherently ambiguous}%%
%%%
A CFG $G$ is called \textbf{unambiguous} if for every string
$\vec{x}$ there is at most one $G$--tree whose associated string
is $\vec{x}$. If $G$ is not unambiguous, it is called 
\textbf{ambiguous}. A CFL $L$ is called \textbf{inherently
ambiguous} if every CFG generating it is ambiguous.
\end{defn}
%%
\begin{prop}
Every transparent grammar is unambiguous.
\end{prop}
%%
There exist inherently ambiguous languages. Here is an example.
%%
\index{Parikh, Rohit}%%%
\begin{thm}[Parikh]
The language $L$ is inherently ambiguous.
%%%
\begin{equation}
L := \{\mbox{\tt a}^n \mbox{\tt b}^n \mbox{\tt c}^m : n,
m \in \omega\} \cup \{\mbox{\tt a}^m \mbox{\tt b}^n \mbox{\tt c}^n
: n, m \in \omega\}
\end{equation}
\end{thm}
%%
\proofbeg %%
$L$ is context free and so there exists a CFG $G$ such that 
$L(G) = L$. We shall show that $G$ must be ambiguous. There is a 
number $k$ which satisfies the
Pumping Lemma (\ref{thm:pumplemma}). Let $n := k! (:= 
\prod_{i = 1}^{k} i)$. Then there exists a decomposition of 
$\mbox{\tt a}^{2n} \mbox{\tt b}^{2n} \mbox{\tt c}^{3n}$ into
%%
\begin{equation}
\label{eq:decomp}
\vec{u}_1\conc \vec{x}_1\conc \vec{v}_1\conc \vec{y}_1\conc \vec{z}_1
\end{equation}
%%
in such a way that $|\vec{u}_1| \leq k$. Furthermore, we may also 
assume that $|\vec{v}_1| \leq k$.
It is easy to see that $\vec{x}_1\, \vec{y}_1$ may not contain 
occurrences of {\tt a}, {\tt b} and {\tt c} at the same time.
Since it contains {\tt a}, it may not contain {\tt c}. So we 
have $\vec{x}_1 = \mbox{\tt a}^p$ and $\vec{y}_1 = \mbox{\tt b}^p$
for some $p$. We consider a maximal constituent of \eqref{eq:decomp} 
of the form $\mbox{\tt a}^q \, \mbox{\tt b}^{q'}$. Such a constituent 
must exist. ($\vec{x}_1\conc\vec{v}_1\conc\vec{y}_1$ is of that 
form.) In it there is a constituent of the form $\mbox{\tt a}^{q - i} \,
\mbox{\tt b}^{q' - i}$ for some $i < k$. This follows from the
Pumping Lemma. Hence we can pump up $\mbox{\tt a}^i$ and
$\mbox{\tt b}^i$ at the same time and get strings of the form
%%
\begin{equation}
\mbox{\tt a}^{2p + ki}\, \mbox{\tt b}^{2p+ki} \, \mbox{\tt c}^{3q}
\end{equation}
%%
while there exists a constituent of the form $\mbox{\tt a}^{2p + ki - r}\,
\mbox{\tt b}^{2p + ki - s}$ for certain $r, s \leq k$. In particular,
for $k := p/i$ we get
%%
\begin{equation}
\mbox{\tt a}^{3p}\, \mbox{\tt b}^{3p}\, \mbox{\tt c}^{3q}
\end{equation}
%%
Now we form a decomposition of
$\mbox{\tt a}^{3n} \mbox{\tt b}^{2n} \mbox{\tt c}^{2n}$ into
%%
\begin{equation}
\vec{u}_2\conc \vec{x}_2\conc \vec{v}_2\conc \vec{y}_2\conc \vec{z}_2
\end{equation}
%%
in such a way that $|\vec{z}_2|, |\vec{v}_2| \leq k$. Analogously
we get a constituent of the form $\mbox{\tt b}^{2p - s'}\conc 
\mbox{\tt c}^{2p - r'}$ for certain $r', s' \leq k$. These
occurrences overlap. For the left hand constituent contains $3p - s$ many
occurrences of {\tt b} and the right hand constituent contains $3p
- s'$ many occurrences of {\tt b}. Since $3p - s + 3p - s' = 6p -
(s + s') > 3p$, these constituents must overlap. However, they are
not equal. But this is impossible. So $G$ is ambiguous. Since $G$
was arbitrary, $L$ is inherently ambigous.
%%
\proofend
%%

Now we discuss a property which is in some sense the opposite of the 
property of unambiguity. It says that if a right hand side occurs in a
constituent, then under some different analysis this occurrence is
actually a constituent occurrence.
%%
\begin{defn}
%%%
\index{NTS--property}%%
\index{language!NTS--\faul}%%
%%%
A CFG has the \textbf{NTS--pro\-per\-ty} if from 
$C \vdash_G \vec{\alpha}_1 \conc \vec{\beta} \conc \vec{\alpha}_2$ 
and $B \pf \vec{\beta} \in R$ follows: $C \vdash_G \vec{\alpha}_1
\conc B \conc \vec{\alpha}_2$. A language is called
an \textbf{NTS--language} if it has an NTS--grammar.
\end{defn}
%%
The following grammar is not an NTS--grammar.
%%
\begin{equation}
\mbox{\tt X} \pf \mbox{\tt aX}, \quad \mbox{\tt X} \pf \mbox{\tt a}
\end{equation}
%%
For we have $\mbox{\tt X} \vdash \mbox{\tt aa}$ but it does
not hold that $\mbox{\tt X} \vdash \mbox{\tt Xa}$.
In general, regular grammars are not NTS. However, we have
%%
\begin{thm}
All regular languages are NTS--languages.
\end{thm}
%%
\proofbeg
Assume that $L$ is regular. Then there exists a finite state automaton
$\GA = \auf A, Q, q_0, F, \delta\zu$ such that $L = L(\GA)$.
Put $N := \{S^{\star}\} \cup \{L(p,q) : p, q \in Q\}$. 
Further, put $G := \auf \mbox{\tt S}^{\star}, N, A, R\zu$, where 
$R$ consists of 
%%
\begin{equation}
\begin{array}{l@{\quad \pf\quad}ll}
\mbox{\tt S}^{\star} & \mbox{\tt L}(q_0,q) & (q \in F) \\
\mbox{\tt L}(p,q)    & \mbox{\tt L}(p,r) \mbox{\tt L}(r,q) \\
\mbox{\tt L}(p,q)    & a           & (q \in \delta(p, a))
\end{array}
\end{equation}
%%
Then we have $\mbox{\tt L}(p,q) \vdash_G \vec{x}$  iff
$q \in \delta(p,\vec{x})$, as is checked by induction.
From this follows that $\mbox{\tt S}^{\star} \vdash_G \vec{x}$
iff $\vec{x} \in L(\GA)$. Hence we have $L(G) = L$. It
remains to show that $G$ has the NTS--property.  To this end let
$\mbox{\tt L}(p,q) \vdash_G \vec{\alpha}_1 \conc \vec{\beta}
\conc \vec{\alpha}_2$ and $\mbox{\tt L}(r,s) \vdash_G \vec{\beta}$.
We have to show that $\mbox{\tt L}(p,q) \vdash_G \vec{\alpha}_1 %
\conc \mbox{\tt L}(r,s) \conc \vec{\alpha}_2$.  In order to do
this we extend the automaton $\GA$ to an automaton which reads
strings from $N \cup A$. Here $q \in \delta(p, C)$
iff for every string $\vec{y}$ with $C \vdash_G \vec{y}$
we have $q \in \delta(p, \vec{y})$. Then it is clear that
$q \in \delta(p, \mbox{\tt L}(p,q))$. Then it still holds that
$\mbox{\tt L}(p,q) \vdash_G \vec{\alpha}$ iff
$q \in \delta(p, \vec{\alpha})$. Hence we have
$r \in \delta(p, \vec{\alpha}_1)$ and
$q \in \delta(s, \vec{\alpha}_2)$. From this follows that
$\mbox{\tt L}(p,q) \vdash_G \mbox{\tt L}(p,r) \mbox{\tt L}(r,s)
\mbox{\tt L}(s,q)$ and finally
$\mbox{\tt L}(p,q) \vdash_G \vec{\alpha}_1 \mbox{\tt L}(r,s) \vec{\alpha}_2$.
%%
\proofend
%%

If a grammar has the NTS--property, strings can be recognized
very fast. We sketch a pushdown automaton that recognizes
$L(G)$. Scanning the string from left to right it puts the
symbols onto the stack. Using its states the automaton
memorizes the content of the stack up to $\kappa$ symbols
deep, where $\kappa$ is the length of a longest right hand
side of a production. If the upper part of the stack matches
a right hand side of a production $A \pf \vec{\alpha}$ in
the appropriate order, then $\vec{\alpha}$ is deleted from
the stack and $A$ is put on top of it. At this moment the
automaton rescans the upper part of the stack up to $\kappa$
symbols deep. This is done using a series of empty moves. The 
automaton pops $\kappa$ symbols and then puts them back onto 
the stack.  Then it continues the procedure above. It is 
important that the replacement of a right hand side by a left 
hand side is done whenever first possible.
%%
\begin{thm}
Let $G$ be an NTS--grammar. Then $G$ is deterministic.
Furthermore, the recognition and parsing problem are in
$\textbf{DTIME}(n)$.
\end{thm}
%%
We shall deepen this result. To this end we abstract somewhat
from the pushdown automata and introduce a calculus which
manipulates pairs $\vec{\alpha} \bvdash \vec{x}$ of strings
separated by a turnstile. Here, we think of $\vec{\alpha}$
as the stack of the automaton and $\vec{x}$ as the string
to the right of the reading head.  It is not really necessary
to have terminal strings on the right hand side; however,
the generalization to arbitrary strings is easy to do.
There are several operations. The first is called
\textbf{shift}. It simulates the reading of the first
symbol.
%%%
\index{shift}%%
%%%
%%
\begin{equation}
\mbox{\rm shift:}\qquad \begin{array}{l@{\,\bvdash\,}l}
\vec{\eta} & x \vec{y} \\\hline
\vec{\eta}x & \vec{y}
\end{array}
\end{equation}
%%
Another operation is \textbf{reduce}.
%%%
\index{reduction}%%
%%%
%%
\begin{equation}
\mbox{reduce $\rho$:}\qquad
    \begin{array}{l@{\,\bvdash\,}l}
\vec{\eta}\vec{\alpha} & \vec{x} \\\hline
\vec{\eta}X & \vec{x}
\end{array}
\end{equation}
%%
Here $\rho = X \pf \vec{\alpha}$ must be a $G$--rule.
This calculus shall be called the \textbf{shift--reduce--calculus for}
$G$. The following theorem is easily proved by induction on the
length of a derivation.
%%
\begin{thm}
Let $G$ be a CFG. $\vec{\alpha} \vdash_G \vec{x}$
iff there is a derivation of $\vec{\alpha} \bvdash \varepsilon$
from $\varepsilon \bvdash \vec{x}$ in the shift--reduce--calculus
for $G$.
\end{thm}
%%
This strategy can be applied to every language. We take the
following grammar.
%%
\begin{equation}
\begin{array}{l@{\quad\pf\quad}l}
\mbox{\tt S} & \mbox{\tt ASB} \mid \mbox{\tt c} \\
\mbox{\tt A} & \mbox{\tt a} \\
\mbox{\tt B} & \mbox{\tt b}
\end{array}
\end{equation}
%%
Then we have $S \vdash_G \mbox{\tt aacbb}$. Indeed, we get
a derivation shown in Table~\ref{tab:schieb}.
%%
\begin{table}
\caption{A Derivation by Shifting and Reducing}
\label{tab:schieb}
$$\begin{array}{l@{\,\bvdash\,}l}
\varepsilon & \mbox{\tt aacbb} \\\hline
\mbox{\tt a} & \mbox{\tt acbb} \\\hline
\mbox{\tt A} & \mbox{\tt acbb} \\\hline
\mbox{\tt Aa} & \mbox{\tt cbb} \\\hline
\mbox{\tt AA} & \mbox{\tt cbb} \\\hline
\mbox{\tt AAc} & \mbox{\tt bb} \\\hline
\mbox{\tt AAS} & \mbox{\tt bb} \\\hline
\mbox{\tt AASb} & \mbox{\tt b} \\\hline
\mbox{\tt AASB} & \mbox{\tt b} \\\hline
\mbox{\tt AS} & \mbox{\tt b} \\\hline
\mbox{\tt ASb} & \varepsilon \\\hline
\mbox{\tt ASB} & \varepsilon \\\hline
\mbox{\tt S}   & \varepsilon
\end{array}$$
\end{table}
%%
Of course the calculus does not provide unique solutions.
On many occasions we have to guess whether to shift or whether 
to reduce, and if the latter, then by what rule. Notice namely 
that if some right hand side of a production is a suffix of a 
right hand side of another production we have an option. We call 
a $k$--\textbf{strategy} a function $f$
%%%
\index{strategy}%%
%%%
which tells us for every pair $\vec{\alpha} \bvdash \vec{x}$
whether or not we shall shift or reduce (and by which rule).
Further, $f$ shall only depend (1) on the reduction rules
which can be at all applied to $\vec{\alpha}$
and (2) on the first $k$ symbols of $\vec{x}$.
We assume that in case of competition only one rule is
chosen. So, a $k$--strategy is a map $R \times \bigcup_{i < k}
A^i$ to $\{s, r\}$. If $\vec{\alpha} \bvdash \vec{x}$ is
given then we determine the next rule application
as follows. Let $\vec{\beta}$ be a suffix of
$\vec{\alpha}$ which is reducible. If
$f(\vec{\beta}, {^{(k)}\vec{x}}) = s$, then we shift; if
$f(\vec{\beta}, {^{(k)}\vec{x}}) = r$ then we apply
reduction to $\vec{\beta}$.  This is in fact not really
unambigous. For a right hand side of a production may
be the suffix of a right hand side of another production.
Therefore, we look at another property.
%%
\begin{multline}
\label{eq:24ast}
\text{If $\rho_1 = X_1 \pf \vec{\beta}_1 \in R$
and $\rho_2 = X_2 \pf  \vec{\beta}_2 \in R$, $\rho_1 \neq
\rho_2$,} \\
\text{and if $|\vec{y}| \leq k$ then $f(\vec{\beta}_1, \vec{y})$ 
or $f(\vec{\beta}_2,\vec{y})$
is undefined.}
\end{multline}
%%
\nocite{knuth:lrk}
%%
\begin{defn}
%%%
\index{grammar!$LR(k)$--\faul}%%
\index{language!$LR(k)$--\faul}%%
%%%
A CFG $G$ is called an $\textbf{LR}(k)$--\textbf{grammar} if
not $\mbox{\tt S} \Pf^+ \mbox{\tt S}$ and if for some
$k \in \omega$ there is a $k$--strategy for the shift--and--reduce
calculus for $G$. A language is called an $LR(k)$--\textbf{language} 
if it is generated by some $LR(k)$--grammar.
\end{defn}
%%
\begin{thm}
A CFG is an $LR(k)$--grammar if the following holds:
Suppose that $\vec{\eta}_1 \vec{\alpha}_1 \vec{x}_1$ and
$\vec{\eta}_2 \vec{\alpha}_2 \vec{x}_2$ have a rightmost
derivation and that with $p := |\vec{\eta}_1 \vec{\alpha}_1| +k$
we have
%%
\begin{equation}
{^{(p)}\vec{\eta}_1 \vec{\alpha}_1 \vec{x}_1} =
{^{(p)}\vec{\eta}_2 \vec{\alpha}_2 \vec{x}_2}
\end{equation}
%%
Then $\vec{\eta}_1 = \vec{\eta}_2$,
$\vec{\alpha}_1 = \vec{\alpha}_2$ and ${^{(k)}\vec{x}_1} =
{^{(k)}\vec{x}_2}$.
\end{thm}
%%
This theorem is not hard to show. It says that the strategy
may be based indeed only on the $k$--prefix of the string
which is to be read. This is essentially the property \eqref{eq:24ast}.
One needs to convince oneself that a derivation in the
shift--reduce--calculus corresponds to a rightmost derivation,
provided reduction is scheduled as early as possible.
%%
\begin{thm}
\label{thm:det}
$LR(k)$--languages are deterministic.
\end{thm}
%%
We leave the proof of this fact to the reader. The task is
to show how to extract a deterministic automaton from a
strategy. The following is easy.
%%
\begin{lem}
Every $LR(k)$--language is an $LR(k+1)$--language.
\end{lem}
%%
So we have the following hierarchy.
%%
\begin{equation}
LR(0) \;\subseteq\; LR(1) \; \subseteq\; LR(2) \; \subseteq \;
LR(3) \dotso
\end{equation}
%%
This hierarchy is stationary already from $k = 1$.
%%
\begin{lem}
\label{lem:reduktion}
Let $k > 0$. If $L$ is an $LR(k+1)$--language then
$L$ also is an $LR(k)$--language.
\end{lem}
%%
\proofbeg
For a proof we construct an $LR(k)$--grammar $G^>$ from an 
$LR(k+1)$--grammar $G$. For simplicity we assume that $G$ is
in Chomsky Normal Form. The general case is easily shown in the
same way. The idea behind the construction is as follows. A
constituent of $G^>$ corresponds to a constituent of $G$ which 
has been shifted one letter to the right. To implement this idea 
we introduce new symbols, $[a,X,b]$, where $a, b \in A$, $X \in N$, and
$[a,X, \varepsilon]$, $a \in A$. The start symbol of $G^>$ is
the start symbol of $G$. The rules are as follows, where $a, 
b, c$ range over $A$.
%%
\begin{equation}
\begin{array}{l@{\quad\pf \quad}ll}
\mbox{\tt S} & \varepsilon & \mbox{if }
        \mbox{\tt S} \pf \varepsilon \in R, \\
\mbox{\tt S} & a\; [a,\mbox{\tt S},\varepsilon] & a \in A, \\
\mbox{}[a, X, b]    & [a, Y, c]\; [c, Z, b]
    & \mbox{if }X \pf YZ \in R, \\
\mbox{} [a, X, \varepsilon] & [a, Y, c]\; [c, Z, \varepsilon] &
    \mbox{if }X \pf YZ \in R, \\
\mbox{}[a, X, b]    & b   & \mbox{if }X \pf a \in R, \\
\mbox{}[a, X, \varepsilon] & \varepsilon & \mbox{if }X \pf a \in R.
\end{array}
\end{equation}
%%
By induction on the length of a derivation the following is
shown.
%%
\begin{subequations}
\begin{align}
[a, X, b] \vdash_{G^>} \vec{\alpha}\, b \qquad\Dpf\qquad
& X \vdash_G a \, \vec{\alpha} \\
[a, X, \varepsilon] \vdash_{G^>} \vec{\alpha} \qquad\Dpf\qquad
& X \vdash_G a\, \vec{\alpha}
\end{align}
\end{subequations}
%%
From this we can deduce that $G^>$ is an $LR(k)$--grammar. To
this end let $\vec{\eta}_1 \vec{\alpha}_1 \vec{x}_1$ and
$\vec{\eta}_2 \vec{\alpha}_2 \vec{x}_2$ be rightmost derivable
in $G^>$, and let $p := |\vec{\eta}_1 \vec{\alpha}_1| +k$ as
well as
%%
\begin{equation}
{^{(p)}\vec{\eta}_1 \vec{\alpha}_1 \vec{x}_1} =
{^{(p)}\vec{\eta}_2 \vec{\alpha}_2 \vec{x}_2}
\end{equation}
%%
Then $a\vec{\eta}_1 \vec{\alpha}_1 \vec{x}_1 =
\vec{\eta}_1' \vec{\alpha}_1' b \vec{x}_1$ for some $a, b \in A$
and some $\vec{\eta}_1'$, $\vec{\alpha}_1'$ with
$a \vec{\eta}_1 = \vec{\eta}_1' c$ for $c \in A$
and $c \vec{\alpha}_1 = \vec{\alpha}_1' b$. Furthermore,
we have $a\vec{\eta}_2 \vec{\alpha}_2 \vec{x}_2 =
\vec{\eta}_2' \vec{\alpha}_2' b \vec{x}_2$,
$a \vec{\eta}_2 = \vec{\eta}_2' c$ and $c \vec{\alpha}_2 =
\vec{\alpha}_2'$ for certain $\vec{\eta}_2'$ and
$\vec{\alpha}_2'$. Hence we have
%%
\begin{equation}
{^{(p+1)}\vec{\eta}_1' \vec{\alpha}_1' b \vec{x}_1} =
{^{(p+1)}\vec{\eta}_2' \vec{\alpha}_2' b \vec{x}_2}
\end{equation}
%%
and $p +1 = |\vec{\eta}_1' \vec{\alpha}_1'| + k + 1$.
Furthermore, the left hand and the right hand string
have a rightmost derivation in $G$. From this it follows,
since $G$ is an $LR(k+1)$--grammar, that $\vec{\eta}_1' = %
\vec{\eta}_2'$ and $\vec{\alpha}_1' = \vec{\alpha}_2'$,
as well as ${^{(k+1)}b \vec{x}_1} = {^{(k+1)}b \vec{x}_2}$.
From this we get $\vec{\eta}_1 = \vec{\eta}_2$, $\vec{\alpha}_1 %
= \vec{\alpha}_2$ and ${^{(k)}\vec{x}_1} = {^{(k)}\vec{x}_2}$,
as required.
\proofend

Now we shall prove the following important theorem.
%%
\begin{thm}
\label{thm:detlr1}
Every deterministic language is an $LR(1)$--lan\-gua\-ge.
\end{thm}
%%
The proof is relatively long. Before we begin we shall
prove a few auxiliary theorems which establish that strictly
deterministic languages are exactly the languages that are
generated by strict deterministic grammars, and that they
are unambiguous and in $LR(0)$. This will give us the key
to the general theorem.

We still owe the reader a proof that strict deterministic grammars 
only generate strict deterministic languages. This is essentially the
consequence of a property that we shall call
\textbf{left transparency}.
%%%
\index{left transparency}%%
%%
We say $\vec{\alpha}$ \textbf{occurs in} $\vec{\eta}_1\, \vec{\alpha}\,
\vec{\eta}_2$ \textbf{with left context} $\vec{\eta}_1$.
%%%
\index{context!left}%%
%%%
%%
\begin{defn}
%%%
\index{grammar!left transparent}%%
%%%
Let $G$ be a CFG. $G$ is called \textbf{left transparent}
if a constituent may never occur in a string accidentally
with the same left context. This means that
if $\vec{x}$ is a constituent of category $C$ in $\vec{y}_1 \, \vec{x}
\, \vec{y}_2$ and if $\vec{z} := \vec{y}_1 \, \vec{x} \, \vec{y}_3$
is a $G$--string then $\vec{x}$ also is a constituent
of category $C$ in $\vec{z}$.
\end{defn}
%%
For the following theorem we need a few definitions.
Let $\GB$ be a tree and $n \in \omega$ a natural number.
Then ${^{(n)}\GB}$ denotes the tree which consists of all
nodes above the first $n$ leaves from the left. Let
$P$ the set of leaves of $\GB$, say $P = \{p_i : i < q\}$,
and let $p_i \sqsubset p_j$ iff $i < j$.
Then put $N_n := \{p_i : i < n\}$, and $O_n := \uppx{N_n}$.
${^{(n)}\GB} := \auf O_n, r, <, \sqsubset\zu$, where
$<$ and $\sqsubset$ are the relations relativized to
$O_n$. If $\ell$ is a labelling function and $\GT = \auf \GB, \ell\zu$
a labelled tree then let ${^{(n)}\GT} := \auf {^{(n)}\GB},
\ell \restriction O_n\zu$. Again, we denote $\ell \restriction
O_n$ simply by $\ell$. We remark that the set
$R_n := {^{(n)}\GB} - {^{(n-1)}\GB}$ is linearly ordered
by $<$.
%There exists for every $y \in R_n$, which is not a leaf
%a $z$ with $z \prec y$.
We look at the largest element $z$ from $R_n$. Two
cases arise. (a) $z$ has no right sister.
(b) $z$ has a right sister. In Case (a) the constituent
of the mother of $z$ is closed at the transition from
${^{(n-1)}\GB}$ to ${^{(n)}\GB}$. Say that $y$ is at the
\textbf{right edge} of $\GT$ if there is no $z$ such that
$y \sqsubset z$. Then $\uppx{R_n}$ consists exactly of the
elements which are at the right edge of ${^{(n)}\GB}$
and $R_n$ consists of all those elements which are at
the right edge of ${^{(n)}\GB}$ but not contained in
${^{(n-1)}\GB}$. Now the following holds.
%%
\begin{prop}
\label{prop:lfp}
Let $G$ be a strict deterministic grammar. Then $G$
is left transparent. Furthermore: let $\GT_1 = \auf \GB_1, \ell_1\zu$
and $\GT_2 = \auf \GB_2, \ell_2\zu$ be partial $G$--trees
such that the following holds.
\begin{dingautolist}{192}
\item
    If $C_i$ is the label of the root of $\GT_i$ then
    $C_1 \equiv C_2$.
\item
    ${^{(n)}k(\GT_1)} = {^{(n)}k(\GT_2)}$.
\end{dingautolist}
%%
Then there is an isomorphism $f \colon {^{(n+1)}\GB_1} \epi {^{(n+1)}\GB_2}$
such that $\ell_2(f(x)) = \ell_1(x)$ in case $x$ is not at the
right edge of ${^{(n+1)}\GB_1}$ and $\ell_2(f(x)) \equiv \ell_1(x)$
otherwise.
\end{prop}
%%
\proofbeg
We show the theorem by induction on $n$. We assume that it holds
for all $k < n$. If $n = 0$, it holds anyway. Now we show
the claim for $n$. There exists by assumption an isomorphism
$f_n \colon {^{(n)}\GB_1} \pf {^{(n)}\GB_2}$ satisfying the
conditions given above. Again, put $R_{n+1} := {^{(n+1)}\GB_1} %
- {^{(n)}\GB_1}$. At first we shall show that $\ell_2(f_n(x)) =
\ell_1(x)$ for all $x \not\in \uppx{R_{n+1}}$. From this it
immediately follows that $\ell_2(f_n(x)) \equiv \ell_1(x)$ for all
$x \in \uppx{R_{n+1}} - R_{n+1}$ since $G$ is strict
deterministic. This claim we show by induction on the height of
$x$. If $h(x) = 0$, then $x$ is a leaf and the claim holds because
of the assumption that $\GT_1$ and $\GT_2$ have the same
associated string. If $h(x) > 0$ then every daughter of $x$ is in
$\uppx{R_{n+1}}$. By induction hypothesis therefore
$\ell_2(f_n(y)) = \ell_1(y)$ for every $y \prec x$. Since $G$ is
strict deterministic, the label of $x$ is uniquely fixed by this
for $\ell_2(f_n(x)) \equiv \ell_1(x)$, by induction hypothesis. So
we now have $\ell_2(f_n(x)) = \ell_1(x)$. This shows the first
claim. Now we extend $f_n$ to an isomorphism $f_{n+1}$ from
${^{(n+1)}\GB_1}$ onto ${^{(n+1)}\GB_2}$ and show at the same time
that $\ell_2(f_n(x)) \equiv \ell_1(x)$ for every $x \in
\uppx{R_{n+1}}$. This holds already by inductive hypothesis for
all $x \not\in R_{n+1}$. So, we only have to show this for $x \in
R_{n+1}$. This we do as follows. Let $u_0$ be the largest node in
$R_{n+1}$. Certainly, $u_0$ is not the root. So let $v$ be the
mother of $u_0$. $f_n$ is defined on $v$ and we have
$\ell_2(f_n(v)) \equiv \ell_1(v)$. By assumption, $\ell_2(f_n(x))
= \ell_1(x)$ for all $x \sqsubset u$. So, we first of all have
that there is a daughter $x_0$ of $f_n(v)$ which is not in the
image of $f_n$. We choose $x_0'$ minimal with this property. Then
we put $f_{n+1}(u_0) := x_0$. Now we have $\ell_2(f_{n+1}(u_0))
\equiv \ell_1(u_0)$. We continue with $u_0$ in place of $v$. In
this way we obtain a map $f_{n+1}$ from ${^{(n)}\GB_1} \cup
R_{n+1} = {^{(n+1)}\GB_1}$ to ${^{(n+1)}\GB_2}$ with
$\ell_2(f_{n+1}(x))  \equiv \ell_1(x)$, if $x \in R_{n+1}$ and
$\ell_2(f_{n+1}(x)) = \ell_1(x)$ otherwise. That $f_{n+1}$ is
surjective is seen as follows. Suppose that $u_k$ is the leaf of
$\GB_1$ in $R_{n+1}$. Then $x_k = f_{n+1}(u_k)$ is not a leaf in
$\GB_2$, and then there exists a $x_{k+1}$ in ${^{(n+1)}\GB_2} -
{^{(n)}\GB_2}$. We have $\ell_2(f_{n+1}(x_k)) \equiv \ell_1(u_k)$.
Let $x_p$ be the leaf in $L$. By Lemma~\ref{lem:linksrekursiv}
$\ell_2(x_p) \not\equiv \ell_2(x_k)$ and therefore also
$\ell_2(x_p) \not\equiv \ell_1(u_k)$. However, by assumption $x_p$
is the $n+1$st leaf of $\GB_2$ and likewise $u_k$ is the $n+1$st
leaf of $\GB_1$, from which we get $\ell_1(u_k) = \ell_2(x_p)$ in
contradiction to what has just been shown. \proofend
%%
\begin{thm}
Let $G$ be a strict deterministic grammar. Then $L(G)$ is
unambiguous. Further, $G$ is an $LR(0)$--grammar and $L(G)$ is
strict deterministic.
\end{thm}
%%
\proofbeg 
The strategy of shifting and reducing can be applied as
follows: every time we have identified a right hand side of a rule
$X \pf \vec{\mu}$ then this is a constituent of category $X$ and
we can reduce. This shows that we have a $0$--strategy. Hence the
grammar is an $LR(0)$--grammar. $L(G)$ is certainly unambiguous.
Furthermore, $L(G)$ is deterministic, by Theorem~\ref{thm:det}.
Finally, we have to show that $L(G)$ is prefix free for then by
Theorem~\ref{thm:praefixfrei} it follows that $L(G)$ is strict
deterministic. Now let $\vec{x}\, \vec{y} \in L(G)$.  If also
$\vec{x} \in L(G)$, then by Proposition~\ref{prop:lfp} we must
have $\vec{y} = \varepsilon$. \proofend

At first sight it appears that Lemma~\ref{lem:reduktion}
also holds for $k = 0$. The construction can be extended to
this case without trouble. Indeed, in this case we get something
of an $LR(0)$--grammar; however, it is to be noted that
a strategy for $G^>$ does not only depend on the next symbol.
Additionally, it depends on the fact whether or not the string that
is yet to be read is empty. The strategy is therefore not
entirely independent of the right context even though
the dependency is greatly reduced. That $LR(0)$--languages
are indeed more special than $LR(1)$--languages is the
content of the next theorem.
%%
\nocite{gellerharrison:lr0}
\index{Harrison, M.~A.}%%%
\index{Geller, M.~M.}%%
%%%%
\begin{thm}[Geller \& Harrison]
Let $L$ be a deterministic CFL. Then the following are equivalent.
%%
\begin{dingautolist}{192}
\item
$L$ is an $LR(0)$--language.
\item
If $\vec{x} \in L$, $\vec{x}\,  \vec{v} \in L$ and $\vec{y} \in L$
then also $\vec{y}\, \vec{v} \in L$.
\item
There are strict deterministic languages $U$ and $V$ such that
$L = U \cdot V^{\ast}$.
\end{dingautolist}
%%
\end{thm}
%%
\proofbeg
Assume \ding{192}. Then there is an $LR(0)$--grammar $G$ for $L$. Hence, 
if $X \pf \vec{\alpha}$ is a rule and if $\vec{\eta} \, \vec{\alpha} 
\, \vec{y}$ is $G$--derivable then also  $\vec{\eta}\, X \, \vec{y}$ is
$G$--derivable. Using induction, this can also be shown of
all pairs $X$, $\vec{\alpha}$ for which $X \vdash_G \vec{\alpha}$.
Now let $\vec{x} \in L$ and $\vec{x}\, \vec{v} \in L$. Then
$\mbox{\tt S} \vdash_G \vec{x}$, and so by the
previous $\vdash_G \mbox{\tt S} \, \vec{v}$. Therefore, 
since $\mbox{\tt S} \vdash_G \vec{y}$ we have 
$\vdash_G \vec{y}\, \vec{v}$. Hence \ding{193} obtains.
Assume now \ding{193}. Let $U$ be the set of all $\vec{x} \in L$ 
such that $\vec{y} \not\in L$ for every proper prefix $\vec{y}$ of 
$\vec{x}$. Let $V$ be the set of all $\vec{v}$ such that $\vec{x}\,
\vec{v} \in L$ for some $\vec{x} \in U$ but
$\vec{x}\, \vec{w} \not\in L$ for every $\vec{x} \in U$
and every proper prefix $\vec{w}$ of $\vec{v}$.
Now, $V$ is the set of all $\vec{y} \in V^{\ast} -
\{\varepsilon\}$ for which no proper prefix is in $V^{\ast} -
\{\varepsilon\}$. We show that $U \cdot V^{\ast} = L$. To this end
let us prove first that $L \subseteq U \cdot V^{\ast}$. Let
$\vec{u} \in L$. We distinguish two cases.
(a) No proper prefix of $\vec{u}$ is in $L$.
Then $\vec{u} \in U$, by definition of $U$.
(b) There is a proper prefix $\vec{x}$ of $\vec{u}$
which is in $L$. We choose $\vec{x}$ minimally.
Then $\vec{x} \in U$. Let $\vec{u} = \vec{x} \, \vec{v}$.
Now two subcases arise. (A) For no proper prefix
$\vec{w}_0$ of $\vec{v}$ we have $\vec{x}\, \vec{w}_0
\in L$. Then $\vec{v} \in V$, and we are done.
(B) There is a proper prefix $\vec{w}_0$ of $\vec{v}$
with $\vec{x}\, \vec{w}_0 \in L$. Let $\vec{v} =
\vec{w}_0 \, \vec{v}_1$. Then, by \ding{193},
we have $\vec{x} \, \vec{v}_1 \in L$. (In \ding{193}, put
$\vec{x}\, \vec{w}_0$ in place of $\vec{x}$ and in place of
$\vec{y}$ put $\vec{x}$ and for $\vec{w}$ put $\vec{v}_1$.)
$\vec{x}\, \vec{v}_1$ has smaller length than $\vec{x}\, \vec{v}$.
Continue with $\vec{x} \, \vec{v}_1$ in the same way.
At the end we get a partition of $\vec{v} =
\vec{w}_0 \, \vec{w}_1 \dotsb \vec{w}_{n-1}$ such that
$\vec{w}_i \in V$ for every $i < n$. Hence $L \subseteq
U \cdot V^{\ast}$. We now show $U \cdot V^{\ast} \subseteq L$.
Let $\vec{u} = \vec{x} \conc \prod_{i < n} \vec{w}_i$.
If $n = 0$, then $\vec{u} = \vec{x}$ and by definition
of $U$ we have $\vec{u} \in L$. Now let $n > 0$. With
\ding{193} we can show that $\vec{x} \conc \prod_{i < n-1}
\vec{w}_i \in L$. This shows that $\vec{u} \in L$.
Finally, we have to show that $U$ and $V$ are deterministic.
This follows for $U$ from Theorem~\ref{thm:prffrei}. Now let
$\vec{x}, \vec{y} \in U$. Then by \ding{193} 
$P := \{\vec{v} : \vec{x}\, \vec{v} \in L\} =
\{\vec{v} : \vec{y}\, \vec{v} \in L\}$. The reader may convince
himself that $P$ is deterministic. Now let $V$ be the set of
all $\vec{v}$ for which there is no prefix in $P - \{\varepsilon\}$.
Then $P = V^{\ast}$ and because of Theorem~\ref{thm:prffrei}
$V$ is strict deterministic. This shows \ding{194}.
Finally, assume \ding{194}. We have to show that
$L$ is an $LR(0)$--language. To this end, let
$G_1 = \auf \mbox{\tt S}, N_1, A, R_1\zu$ be a strict
deterministic grammar which generates $U$ and
$G_2 = \auf \mbox{\tt S}_2, N_2, A, R_2\zu$
a strict deterministic grammar which generates $V$.
Then let $G_3 := \auf \mbox{\tt S}_3, N_1 \cup N_2 \cup
\{\mbox{\tt S}_3, \mbox{\tt S}_4\}, A, R_3\zu$ be defined
as follows.
%%
\begin{equation}
R_3 := R_1 \cup R_2 \cup \{\mbox{\tt S}_3 \pf \mbox{\tt S}_1,
\mbox{\tt S}_3 \pf \mbox{\tt S}_1 \, \mbox{\tt S}_4,
\mbox{\tt S}_4 \pf \mbox{\tt S}_2, \mbox{\tt S}_4 \pf
\mbox{\tt S}_2 \, \mbox{\tt S}_4\}  
\end{equation}
%%
It is not hard to show that $G_3$ is an $LR(0)$--grammar
and that $L(G_3) = L$.
\proofend
%%

The decomposition in \ding{194} is unique, if we exclude the
possibility that $V = \varnothing$ and if we require that 
$U = \{\varepsilon\}$ shall be the case only if $V = \{\varepsilon\}$. 
In this way we take care of the cases $L = \varnothing$ and $L = U$.
The case $U = V$ may arise. Then $L = U^+$.
The semi Dyck languages are of this kind.

Now we proceed to the proof of Theorem~\ref{thm:detlr1}. Let
$L$ be deterministic. Then put $M := L \cdot \{\$\}$,
where \$ is a new symbol. $M$ is certainly deterministic;
and it is prefix free and so strict deterministic.
It follows that $M$ is an $LR(0)$--language. Therefore there
exists a strict deterministic grammar $G$ which generates $M$.
From the next theorem we now conclude that $L$ is an
$LR(1)$--language.
%%
\begin{lem}
Let $G$ be an $LR(0)$--grammar of the form $G = \auf \mbox{\tt S},
N \cup \{\mbox{\tt \$}\}, A, R\zu$ with $R \subseteq N \times ((N \cup A)^{\ast}
\cup (N \cup A)^{\ast} \cdot \mbox{\tt \$})$ and 
$L(G) \subseteq A^{\ast}\mbox{\tt \$}$, and assume that there is 
no derivation $\mbox{\tt S} \Pf_R \mbox{\tt S\$}$ in $G$. Then 
let $H := \auf \mbox{\tt S}, N, A, R'\zu$, where
%%
\begin{align}
R' := & \phantom{\mbox{}\cup\mbox{}}
    \{A \pf \vec{\alpha} : A \pf \vec{\alpha} \in R, \vec{\alpha} \in
    (N \cup A)^{\ast}\} \\\notag
   & \cup \{A \pf \vec{\alpha} : A \pf \vec{\alpha}\, \mbox{\tt \$} \in R\}
\end{align}
%%
Then $H$ is an $LR(1)$--grammar and $L(H)\cdot\mbox{\tt \$} = L(G)$.
\end{lem}
%%
For a proof consider the following. We do not have
$\mbox{\tt S} \Pf^+_L \mbox{\tt S}$ in $H$. Further: if
$\mbox{\tt S} \Pf_L^+ \vec{\alpha}$ in $H$ then there exists
a $D$ such that $\mbox{\tt S} \Pf^+_L \vec{\alpha}\, D$ in $G$,
and if $\mbox{\tt S} \Pf^+_L \vec{\beta}$ in $G$ then we have
$\vec{\beta} = \vec{\alpha}\, D$ and $\mbox{\tt S} \Pf ^+_L
\vec{\alpha}$ in $H$.  From this we can immediately conclude
that $H$ is an $LR(1)$--grammar.

Finally, let us return to the calculus of shifting and reducing.
We generalize this strategy as follows. For every symbol
$\alpha$ of our grammar we add a symbol $\uli{\alpha}$. This
symbol is a formal inverse of $\alpha$; it signals that at its
place we look for an $\alpha$ but haven't identified it yet.
This means that we admit the following transitions.
%%
\begin{equation}
\begin{array}{l@{\, \bvdash\,}l}
\vec{\eta}\uli{\alpha}\alpha & \vec{x} \\\hline
\vec{\eta}                   & \vec{x}
\end{array}
\end{equation}
%%
We call this rule \textbf{cancellation}.
%%%
\index{cancellation}%%
%%%
We write for strings $\vec{\alpha}$ also
$\uli{\vec{\alpha}}$. This denotes the formal inverse of the
entire string. If $\vec{\alpha} = \prod_{i < n} \alpha_i$ then
$\uli{\alpha} = \prod_{i < n} \uli{\alpha_{n-i}}$. Notice that
the order is reversed.  For example $\uli{\mbox{\tt AB}} =
\uli{\mbox{\tt B}}\conc \uli{\mbox{\tt A}}$.
These new strings allow to perform reductions on the left hand side
even when only part of the right hand side of a production has
been identified. The most general rule is this one.
%%
\begin{equation}
\begin{array}{l@{\, \bvdash\, }l}
\vec{\eta}\uli{X}\vec{\alpha} & \vec{x} \\\hline
\vec{\eta} \uli{\vec{\tau}}  & \vec{x}
\end{array}
\end{equation}
%%
This rule is called the \textbf{LC--rule}.
%%%
\index{LC--rule}%%
\index{LC--calculus}%%
%%%
Here $X \pf \vec{\alpha}\vec{\tau}$ must be a $G$--rule.
This means intuitively speaking that $vec{\alpha}$ is an $X$ 
if followed by $\vec{\tau}$. Since $\vec{\tau}$ is not yet there
we have to write $\uli{\vec{\tau}}$.  The \textbf{LC--calculus} 
consists of the rules shift, reduce and LC. Now the following holds.
%%
\begin{thm}
Let $G$ be a  grammar. $\vec{\alpha} \vdash_G \vec{x}$ holds
iff there is a derivation of $\varepsilon \bvdash
\varepsilon$ from $\uli{\vec{\alpha}} \bvdash \vec{x}$ in the
\textbf{LC}--calculus.
\end{thm}
%%
A special case is $\vec{\alpha} = \varepsilon$. Here no part of the
production has been identified, and one simply guesses a rule.
If in place of the usual rules only this rule is taken, we get
a strategy known as \textbf{top--down strategy}.
%%%
\index{strategy!top--down}%%
%%%
In it, one may shift, reduce and guess a rule.  A grammar is called
an $LL(k)$--grammar if it has a deterministic recognition algorithm
using the top--down--strategy in which the next step depends on the
first $k$ symbols of $\vec{x}$. The case $k = 0$ is of little
use (see the exercises).

This method is however too flexible to be really useful.
However, the following is an interesting strategy. The right
hand side of a production is divided into two parts, which
are separated by a dot.
%%
\begin{equation}
\begin{array}{l@{\quad \pf\quad}l}
\mbox{\tt S} & \mbox{\tt A.SB} \mid \mbox{\tt c.} \\
\mbox{\tt A} & \mbox{\tt a.} \\
\mbox{\tt B} & \mbox{\tt b.}
\end{array}
\end{equation}
%%
This dot fixes the part of the rule that must have been read
when the corresponding LC--rule is triggered. A strategy of this
form is called \textbf{generalized left corner strategy}.
%%%
\index{strategy!generalized left corner}%%%
%%%
If the dot is at the right edge we get the bottom--up strategy,
if it is at the left edge we get the top--down strategy.
%%%
\index{strategy!bottom--up}%%
%%%
%%
\begin{table}
\caption{The Generalized LC--Strategy}
\label{tab:lc}
$$\begin{array}{l@{\, \bvdash\,}l}
\uli{\mbox{\tt S}} & \mbox{\tt aacbb} \\\hline
\uli{\mbox{\tt S}}\, \mbox{\tt a} & \mbox{\tt acbb} \\\hline
\uli{\mbox{\tt S}}\, \mbox{\tt A} & \mbox{\tt acbb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}} & \mbox{\tt acbb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}} \mbox{\tt a}
    & \mbox{\tt cbb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}} \,\mbox{\tt A}
    & \mbox{\tt cbb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}}
    & \mbox{\tt cbb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}}
    \mbox{\tt c} & \mbox{\tt bb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}}\, \uli{\mbox{\tt S}} \mbox{\tt S}
    & \mbox{\tt bb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}} & \mbox{\tt bb} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}}\, \mbox{\tt b}
    & \mbox{\tt b} \\\hline
\uli{\mbox{\tt B}}\, \uli{\mbox{\tt B}}\, \mbox{\tt B} & \mbox{\tt b} \\\hline
\uli{\mbox{\tt B}} & \mbox{\tt b} \\\hline
\uli{\mbox{\tt B}}\, \mbox{\tt b} & \varepsilon \\\hline
\uli{\mbox{\tt B}}\, \mbox{\tt B} & \varepsilon \\\hline
\varepsilon & \varepsilon
\end{array}$$
\end{table}
%%
\vplatz
\exercise
Let $R$ be a set of context free rules, $\mbox{\tt S}$ 
a symbol, $N$ and $A$ finite sets, and $G := 
\auf \mbox{\tt S}, N, A, R\zu$. Show that if 
$\Pf^{\ast}_R \varepsilon$ and $G$ is transparent 
then $G$ is a CFG. {\it Remark.} Transparency can 
obviously be generalized to any grammar that uses 
context free rules.
%%
\vplatz
\exercise
Show Theorem~\ref{thm:det}.
%%
\vplatz
\exercise
\label{ex:PN}
Prove Lemma~\ref{lem:zahl}. Show in addition: {\it If
$\vec{x}$ is a term then the set $P(\vec{x})
:= \{\gamma(\vec{y}) : \vec{y} \text{ is a prefix of }
\vec{x}\}$ is convex.}
%%
\vplatz
\exercise
Show the following: {\it If $L$ is deterministic then also
$L/\{\vec{x}\}$ as well as $\{\vec{x}\}\backslash L$
are deterministic.} (See Section~\ref{kap1}.\ref{kap1-2} for notation.)
%%
\vplatz
\exercise
Show that a grammar is an $LL(0)$--grammar if it generates
exactly one tree.
%%
\vplatz
\exercise
Give an example of an NTS--language which is not an
$LR(0)$--language.
