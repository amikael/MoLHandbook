\section{Rewriting Systems}
\label{kap1-5}
\label{einsdrei}
%
%
%
Languages are by Definition~\ref{defn:sprache} arbitrary sets of
strings over a (finite) alphabet. However, languages that
interest us here are those sets which can be described by finite 
means, particularly by finite processes. These can be processes 
which generate strings directly or by means of some intermediate 
structure (for example, labelled trees). The most popular approach 
is by means of rewrite systems on strings.
%%
\begin{defn}
%%%
\index{semi Thue system}%%
\index{$\Pf_{T}$, $\Pf_{T}^n$, $\Pf_{T}^{\ast}$}%%
%%%
Let $A$ be a set. A \textbf{semi Thue system}
over $A$ is a finite set $T = \{\auf \vec{x}_i,
\vec{y}_i\zu : i < m\}$ of pairs of $A$--strings.
If $T$ is given, write $\vec{u} \Pf^1_T \vec{v}$
if there are $\vec{s}, \vec{t}$ and some $i < m$
such that $\vec{u} = \vec{s} \conc \vec{x}_i \conc \vec{t}$
and $\vec{v} = \vec{s} \conc \vec{y}_i \conc \vec{t}$.
We write $\vec{u} \Pf^0_T \vec{v}$ if $\vec{u} = \vec{v}$, 
and $\vec{u} \Pf^{n+1}_T \vec{v}$ if there is a
$\vec{z}$ such that $\vec{u} \Pf^1_T \vec{z}
\Pf^n_T \vec{v}$. Finally, we write $\vec{u}
\Pf_T^{\ast} \vec{v}$ if $\vec{u} \Pf^n_T \vec{v}$
for some $n \in \omega$, and we say  that $\vec{v}$
is \textbf{derivable in $T$ from $\vec{u}$}.
\end{defn}
%%
We can define $\Pf^1_T$ also as follows.
$\vec{u} \Pf^1_T \vec{v}$ iff
there exists a context $C$ and $\auf \vec{x},
\vec{y}\zu \in T$ such that $\vec{u} = C(\vec{x})$ and
$\vec{v} = C(\vec{y})$. A semi Thue system $T$ is called 
a \textbf{Thue system}
%%%
\index{Thue system}%%
%%%
if from $\auf \vec{x}, \vec{y}\zu \in T$ follows
$\auf \vec{y}, \vec{x}\zu \in T$. In this case
$\vec{v}$ is derivable from $\vec{u}$ iff
$\vec{u}$ is derivable from $\vec{v}$.
%%%
\index{derivation}%%
%%%
A \textbf{derivation of} $\vec{y}$ \textbf{from} $\vec{x}$ 
\textbf{in} $T$ is a finite sequence $\auf \vec{v}_i : i < n+1\zu$ 
such that $\vec{v}_0 = \vec{x}$, $\vec{v}_n = \vec{y}$ and for 
all $i < n$ we have $\vec{v}_i \Pf^1_{T} \vec{v}_{i+1}$.  The 
\textbf{length} of this derivation is $n$. (A more careful 
definition will be given on Page~\pageref{derivation}.) Sometimes 
it will be convenient to admit $\vec{v}_{i+1} = \vec{v}_i$ even 
if there is no corresponding rule. 

A {\it grammar\/} differs from a semi Thue system as follows.
First, we introduce a distinction between the alphabet proper
and an auxiliary alphabet, and secondly,  the language is defined
by means of a special symbol, the so called {\it start symbol}.
%%
\begin{defn}
%%%
\index{grammar}%%
\index{start symbol}%%
\index{rule}%%
\index{symbol!terminal}%%
\index{symbol!nonterminal}%%
%%%
A \textbf{grammar} is a quadruple $G = \auf S, N, A, R\zu$
such that $N, A$ are nonempty disjoint sets, $S \in N$ and $R$ a
semi Thue system over $N \cup A$ such that
$\auf \vec{\gamma}, \vec{\eta}\zu \in R$ only if
$\vec{\gamma} \not\in A^{\ast}$. We call $S$ the
\textbf{start symbol}, $N$ the \textbf{nonterminal alphabet},
$A$ the \textbf{terminal alphabet} and $R$ the \textbf{set of rules}.
\end{defn}
%%
Elements of the set $N$ are also called \textbf{categories}. 
%%%
\index{category}%%
%%%
Notice that often the word `type' is used instead of `category', 
but this usage is dangerous for us in view of the fact that `type' 
is reserved here for types in the $\lambda$--calculus. 
As a rule, we choose $S = \mbox{\tt S}$. This is not necessary.
The reader is warned that {\tt S} need not always be the start
symbol. But if nothing else is said it is. As is common practice,
nonterminals are denoted by upper case Roman letters, terminals by
lower case Roman letters. A lower case Greek letter signifies
a letter that is either terminal or nonterminal. The use of
vector arrows follows the practice established for strings. We
write $G \vdash \vec{\gamma}$ or $\vdash_G \vec{\gamma}$ in case
%%%%
\index{$G \vdash \vec{x}$, $\vdash_G \vec{x}$}%%%
%%%%
that $S \Pf^{\ast}_R \vec{\gamma}$ and say that $G$ \textbf{generates}
$\vec{\gamma}$. Furthermore, we write $\vec{\gamma} \vdash_G
\vec{\eta}$ if $\vec{\gamma} \Pf^{\ast}_R \vec{\eta}$. The
language generated by $G$ is defined by
%%
%%%
\index{$L(G)$}%%
%%%%
\begin{equation}
L(G) := \{\vec{x} \in A^{\ast} : G \vdash \vec{x}\} 
\end{equation}
%%
Notice that $G$ generates strings which may contain terminal
as well as nonterminal symbols. However, those that contain also
nonterminals do not belong to the language that $G$ generates.
A grammar is therefore a semi Thue system which additionally 
defines how a derivation begins and how it ends.

Given a grammar $G$ we call the \textbf{analysis problem}
%%%
\index{analysis problem}%%
\index{parsing problem}%%
%%%
(or \textbf{parsing problem}) for $G$ the problem (1) to say for
a given string whether it is derivable in $G$ and (2) to name
a derivation in case that a string is derivable. The problem (1)
alone is called the \textbf{recognition problem for} $G$.
%%%
\index{recognition problem}%%
%%%

%%%
\index{production}%%
%%%
A rule $\auf \vec{\alpha}, \vec{\beta}\zu$ is often also called a
\textbf{production} and is alternatively written 
%%%
\index{$\vec{\alpha} \pf \vec{\beta}$}%%
%%%
$\vec{\alpha} \pf \vec{\beta}$. We call $\vec{\alpha}$ the 
\textbf{left hand side} and $\vec{\beta}$ the \textbf{right hand 
side} of the production. The 
%%%%
\index{productivity}%%%
%%%%
\textbf{productivity} $p(\rho)$ of a rule $\rho = \vec{\alpha} \pf %
\vec{\beta}$ is the
difference $|\vec{\beta}| - |\vec{\alpha}|$. $\rho$ is called
%%%
\index{production!expanding}%%
\index{production!strictly expanding}%%
\index{production!contracting}%%
\index{rule!terminal}%%
%%%
\textbf{expanding} if $p(\rho) \geq 0$, \textbf{strictly expanding}
if $p(\rho) > 0$ and \textbf{contracting} if $p(\rho) < 0$.
A rule is \textbf{terminal} if it has the form $\vec{\alpha} \pf
\vec{x}$ (notice that by our convention, $\vec{x} \in A^{\ast}$).

This notion of grammar is very general. There are only countably
many grammars over a given alphabet --- and hence only countably
many languages generated by them ---; nevertheless, the variety
of these languages is bewildering. We shall see that every
recursively enumerable language can be generated by some grammar.
So, some more restricted notion of grammar is called for. Noam
Chomsky has proposed the following hierarchy of grammar types.
%%%
\index{$X_{\varepsilon}$}%%
%%%
(Here, $X_{\varepsilon}$ is short for $X \cup \{\varepsilon\}$.)
%%
\begin{dinglist}{43}
%%
\index{grammar!of Type 0,1,2,3}%%
\index{grammar!context sensitive}%%
\index{grammar!context free}%%
\index{grammar!regular}%%
%%%
\item
Any grammar is of \textbf{Type 0}.
\item
A grammar is said to be of \textbf{Type 1} or \textbf{context sensitive}
if all rules are of the form $\vec{\delta}_1 X \vec{\eta}_2 \pf
\vec{\eta}_1 \vec{\alpha} \vec{\eta}_2$ and either (i) always
$\vec{\alpha} \neq \varepsilon$ or (ii) $\mbox{\tt S} \pf \varepsilon$ 
is a rule and {\tt S} never occurs on the right hand side of a production.
\item
A grammar is said to be of \textbf{Type 2} or \textbf{context free}
if it is context sensitive and all productions are of the form
$X \pf \vec{\alpha}$.
\item
A grammar is said to be of \textbf{Type 3} or \textbf{regular} if it is
context free and all productions are of the form $X \pf
\vec{\alpha}$ where $\vec{\alpha} \in A_{\varepsilon} \cdot
N_{\varepsilon}$.
\end{dinglist}
%%
A context sensitive rule $\vec{\eta}_1 X\vec{\eta}_2 \pf
\vec{\eta}_1\vec{\alpha}\vec{\eta}_2$ is also written
%%
\begin{equation}
X \pf \vec{\alpha} /\vec{\eta}_1\underline{\quad}\vec{\eta}_2
\end{equation}
%%
One says that $X$ can be rewritten into $\vec{\alpha}$ \textbf{in 
the context} $\vec{\eta}_1\underline{\quad}\vec{\eta}_2$.
A {\it language\/} is said to be of \textbf{Type} $i$ 
%%%%
\index{language!of Type 0,1,2,3}%%%
%%%
if it can be
generated by a grammar of Type $i$. It is not relevant if there
also exists a grammar of Type $j$, $j \neq i$, that generates this
language in order for it to be of Type $i$. We give examples of 
grammars of Type 3, 2 and 0.
%%%
\index{language!context free}%%%
\index{language!context sensitive}%%%
\index{language!regular}%%

{\sc Example 1.}
There are regular grammars which generate number expressions.
Here a number expression is either a number, with or without
sign, or a pair of numbers separated by a dot, again
with or without sign. The grammar is as follows. The set of
terminal symbols is $\{\mbox{\tt 0}, \mbox{\tt 1}, \mbox{\tt 2},
\mbox{\tt 3}, \mbox{\tt 4}, \mbox{\tt 5}, \mbox{\tt 6}, \mbox{\tt 7},
\mbox{\tt 8}, \mbox{\tt 9}, \mbox{\tt +}, \mbox{\tt -}, \mbox{\tt .}\}$,
the set of nonterminals is $\{\mbox{\tt V},
\mbox{\tt Z}, \mbox{\tt F}, \mbox{\tt K}, \mbox{\tt M}\}$.
The start symbol is {\tt V} and the productions are
%%
\begin{equation}
\begin{split}
\mbox{\tt V} & \pf \mbox{\tt +Z} \mid \mbox{\tt -Z} \mid 
	\mbox{\tt Z} \\
\mbox{\tt Z} & \pf \mbox{\tt 0Z} \mid \mbox{\tt 1Z} \mid 
	\mbox{\tt 2Z} \mid \dotsb \mid \mbox{\tt 9Z} \mid 
	\mbox{\tt F} \\
\mbox{\tt F} & \pf \mbox{\tt 0} \mid \mbox{\tt 1} \mid \mbox{\tt 2}
    \mid \dotsb \mid \mbox{\tt 9} \mid \mbox{\tt K} \\
\mbox{\tt K} & \pf \mbox{\tt .M} \\
\mbox{\tt M} & \pf \mbox{\tt 0M} \mid \mbox{\tt 1M} \mid 
	\mbox{\tt 2M} \mid \dotsb \mid \mbox{\tt 9M}
    \mid \mbox{\tt 0} \mid \mbox{\tt 1}
    \mid \mbox{\tt 2} \mid \dotsb \mid \mbox{\tt 9}
\end{split}
\end{equation}
%%
Here, we have used the following convention. The symbol `$\mid$' on
the right hand side of a production indicates that the part on
the left of this sign and the one to the right are alternatives.
So, using the symbol `$\mid$' saves us from writing two rules
expanding the same symbol. For example, {\tt V} can be expanded
either by {\tt +Z}, {\tt -Z} or by {\tt Z}. The syntax of the
language {\rm ALGOL} 
%%%
\index{ALGOL}%%
%%%
has been written down in this notation,
which became to be known as the \textbf{Backus--Naur Form}. The
arrow was written `$::=$'. (The Backus--Naur form actually allowed 
for context--free rules.)
%%
\index{Backus--Naur form}%%
%%%

{\sc Example 2.}
The set of strings representing terms over a finite signature with
finite set $X$ of variables can be generated by a context free
grammar. Let $F = \{\mbox{\tt F}_i : i < m\}$ and
$\Omega(i) := \Omega(\mbox{\tt F}_i)$.
%%
\begin{equation}
\mbox{\tt T} \quad\pf\quad \mbox{\tt F}_i\mbox{\tt T}^{\Omega(i)} 
\qquad (i < m)
\end{equation}
%%
Since the set of rules is finite, so must be $F$.
The start symbol is {\tt T}. This grammar generates the associated
strings in 
%%%
\index{Polish Notation}%%%
%%%
Polish Notation. Notice that this grammar reflects
exactly the structural coding of the terms. More on that later.
If we want to have dependency coding, we  have to choose instead
the following grammar.
%%
\begin{equation}
\begin{split}
\mbox{\tt S} & \pf \mbox{\tt F}_{j_0}\mbox{\tt F}_{j_1}\dotso
\mbox{\tt F}_{j_{\Omega(i)-1}} \\
\mbox{\tt F}_i & \pf \mbox{\tt F}_{j_0}\mbox{\tt F}_{j_1}\dotso
\mbox{\tt F}_{j_{\Omega(i)-1}}
\end{split}
\end{equation}
%%
This is a scheme of productions. Notice that for technical reasons 
the root symbol must be {\tt S}. We could dispense with the first 
kind of rules if we are allowed to have several start symbols. We 
shall return to this issue below.

{\sc Example 3.}
Our example for a Type 0 grammar is the following, taken from
\cite{salomaa:formal}.
%%
\begin{equation}
\begin{array}{l@{\qquad}r@{\quad \pf \quad}l@{\qquad\qquad}%
r@{\quad \pf \quad}l}
{\rm (a)} & \mbox{\tt X}_{\snull}   & \mbox{\tt a}
    & \mbox{\tt X}_{\snull}   & \mbox{\tt aXX}_{\szwei} \mbox{\tt Z} \\
{\rm (b)} & \mbox{\tt X}_{\szwei} \mbox{\tt Z} & \mbox{\tt aa}
    & \multicolumn{2}{c}{} \\
{\rm (c)} & \mbox{\tt Xa}    & \mbox{\tt aa}
    & \mbox{\tt Ya}    & \mbox{\tt aa} \\
{\rm (d)} & \mbox{\tt X}_{\szwei} \mbox{\tt Z} & \mbox{\tt Y}_{\seins} \mbox{\tt YXZ}
    & \multicolumn{2}{c}{} \\
{\rm (e)} & \mbox{\tt XY}_{\seins} & \mbox{\tt Y}_{\seins} \mbox{\tt YX}
    & \mbox{\tt YY}_{\seins} & \mbox{\tt Y}_{\seins} \mbox{\tt Y} \\
%{\rm (f)} & \mbox{\tt XY}_{\seins} & \mbox{\tt X}_{\seins} \mbox{\tt Y}
%    & \mbox{\tt YY}_{\seins} & \mbox{\tt Y}_{\seins} \mbox{\tt Y} \\
{\rm (f)} & \mbox{\tt aY}_{\seins} & \mbox{\tt aXXYX}_{\szwei}
    & \multicolumn{2}{c}{} \\
{\rm (g)} & \mbox{\tt X}_{\szwei}\mbox{\tt Y}  & \mbox{\tt XY}_{\szwei}
    & \mbox{\tt Y}_{\szwei} \mbox{\tt Y} & \mbox{\tt YY}_{\szwei} \\
\multicolumn{3}{c}{}
    & \mbox{\tt Y}_{\szwei} \mbox{\tt X} & \mbox{\tt YX}_{\szwei}
\end{array}
\end{equation}
%%
$\mbox{\tt X}_{\snull}$ is the start symbol. This grammar generates
the language $\{\mbox{\tt a}^{n^2} : n > 0\}$. This can be seen
as follows. To start, with (a) one can either generate the string
{\tt a} or the string $\mbox{\tt aXX}_{\szwei}\mbox{\tt Z}$. Let
$\vec{\gamma}_i = \mbox{\tt aX}\vec{\delta}_i %
\mbox{\tt X}_{\szwei} \mbox{\tt Z}$,
$\vec{\delta}_i \in \{\mbox{\tt X}, \mbox{\tt Y}\}^{\ast}$.
We consider derivations which go from $\vec{\gamma}_i$ to a
terminal string. At the beginning, only (b) or (d) can be applied.
Let it be (b). Then we can only continue with (c) and then we
create a string of length $4 + |\vec{\delta}_i|$. Since we
have only one letter, the string is uniquely determined.
Now assume that (d) has been chosen. Then we get the string
$\mbox{\tt aX}\vec{\delta}_i\mbox{\tt Y}_{\seins} \mbox{\tt YXZ}$.
The only possibility to continue is using (e). This moves the
index 1 stepwise to the left and puts {\tt Y} before every occurrence
of an {\tt X}. Finally, it hits {\tt a} and we use (f) to get 
$\mbox{\tt aXXYX}_{\szwei} \vec{\delta}'_i\mbox{\tt YYXZ}$. Now there 
is no other choice but to move the index 2 to the right with the help 
of (g). This gives a string $\vec{\gamma}_{i+1} = 
\mbox{\tt aX}\vec{\delta}_{i+1}\mbox{\tt X}_{\szwei}\mbox{\tt Z}$ with 
$\vec{\delta}_{i+1} = \mbox{\tt XYX}\vec{\delta}'_i\mbox{\tt YY}$. 
We have
%%
\begin{equation}
|\vec{\delta}_{i+1}| = |\vec{\delta}_i| + \ell_x(\vec{\delta}_i)
+ 5
\end{equation}
%%
where $\ell_x(\delta_i)$ counts the number of {\tt X} in
$\vec{\delta}_i$. Since $\ell_x(\vec{\delta}_{i+1}) = \ell_x(\vec{\delta}_i)
+ 2$, $\vec{\delta}_0 = \varepsilon$, we conclude that 
$\ell_x(\vec{\delta}_i) = 2i$ and so $|\vec{\delta}_i| = (i+1)^2 - 4$, 
$i > 0$.  Hence, $|\vec{\gamma}_i| = (i+1)^2$, as promised. 

In the definition of a context sensitive grammar the following
must be remembered. By intention, context sensitive grammars only
consist of noncontracting rules. However, since we must begin with a
start symbol, there would be no way to derive the empty string if
no rule is contracting. Hence, we do admit the rule $\mbox{\tt S} %
\pf \varepsilon$. But in order not to let other contracting uses of 
this rule creep in we require that {\tt S} is not on the right hand 
side of any rule whatsoever. Hence, $\mbox{\tt S} \pf \varepsilon$ 
can only be applied once, at the beginning of the derivation. The 
derivation immediately terminates. This condition is also in force 
for context free and regular grammars although without it no more 
languages can be generated (see the exercises). For assume that in a 
grammar $G$ with rules of the form $X \pf \vec{\alpha}$ there
are rules where {\tt S} occurs on the right hand side of a production,
and nevertheless replace {\tt S} by $Z$ in all rules which are not not
of the form $\mbox{\tt S} \pf \varepsilon$. Add also all rules 
$\mbox{\tt S} \pf \vec{\alpha}'$, where $\mbox{\tt S} \pf \vec{\alpha}$ 
is a rule of $G$ and $\vec{\alpha}'$ results from $\vec{\alpha}$ by 
replacing {\tt S} by $Z$. This is a context free grammar which generates 
the same language, and even the same structures. (The only difference 
is with the nodes labelled {\tt S} or $Z$.)

%%%
\index{RG, CFG, CSG, GG}%%
\index{RL, CFL, CSL, GL}%%
%%%
The class of regular grammars is denoted by RG,  the class of all
context free grammars by CFG, the class of context sensitive
grammars by CSG and the class of Type 0 grammars by GG.
The languages generated by these grammars is analogously denoted by
RL, CFL, CSL and GL. The grammar classes form a
proper hierarchy.
%%
\begin{equation}
\mbox{\rm RG} \subsetneq \mbox{\rm CFG} \subsetneq \mbox{\rm CSG}
\subsetneq \mbox{\rm GG}
\end{equation}
%%
This is not hard to see. It follows immediately that the languages
generated by these grammar types also form a hierarchy, but not 
that the inclusions are proper. However, the hierarchy is once again 
strict.
%%
\begin{equation}
\mbox{\rm RL} \subsetneq \mbox{\rm CFL} \subsetneq \mbox{\rm CSL}
\subsetneq \mbox{\rm GL}
\end{equation}
%%
We shall prove each of the proper inclusions. In 
Section~\ref{kap1}.\ref{einsfuenf}
(Theorem~\ref{0-1-echt}) we shall show that there are languages of
Type 0 which are not of Type 1. Furthermore, from the Pumping Lemma
(Theorem~\ref{thm:pumplemma}) for CFLs it follows
that $\{\mbox{\tt a}^n \mbox{\tt b}^n \mbox{\tt c}^n : n \in \omega\}$
is not context free. However, it is context sensitive (which is left
as an exercise in that section). Also, by Theorem~\ref{thm:noncontracting} 
below, the language $\{\mbox{\tt a}^{n^2} : n \in \omega\}$ has a grammar 
of Type 1. However, this language is not semilinear, whence it is not 
of Type 2 (see Section~\ref{kap2}.\ref{kap2-5}). Finally, it will be shown that
$\{\mbox{\tt a}^n \mbox{\tt b}^n : n \in \omega\}$
is context free but not regular.  (See Exercise~\ref{ue:index}.)

Let $\rho = \vec{\gamma} \pf \vec{\eta}$.
We call a triple $A = \auf \vec{\alpha}, C, \vec{\zeta}\zu$
%%%
\index{rule!instance of a \faul}%%
%%%
an \textbf{instance of} $\rho$ if $C$ is an occurrence of
$\vec{\gamma}$ in $\vec{\alpha}$ and also an occurrence of
$\vec{\eta}$ in $\vec{\zeta}$.  This means that there exist 
$\vec{\kappa}_1$ and $\vec{\kappa}_2$ such that
$C = \auf \vec{\kappa}_1, \vec{\kappa}_2\zu$ and
$\vec{\alpha} = \vec{\kappa}_1 \conc \vec{\gamma}
\conc \vec{\kappa}_2$ as well as $\vec{\zeta} =
\vec{\kappa}_1 \conc \vec{\eta} \conc \vec{\kappa}_2$.
%%%
\index{rule instance!domain of a \faul}%%
\label{derivation}
%%%
We call $C$ the \textbf{domain of} $A$. A \textbf{derivation of length} 
$n$ is a sequence $\auf A_i : i < n\zu$ of {\it instances\/} of 
rules from $G$ such that $A_i = \auf \vec{\alpha}_i, C_i,
\vec{\zeta}_i\zu$ for $i < n$ and for every $j < n-1$ 
$\vec{\alpha}_{j+1} = \vec{\zeta}_j$. $\vec{\alpha}_0$ is 
called the \textbf{start} of the derivation, $\vec{\zeta}_{n-1}$ 
the \textbf{end}. 
%%%
\index{derivation}%%
\index{derivation!start of a \faul}%%
\index{derivation!end of a \faul}%%
\index{$\der(G,\vec{\alpha})$, $\der(G)$}%%%
%%%
We denote by $\der(G,\vec{\alpha})$ the set of derivations
$G$ from the string $\vec{\alpha}$ and $\der(G) := \der(G,S)$. 

This definition has been carefully chosen. Let $\auf A_i : i < n\zu$
be a derivation in $G$, where $A_i = \auf \vec{\alpha}_i, C_i, 
\vec{\alpha}_{i+1}\zu$ ($i < n$). Then we call
$\auf \vec{\alpha}_i : i < n+1\zu$ the (\textbf{associated})
%%%
\index{string sequence}%%
\index{string sequence!associated}%%
%%%%
\textbf{string sequence}. Notice that the string sequence has one
more element than the derivation. In what is to follow we shall 
often also call the string sequence a derivation. However, this is not
quite legitimate, since the string sequence does not determine the
derivation uniquely. Here is an example. Let $G$ consist of the rules
$\mbox{\tt S} \pf \mbox{\tt AB}$, $\mbox{\tt A} \pf \mbox{\tt AA}$ 
and $\mbox{\tt B} \pf \mbox{\tt AB}$.  Take the string sequence 
$\auf \mbox{\tt S}, \mbox{\tt AB}, \mbox{\tt AAB}\zu$. There are 
two derivations for this sequence. 
%%
\begin{subequations}
\begin{align}
& \auf\auf \mbox{\tt S}, \auf \varepsilon, \varepsilon\zu,
    \mbox{\tt AB}\zu, \auf \mbox{\tt AB},
    \auf \varepsilon, \mbox{\tt B}\zu, \mbox{\tt AAB}\zu\zu \\
& \auf\auf \mbox{\tt S}, \auf \varepsilon, \varepsilon\zu, 
	\mbox{\tt AB}\zu, \auf \mbox{\tt AB}, \auf \mbox{\tt A}, 
	\varepsilon\zu, \mbox{\tt AAB}\zu\zu
\end{align}
\end{subequations}
%%
After application of a rule $\rho$, the left hand side $\vec{\gamma}$
is replaced by the right hand side, but the context parts $\vec{\kappa}_1$
and $\vec{\kappa}_2$ remain as before. It is intuitively clear that
if we apply a rule to parts of the context, then this application
could be permuted with the first. This is clarified in the following
definition and theorem.
%%
\begin{defn}
%%%%
\index{domains!disjoint}%%
%%%%
Let $\auf \vec{\alpha}, \auf \vec{\kappa}_1, \vec{\kappa}_2\zu,
\vec{\beta}\zu$ be an instance of the rule 
$\rho = \vec{\eta} \pf \vec{\vartheta}$, and let 
$\auf \vec{\beta}, \auf \vec{\mu}_1, \vec{\mu}_2\zu, \vec{\gamma}\zu$
be an instance of $\sigma = \vec{\zeta} \pf \vec{\xi}$. We call the
domains of these applications \textbf{disjoint} if either
(a) $\vec{\kappa}_1\conc\vec{\vartheta}$ is a prefix
of $\vec{\mu}_1$ or (b) $\vec{\vartheta}\conc \vec{\kappa}_2$ is
a suffix of $\vec{\mu}_2$.
\end{defn}
%%%
\begin{lem}[Commuting Instances]
%%%
\index{Commuting Instances Lemma}%%%
%%%
\label{vertausch}
Let $\auf \vec{\alpha}, C, \vec{\beta}\zu$ be an instance of 
$\rho = \vec{\eta} \pf \vec{\vartheta}$, and 
$\auf \vec{\beta}, D, \vec{\gamma}\zu$
an instance of $\sigma = \vec{\zeta} \pf \vec{\xi}$. Suppose that the
instances are disjoint. Then there exists an instance
$\auf \vec{\alpha}, D', \vec{\delta}\zu$ of $\sigma$ as well as an
instance $\auf \vec{\delta}, C', \vec{\gamma}\zu$ of $\rho$,
and both have disjoint domains.
\end{lem}
%%
The proof is easy and left as an exercise. Analogously, suppose that
to the {\it same\/} string the rule $\rho$ can be applied with context 
$C$ and the rule $\sigma$ can be applied with context $D$. Then if 
$C$ precedes $D$, after applying one of them the domains remain 
disjoint, and the other can still be applied (with the context modified 
accordingly).

We give first an example where the instances are not disjoint. Let the 
following rules be given.
%%
\begin{equation}
\begin{array}{l@{\quad\pf\quad}l@{\qquad\qquad}l@{\quad\pf\quad}l}
\mbox{\tt AX} & \mbox{\tt XA} & \mbox{\tt XA} & \mbox{\tt Xa} \\
\mbox{\tt XB} & \mbox{\tt Xb} & \mbox{\tt Xa} & \mbox{\tt a}
\end{array}
\end{equation}
%%
There are two possibilities to apply the rules to {\tt AXB}.
The first has domain $\auf \varepsilon, \mbox{\tt B}\zu$,
the second the domain $\auf \mbox{\tt A}, \varepsilon\zu$. The
domains overlap and indeed the first rule when applied destroys
the domain of the second. Namely, if we apply the rule
$\mbox{\tt AX} \pf \mbox{\tt XA}$ we cannot reach a terminal
string.
%%
\begin{equation}
\mbox{\tt AXB} \Pf \mbox{\tt XAB} \Pf \mbox{\tt XaB}
\end{equation}
%%
If on the other hand we first apply the rule 
$\mbox{\tt XB} \pf \mbox{\tt Xb}$ we do get one.
%%
\begin{equation}
\mbox{\tt AXB} \Pf \mbox{\tt AXb} \Pf \mbox{\tt XAb}
 \Pf \mbox{\tt Xab} \Pf \mbox{\tt ab}
\end{equation}
%%
So much for noncommuting instances. Now take the string {\tt AXXB}.
Again, the two rules are in competition. However, this time none
destroys the applicability of the other.
%%
\begin{equation}
\mbox{\tt AXXB} \Pf \mbox{\tt AXXb} \Pf \mbox{\tt XAXb}
\end{equation}
%%
\begin{equation}
\mbox{\tt AXXB} \Pf \mbox{\tt XAXB} \Pf \mbox{\tt XAXb}
\end{equation}
%%
As before we can derive the string {\tt ab}.
Notice that in a CFG every pair of rules that are
in competition for the same string can be used in succession
with either order on condition that they do not compete for the
same occurrence of a nonterminal.
%%%
\begin{defn}
%%%
\index{standard form}%%%
%%%
A grammar is in \textbf{standard form} if all rules are of the
form $\vec{X} \pf \vec{Y}$, $X \pf \vec{x}$.
\end{defn}
%%%
In other words, in a grammar in standard form the right hand side 
either consists of a string of nonterminals or a string of terminals. 
Typically, one restricts terminal strings to a single symbol or the 
empty string, but the difference between these requirements is 
actually marginal.
%%%
\begin{lem}
For every grammar $G$ of Type $i$ there exists a grammar $H$ of
Type $i$ in standard form such that $L(G) = L(H)$.
\end{lem}
%%%
\proofbeg
Put $N' := \{\mbox{\tt N}_a : a \in A\} \cup N$ and $h :
a \mapsto \mbox{\tt N}_a, X \mapsto X : N \cup A \pf N^1$.
For each rule $\rho$ let $h(\rho)$ be the result of
applying $\oli{h}$ to both strings. Finally, let
$R' := \{h(\rho) : \rho \in R\} \cup \{\mbox{\tt N}_a \pf a : a
\in A\}$, $H := \auf \mbox{\tt S}, N', A, R'\zu$. It is easy to 
verify, using the Commuting Instances 
%%%
\index{Commuting Instances Lemma}%%%
%%%
Lemma, that $L(H) = L(G)$. (See also 
below for proofs of this kind.) 
\proofend

We shall now proceed to show that the conditions on Type 0 grammars
are actually insignificant as regards the class of generated languages.
First, we may assume a set of start symbols rather than a single one.
%%%
\index{grammar$^{\ast}$}%%
%%%%
Define the notion of a \textbf{\bf grammar}$^{\ast}$ (\textbf{of Type} 
$i$) to be a quadruple $G = \auf \Sigma, N, A, R\zu$ such that 
$\Sigma \subseteq N$ and for all $S \in \Sigma$, $\auf S, N, A, R\zu$ 
is a grammar (of Type $i$). Write $G \vdash \vec{\gamma}$ if there is 
an $S \in \Sigma$ such that $S \Pf^{\ast}_R \vec{\gamma}$. We shall 
see that grammars$^{\ast}$
are not more general than grammars with respect to languages.
Let $G$ be a grammar$^{\ast}$. Define $G^{\heartsuit}$ as follows.
Let $S^{\heartsuit} \not\in A \cup N$ be a new nonterminal and
add the rules $S^{\heartsuit} \pf X$ to $R$ for all
$X \in \Sigma$. It is easy to see that $L(G^{\heartsuit}) = L(G)$.
(Moreover, the derivations differ minimally.) Notice also that
we have not changed the type of the grammar.

The second simplification concerns the requirement that the set of
terminals and the set of nonterminals be disjoint. We shall show
that it too can be dropped without increasing the generative power.
We shall sometimes work without this condition, as it can be
cumbersome to deal with.
%%
\begin{defn}
%%%
\index{quasi--grammar}%%
%%%
A \textbf{quasi--grammar} is a quadruple $\auf \mbox{\tt S}, N, A, R\zu$
such that $A$ and $N$ are finite and nonempty sets, $\mbox{\tt S} \in N$,
and $R$ a semi Thue system over $N \cup A$ such that if
$\auf \vec{\alpha}, \vec{\beta}\zu \in R$ then
$\vec{\alpha}$ contains a symbol from $N$.
\end{defn}
%%
%%
\begin{prop}
For every quasi--grammar there exists a grammar which generates
the same language.
\end{prop}
%%
\proofbeg
Let $\auf \mbox{\tt S}, N, A, R\zu$ be a quasi--grammar.
Put $N_1 := N \cap A$. Then assume for every
$a \in N_1$ a new symbol $\mbox{\tt Y}_a$.
Put $Y := \{\mbox{\tt Y}_a : a \in N_1\}$,
$N^{\circ} := (N - N_1) \cup Y$, $A^{\circ} := A$.
Now $N^{\circ} \cap A^{\circ} = \varnothing$.
We put $\mbox{\tt S}^{\circ} := \mbox{\tt S}$
if $\mbox{\tt S} \not\in A$ and $\mbox{\tt S}^{\circ}
:= \mbox{\tt Y}_{\tt S}$ if $\mbox{\tt S} \in A$. Finally, we
define the rules. Let $\vec{\alpha}^{\circ}$ be the result of
replacing every occurrence of an $a \in N_1$ by
the corresponding $\mbox{\tt Y}_a$. Then let
%%
\begin{equation}
R^{\circ} := \{\vec{\alpha}^{\circ} \pf \vec{\beta}^{\circ} :
\vec{\alpha} \pf \vec{\beta} \in R\} \cup
\{\mbox{\tt Y}_a \pf a : a \in N_1\} 
\end{equation}
%%
Put $G^{\circ} := \auf \mbox{\tt S}^{\circ}, N^{\circ}, A^{\circ},
R^{\circ}\zu$. We claim that $L(G^{\circ}) = L(G)$.
To that end we define a homomorphism
$h \colon (A \cup N)^{\ast} \pf (A^{\circ} \cup N^{\circ})^{\ast}$
by $h(a) := a$ for $a \in A - N_1$, $h(a) := \mbox{\tt Y}_a$ for
$a \in N_1$ and $h(X) := X$ for all $X \in N - N_1$. Then
$h(\mbox{\tt S}) = \mbox{\tt S}^{\circ}$ as well as $h(R) %
\subseteq R^{\circ}$. From this it immediately follows that 
if $G \vdash \vec{\alpha}$ then $G^{\circ} \vdash h(\vec{\alpha})$. 
(Induction on the length of a derivation.)  
Since we can derive $\vec{\alpha}$ in $G^{\circ}$ from 
$h(\vec{\alpha})$, we certainly have $L(G) \subseteq L(G^{\circ})$. 
For the converse we have to convince ourselves that an instance of 
a rule $\mbox{\tt Y}_a \pf a$ can always be moved to the end of 
the derivation. For if
$\vec{\alpha} \pf \vec{\beta}$ is a rule then it is of type
$\mbox{\tt Y}_b \pf b$ and replaces a  $\mbox{\tt Y}_b$ by $b$;
and hence it commutes with that instance of the first rule.
Or it is of a different form, namely $\vec{\alpha}^{\circ} \pf %
\vec{\beta}^{\circ}$; since $a$ does not occur in $\vec{\alpha}^{\circ}$,
these two instances of rules commute. Now that this is shown,
we conclude from $G^{\circ} \vdash \vec{\alpha}$ already
$G^{\circ} \vdash \vec{\alpha}^{\circ}$.
This implies $G \vdash \vec{\alpha}$.
\proofend

The last of the conditions, namely that the left hand side of a
production must contain a nonterminal, is also no restriction.
For let $G = \auf \mbox{\tt S}, N, A, R\zu$ be a grammar which does
not comply with this condition. Then for every terminal $a$
let $a^1$ be a new symbol and let $A^1 := \{a^1 : a \in A\}$.
Finally, for each rule $\rho = \vec{\alpha} \pf \vec{\beta}$
let $\rho^1$ be the result of replacing every occurrence of
an $a \in A$ by $a^1$ (on every side of the production).
Now set $\mbox{\tt S}':= \mbox{\tt S}$ if $\mbox{\tt S} \not\in A$
and $\mbox{\tt S}' := \mbox{\tt S}^{\seins}$ otherwise, 
$R' := \{\rho^1 : \rho \in R\} \cup \{a^{\seins} \pf a : a \in A\}$.
Finally put $G' := \auf \mbox{\tt S}', N \cup A^1, A, R'\zu$.
It is not hard to show that $L(G') = L(G)$. These steps have 
simplified the notion of a grammar considerably. Its most general 
form is $\auf \Sigma, N, A, R\zu$, where $\Sigma \subseteq 
N$ is the set of start symbols and $R \subseteq (N \cup A)^{\ast} 
\times (N \cup A)^{\ast}$ a finite set.

%%%
\index{grammar!noncontracting}%%
%%%
Next we shall show a general theorem for context sensitive languages.
A grammar is called \textbf{noncontracting} if either no rule is
contracting or only the rule $\mbox{\tt S} \pf \varepsilon$ is contracting
and in this case the symbol {\tt S} never occurs to the right of a
production. Context sensitive grammars are contracting. However, 
not all noncontracting grammars are context sensitive. It turns out, 
however, that {\it all\/} noncontracting grammars generate context 
sensitive languages. (This can be used also to show that the context 
sensitive languages are exactly those languages that are recognized 
by a linearly space bounded Turing machine.)
%%
\begin{thm}
\label{thm:noncontracting}
A language is context sensitive iff there is a noncontracting
grammar that generates it.
\end{thm}
%%
\proofbeg
($\Pf$) Immediate. ($\Leftarrow$) Let $G$ be a noncontracting grammar. 
We shall construct a grammar $G^{\spadesuit}$ which is context sensitive 
and such that $L(G^{\spadesuit}) = L(G)$. To this end, let
$\rho = X_0 X_1 \dotsb X_{m-1} \pf Y_0 Y_1 \dotsb Y_{n-1}$, $m \leq n$, 
be a production. (As remarked above, we can reduce
attention to such rules and rules of the form $X \pf a$. Since the
latter are not contracting, only the former kind needs attention.)
We assume $m$ new symbols, $Z_0$, $Z_1, \dotsc, Z_{m-1}$. Let 
$\rho^{\spadesuit}$ be the following set of rules. 
%%
\begin{equation}
\begin{split}
X_0 X_1 \dotsb X_{m-1} & \pf Z_0 X_1 \dotsb X_{m-1} \\
Z_0 X_1 X_2 \dotsb X_{m-1} & \pf Z_0 Z_1 X_2 \dotsb X_{m-1} \\
     & \dotso \\
Z_0 Z_1 \dotsb Z_{m-2} X_{m-1} & \pf Z_0 Z_1 \dotsb Z_{m-1} \\
Z_0 Z_1 \dotsb Z_{m-1} & \pf Y_0 Z_1 \dotsb Z_{m-1} \\
Y_0 Z_1 Z_2 \dotsb Z_{m-1} & \pf Y_0 Y_1 Z_2 \dotsb Z_{m-1} \\
     & \dotso \\
Y_0 Y_1 \dotsb Y_{m-2} Z_{m-1} & \pf Y_0 Y_1 \dotsb Y_{n-1}
\end{split}
\end{equation}
%%
Let $G^{\spadesuit}$ be the result of replacing all non context 
sensitive rules $\rho$ by $\rho^{\spadesuit}$. The new grammar is
context sensitive. Now let us be given a derivation in $G$.
Then replace every instance of a rule $\rho$ by the given
sequence of rules in $\rho^{\spadesuit}$. This gives a derivation
of the same string in $G^{\spadesuit}$. Conversely, let us be
given a derivation in $G^{\spadesuit}$. Now look at the following.
If somewhere the rule $\rho^{\spadesuit}$ is applied, and then a
rule from $\rho_{1}^{\spadesuit}$ then the instances commute unless
$\rho_{1} = \rho$ and the second instance is inside that of
that rule instance of $\rho^{\spadesuit}$. Thus, by suitably
reordering the derivation is a sequence of segments, where each
segment is a sequence of the rule $\rho^{\spadesuit}$ for some
$\rho$, so that it begins with $\vec{X}$ and ends with
$\vec{Y}$. This can be replaced by $\rho$. Do this for every segment.
This yields a derivation in $G$.
\proofend

Given that there are Type 0 languages that are not Type 0 
(Theorem~\ref{0-1-echt}) the following theorem shows that the 
languages of Type 1 are not closed under arbitrary homomorphisms. 
%%
\begin{thm}
\label{thm:erase}
Let $\mbox{\tt a}, \mbox{\tt b} \not\in A$ be (distinct) symbols.
For every language $L$ over $A$ of Type 0 there is a language
$M$ over $A \cup \{\mbox{\tt a}, \mbox{\tt b}\}$ of Type 1 such that
for every $\vec{x} \in L$ there is an $i$ with
$\mbox{\tt a}^i \mbox{\tt b} \vec{x} \in M$ and
every $\vec{y} \in M$ has the form $\mbox{\tt a}^i \mbox{\tt b} \vec{x}$
with $\vec{x} \in L$.
\end{thm}
%%
\proofbeg
We put $N^{\clubsuit} := N \cup \{\mbox{\tt A}, \mbox{\tt B},
\mbox{\tt S}^{\clubsuit}\}$. Let 
%%
\begin{equation}
\rho = X_0 X_1 \dotsb X_{m-1} \pf Y_0 Y_1 \dotsb Y_{n-1}
\end{equation}
%%
be a contracting rule. Then put 
%%
\begin{equation}
\rho^{\clubsuit} := X_0 X_1 \dotsb X_{m-1} \pf \mbox{\tt A}^{m-n} 
Y_0 Y_1 \dotsb Y_{n-1}
\end{equation}
%%
$\rho^{\clubsuit}$ is certainly not contracting. If
$\rho$ is not contracting then put $\rho^{\clubsuit} :=
\rho$. Let $R^{\clubsuit}$ consist of all rules of the form
$\rho^{\clubsuit}$ for $\rho \in R$ as well as the following rules.
%%
\begin{equation}
\begin{split}
\mbox{\tt S}^{\clubsuit}  & \pf \mbox{\tt BS}  \\
X \mbox{\tt A} & \pf \mbox{\tt A}X \qquad (X \in N^{\clubsuit})  \\
\mbox{\tt BA} & \pf \mbox{\tt aB} \\
\mbox{\tt B}  & \pf \mbox{\tt b}
\end{split}
\end{equation}
%%
Let $M := L(G^{\clubsuit})$. Certainly, $\vec{y} \in M$ only if
$\vec{y} = \mbox{\tt a}^i \mbox{\tt b} \vec{x}$ for some
$\vec{x} \in A^{\ast}$. For strings contain {\tt B} (or {\tt b})
only once. Further, {\tt A} can be changed into {\tt a} only if
it occurs directly before {\tt B}. After that we get {\tt B} followed
by {\tt a}. Hence {\tt b} must occur after all occurrences of
{\tt a} but before all occurrences of {\tt B}. Now consider the
homomorphism $\oli{v}$ defined by $v \colon
\mbox{\tt A}, \mbox{\tt a}, \mbox{\tt B}, \mbox{\tt b},
\mbox{\tt S}^{\clubsuit} \mapsto \varepsilon$ and
$v \colon X \mapsto X$ for $X \in N$, $v \colon a \mapsto a$ for
$a \in A$. If $\auf \vec{\alpha}_i : i < n\zu$ is a
derivation in $G^{\clubsuit}$ then
$\auf \oli{v}(\vec{\alpha}_i) : 0 < i < n\zu$ is a derivation
in $G$ (if we disregard repetitions).  In this way one shows
that $\mbox{\tt a}^i \mbox{\tt b} \vec{x} \in M$
implies $\vec{x} \in L(G)$. Next, let $\vec{x} \in L(G)$.
Let $\auf \vec{\alpha}_i : i < n\zu$ be a derivation of
$\vec{x}$ in $G$. Then do the following. Define
$\vec{\beta}_0 := S^{\clubsuit}$ and $\vec{\beta}_1
= \mbox{\tt BS}$. Further, let $\vec{\beta}_{i+1}$ be of the form
$\mbox{\tt BA}^{k_i}\vec{\alpha}_i$ for some $k_i$ which is
determined inductively. It is easy to see that $\vec{\beta}_{i+1}
\vdash_{G^{\clubsuit}} \vec{\beta}_{i+2}$, so that
one can complete the sequence  $\auf \vec{\beta}_i : i < n+1\zu$
to a derivation. From $\mbox{\tt BA}^{k_n} \vec{x}$ one can derive
$\mbox{\tt a}^{k_n} \mbox{\tt b} \vec{x}$.
This shows that $\mbox{\tt a}^{k_n} \mbox{\tt b} \vec{x} \in M$, 
as desired.
\proofend
%%

Now let $v \colon A \pf B^{\ast}$ be a map. $v$ (as well as the generated
homomorphism $\oli{v}$) is called $\varepsilon$--\textbf{free}
%%%
\index{homomorphism!$\varepsilon$--free}%%
%%%
if $v(a) \neq \varepsilon$ for all $a \in A$.
%%
\begin{thm}
\label{thm:afl}
Let $L_1$ and  $L_2$ be languages of Type $i$, $0 \leq i \leq 3$.
Then the following are also languages of Type $i$.
%%
\begin{dingautolist}{192}
%%
\item $L_1 \cup L_2$, $L_1 \cdot L_2$, $L_1^{\ast}.$
\item $\oli{v}[L_1]$, where $v$ is $\varepsilon$--free.
\end{dingautolist}
%%
If $i \neq 1$ then  $\oli{v}[L_1]$ also is of Type $i$ even if
$v$ is not $\varepsilon$--free.
%%
\end{thm}
%%
\proofbeg
Before we begin, we remark the following. If $L \subseteq A^{\ast}$
is a language and $G = \auf \mbox{\tt S}, N, A, R\zu$ a grammar over $A$
which generates $L$ then for an arbitrary $B \supseteq A$
$\auf \mbox{\tt S}, N, B, R\zu$ is a  grammar over $B$ which generates $L
\subseteq B^{\ast}$. Therefore we may now assume that
$L_1$ and $L_2$ are languages over the same alphabet.
\ding{192} is seen as follows. We have $G_1 =
\auf \mbox{\tt S}_1, N_1, A, R_1\zu$ and $G_2 = \auf \mbox{\tt S}_2,
N_2, A, R_2\zu$ with $L(G_1) = L(G_2)$. By renaming the nonterminals
of $G_2$ we can see to it that $N_1 \cap N_2 = \varnothing$.
Now we put $N_3 := N_1 \cup N_2 \cup \{\mbox{\tt S}^{\diamondsuit}\}$
(where $\mbox{\tt S}^{\diamondsuit} \not\in N_1 \cup N_2$)
and $R := R_1 \cup R_2 \cup \{\mbox{\tt S}^{\diamondsuit} \pf \mbox{\tt S}_1,
\mbox{\tt S}^{\diamondsuit} \pf \mbox{\tt S}_2\}$. This defines
$G_3 := \auf \mbox{\tt S}^{\diamondsuit}, N_3, A, R_3\zu$.
This is a grammar which generates $L_1 \cup L_2$. We introduce a new 
start symbol
$\mbox{\tt S}^{\times}$ together with the rules $\mbox{\tt S}^{\times} \pf
\mbox{\tt S}_1 \mbox{\tt S}_2$ where $\mbox{\tt S}_1$
is the  start symbol of $G_1$ and $G_2$ the start symbol of $G_2$.
This yields a grammar of Type $i$ except if $i = 3$.
In this case the fact follows from the results of
Section~\ref{kap2}.\ref{zweieins}. It is however not difficult to construct
a grammar which is regular and generates the language
$L_1 \cdot L_2$.  Now for $L_1^{\ast}$. Let {\tt S} be the
start symbol for a grammar $G$ which generates $L_1$.
Then introduce a new symbol $\mbox{\tt S}^+$ as well as a new
start symbol $\mbox{\tt S}^{\ast}$ together with the rules
%%
\begin{equation}
\begin{split}
\mbox{\tt S}^{\ast} & \pf \varepsilon \mid  \mbox{\tt S}
    \mid  \mbox{\tt S}\mbox{\tt S}^+ \\
\mbox{\tt S}^+      & \pf \mbox{\tt S} \mid \mbox{\tt SS}^+
\end{split}
\end{equation}
%%
This grammar is of Type $i$ and generates $L_1^{\ast}$.
(Again the case $i = 3$ is an exception that can be dealt with
in a  different way.) Finally, \ding{193}. Let $v$ be 
$\varepsilon$--free. We extend it by putting $v(X) := X$ for all 
nonterminals $X$. Then replace the rules $\rho = \vec{\alpha} \pf \vec{\beta}$
by $\oli{v}(\rho) := \oli{v}(\vec{\alpha}) \pf \oli{v}(\vec{\beta})$.
If $i = 0, 2$, this does not change the type. If $i = 1$ we must
additionally require that $v$ is $\varepsilon$--free.
For if $\vec{\gamma} X \vec{\delta} \pf
\vec{\gamma} \vec{\alpha} \vec{\delta}$ is a rule
and $\vec{\alpha}$ is a terminal string we may have
$\oli{v}(\alpha) = \varepsilon$. This is however not the case
if $v$ is $\varepsilon$--free. If $i = 3$ again a different
method must be used.  For now --- after applying the replacement
--- we have rules of the form $X \pf \vec{x} Y$ and $X \pf \vec{x}$,
$\vec{x} = x_0 x_1 \dotsb x_{n-1}$. Replace the latter by $X \pf
x_0 Z_0$, $Z_i \pf x_i Z_{i+1}$ and $Z_{n-2} \pf x_{n-1} Y$ and
$Z_{n-2} \pf x_{n-1}$, respectively. \proofend
%%
\begin{defn}
%%%
\index{abstract family of languages}%%
\index{AFL}%%
%%%%
Let $A$ be a (possibly infinite) set. A nonempty set $\CS \subseteq 
\wp(A^{\ast})$ is called an \textbf{abstract family of languages} 
(\textbf{AFL}) \textbf{over} $A$ if the following holds.
%%
\begin{dingautolist}{192}
\item For every $L \in \CS$ there is a finite
    $B \subseteq A$ such that $L \subseteq B^{\ast}$.
\item If $h \colon A^{\ast} \pf A^{\ast}$ is a homomorphism
    and $L \in \CS$ then also $h[L] \in \CS$.
\item If $h \colon A^{\ast} \pf A^{\ast}$ is a homomorphism
    and $L \in \CS$, $B \subseteq A$ finite, then also
    $h^{-1}[L] \cap B^{\ast} \in \CS$.
\item If $L \in \CS$ and $R$ is a  regular language then
    $L \cap R \in \CS$.
\item If $L_1, L_2 \in \CS$ then also $L_1 \cup L_2 \in \CS$ and
    $L_1 \cdot L_2 \in \CS$.
\end{dingautolist}
%%
\end{defn}
%%
We still have to show that the languages of Type $i$ are closed
with respect to intersections with regular languages. A proof for
the Types 3 and 2 is found in Section~\ref{kap2}.\ref{kap2-1},
Theorem~\ref{thm:cfintersekt}.
This proof can be extended to the other types without problems.

The regular, the context free and the Type 0 languages over a fixed
alphabet form an abstract family of languages. The context sensitive
languages fulfill all criteria except for the closure under
homomorphisms. It is easy to show that the regular languages over
$A$ form the smallest abstract family of languages. More on this
subject can be found in \cite{ginsburg:formal}.

{\it Notes on this section.} It is a gross simplification to view 
languages as sets of strings. The idea that they can be defined by 
means of formal processes did not become apparent until
the 1930s. The idea of formalizing rules for transforming strings
was first formulated by Axel Thue \shortcite{thue:zeichenreihen}. 
%%%
\index{Thue, Axel}%%%
%%%
The observation that languages (in his case formal languages) could be seen as
generated from semi Thue systems, is due to Emil Post. 
%%%
\index{Post, Emil}%%
%%%
Also, he has
invented independently what is now known as the Turing machine and
has shown that this machine does nothing but string transformations.
The idea was picked up by Noam Chomsky 
%%%
\index{Chomsky, Noam}%%
%%%
and he defined the
hierarchy which is now named after him (see for example
\cite{chomsky:properties}, but the ideas have been circulating
earlier). In view of Theorem~\ref{thm:erase} it is unclear,
however, whether grammars of Type 0 or 1 have any relevance for
natural language syntax, since there is no notion of a constituent
that they define as opposed to context free grammars. There are 
other points to note about these types of grammars. 
\cite{langholm:indexed} voices clear discontentment with 
the requirement of a single start symbol, which is in practice 
anyway not complied with.
%%
\vplatz
\exercise
Let $T$ be a semi Thue system over $A$ and $A \subseteq B$.
Then $T$ is also a semi Thue system $T'$ over $B$. Characterize
$\Pf^{\ast}_{T'} \subseteq B^{\ast} \times B^{\ast}$ by means
of $\Pf_T^{\ast} \subseteq A^{\ast}\times A^{\ast}$.
{\it Remark.} This exercise shows that with the Thue system
we also have to indicate the alphabet on which it is based.
%%
\vplatz
\exercise
Let $A$ be a finite alphabet. Every string $\vec{x}$ is the value 
of a constant term $\vec{x}^E$ composed  from constants $\uli{a}$ 
for every $a \in A$, the symbol $\varepsilon$, and $^{\smallfrown}$. 
Let $T$ be a Thue system over $A$. Write $T^E := \{\vec{x}^E%
\boldsymbol{\doteq}\vec{y}^E : \auf \vec{x}, \vec{y}\zu \in T\}$. 
Let $M$ be consist of Equations~\eqref{eqn:null} and 
\eqref{eqn:eins}. $T^E$ is an equational theory. Show that 
$\vec{x} \Pf^{\ast}_T \vec{y}$ iff $\vec{y} \Pf^{\ast}_T \vec{x}$ 
iff $T^E \cup M \vdash \vec{x}^E\boldsymbol{\doteq}\vec{y}^E$.  
%%%
\vplatz
\exercise
\index{Commuting Instances Lemma}%%%
%%%%
Prove the Commuting Instances Lemma.
%%
\vplatz
\exercise
Show that every finite language is regular.
%%
\vplatz
\exercise
Let $G$ be a grammar with rules of the form $X \pf \vec{\alpha}$.
Show that $L(G)$ is context free. Likewise show that
$L(G)$ is regular if all rules have the form
$X \pf \alpha_0 \conc \alpha_1$ where
$\alpha_0 \in A \cup \{\varepsilon\}$ and $\alpha_1
\in N \cup \{\varepsilon\}$.
%%
\vplatz
\exercise
Let $G$ be a grammar in which every rule distinct from
$X \pf a$ is strictly expanding. Show that a derivation
of a string of length $n$ takes at most $2n$ steps.
%%
\vplatz
\exercise
Show that the language
$\{\mbox{\tt a}^n\mbox{\tt b}^n : n \in  \omega\}$
is context free.
%%
\vplatz
\exercise
Write a Type 1 grammar for the language
$\{\mbox{\tt a}^n \mbox{\tt b}^n \mbox{\tt c}^n : n \in \omega\}$
and one for $\{ \vec{x} \conc \vec{x} : \vec{x} \in A^{\ast}\}$.
%%
