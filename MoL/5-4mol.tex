\section{Discontinuity}
%%
%%
\label{kap4-4}
%
%
%
In this section we shall study a very important type of grammars,
the so--called \textbf{Linear Context--Free Rewrite Systems} --- {\bf
LCFRS} for short  (see \cite{vijay-weir-joshi:LCFRS}). These 
grammars are weakly equivalent to what we call linear LMGs.  
%%
\begin{defn}
%%%
\index{literal movement grammar!linear}%%
%%%
A $k$--LMG is called \textbf{linear} if it is a simple
$k$--LMG and every rule which is not 0--ary is downward
nondeleting and downward linear, while 0--ary rules have
the form $X(\vec{x}_0, \dotsc, \vec{x}_{\Xi(X)-1}) 
\hrn .$ with $\vec{x}_i \in A$ for all $i < \Xi(X)$.
\end{defn}
%%
In other words, if we have a rule of this form
%%
\begin{equation}
A(t_0, \dotsc, t_{k-1}) \hrn
B_0(s_0^0, \dotsc, s^0_{k-1}), 
\dotsc, B_{n-1}(s_0^{n-1}, \dotsc, s_{k-1}^{n-1}).
\end{equation}
%%
then for every $i < k$ and every $j < n$: $s_i^j = x_i^j$ 
and $x^i_j = x^{i'}_{j'}$ implies $i = i'$ and $j = j'$. 
Finally, $\prod_{i < k} t_i$ is a term containing each of these 
variables exactly once, in addition to occurrences of constants.

It is easy to see that the generative capacity is not diminished 
if we disallowed constants. In case $k = 1$ we get exactly the 
CFGs, though in somewhat disguised form: for now 
--- if there are no constants --- a rule is of the form
%%
\begin{equation}
A(\prod_{i < n} x_{\pi(i)}) \hrn
B_0(x_0), B_1(x_1), \dotsc, B(x_{n-1}).
\end{equation}
%%
where $\pi$ is a permutation of the numbers $< n$. If $\rho
:= \pi^{-1}$ is the permutation inverse to $\pi$ we can
write the rule in this way.
%%
\begin{equation}
A(\prod_{i < n} x_i) \hrn
B_{\rho(0)}(x_0), B_{\rho(1)}(x_1), \dotsc, B_{\rho(n-1)}(x_{n-1})
.
\end{equation}
%%
(To this end we replace the variable $x_i$ by the variable
$x_{\rho(i)}$ for every $i$. After that we permute the
$B_i$. The order of the conjuncts is anyway insignificant 
in an LMG.) This is as one can easily see exactly the form of
a context free rule. For we have
%%
\begin{equation}
\prod_{i< n} x_i = x_0x_1 \dotsb x_{n-1} 
\end{equation}
%%
This rule says therefore that if we have constituents
$\vec{u}_i$ of type $B_{\rho(i)}$ for $i < n$, then
$\prod_{i < n} \vec{u}_i$ is a constituent of type $A$.

The next case is $k = 2$. This defines a class of
grammars which have been introduced before, using a somewhat
different notation and which have been shown to be powerful
enough to generate non CFLs such as Swiss German. In linear 2--LMGs 
we may have rules of this kind.
%%
\begin{equation}
\begin{split}
A(x_1x_2, y_1y_2) & \hrn B(x_1, y_1), C(x_2, y_2). \\
A(x_2x_1, y_1y_2) & \hrn B(x_1, y_1), C(x_2, y_2). \\
A(y_1x_1y_2, x_2) & \hrn B(x_1, y_1), C(x_2, y_2). \\
A(x_1, y_1x_2y_2) & \hrn B(x_1, y_1), C(x_2, y_2).
\end{split}
\end{equation}
%%
The following rules, however, are excluded.
%%
\begin{align}
A(x_1, y_1y_2) & \hrn B(x_1, y_1), C(x_2, y_2). \\
A(x_2x_2x_1, y_1y_2) & \hrn B(x_1, y_1), C(x_2, y_2).
\end{align}
%%
The first is upward deleting, the second not linear. We shall see
that the language $\{\mbox{\tt a}^n\mbox{\tt b}^n\mbox{\tt c}^n
\mbox{\tt d}^n : n \in \omega\}$ can be generated by a linear
2--LMG, the language $\{\mbox{\tt a}^n \mbox{\tt b}^n %
\mbox{\tt c}^n\mbox{\tt d}^n\mbox{\tt e}^n : n \in \omega\}$
however cannot. The second fact follows from Theorem~\ref{thm:2kpump}. 
For the first language we give the following grammar.
%%
\begin{equation}
\begin{split}
\mbox{\tt S}(y_0x_0y_1,z_0x_1z_1) & \hrn
\mbox{\tt S}(x_0,x_1), \mbox{\tt A}(y_0,y_1),
\mbox{\tt B}(z_0, z_1). \\
\mbox{\tt S}(\varepsilon, \varepsilon) & \hrn . \\
\mbox{\tt A}(\mbox{\tt a},\mbox{\tt b}) & \hrn . \\
\mbox{\tt B}(\mbox{\tt c},\mbox{\tt d}) & \hrn .
\end{split}
\end{equation}
%%
This shows that $2$--linear LMGs are strictly stronger than
CFGs. As a further example we shall look again at Swiss German 
(see Section~\ref{kap2}.\ref{kap2-6}). We define the following 
grammar. (Recall that $x \oconc y := x^{\smallfrown}%
\Box^{\smallfrown}y$.) 
%%
\begin{equation}
$$\begin{array}{l@{\, \hrn \,}l@{\qquad\qquad}l@{\,\hrn\,}l}
\mbox{\tt NPa}(\mbox{\tt d'chind}) & . &
\mbox{\tt NPd}(\mbox{\tt em Hans}) & . \\
\mbox{\tt NPsm}(\mbox{\tt Jan}) & . &
\mbox{\tt NPa}(\mbox{\tt es huus}) & . \\
\mbox{\tt Vdr}(\mbox{\tt h\"alfe}) & . &
\mbox{\tt Vf}(\mbox{\tt l\"ond}) & . \\
\mbox{\tt Var}(\mbox{\tt laa}) & . \\
\mbox{\tt Van}(\mbox{\tt aastriche}) & . &
\mbox{\tt C}(\mbox{\tt das}) & . \\
\mbox{\tt NPss}(\mbox{\tt mer}) & . &
\mbox{\tt Vc}(\mbox{\tt s\"ait}) & . \\
\mbox{\tt S}(x\oconc y\conc\mbox{\tt .}) & 
	\multicolumn{3}{l}{\mbox{\tt NPsm}(x), \mbox{\tt VP}(y).} \\
\mbox{\tt VP}(x\conc\mbox{\tt ,}\oconc y) & 
	\multicolumn{3}{l}{\mbox{\tt Vc}(x), \mbox{\tt CP}(y).} \\
\mbox{\tt CP}(x\oconc y\oconc z_0\oconc z_1\oconc u) & 
	\multicolumn{3}{l}{\mbox{\tt C}(x), \mbox{\tt NPss}(y),
    \mbox{\tt VI}(z_0, z_1), \mbox{\tt Vf}(u).} \\
\mbox{\tt VI}(x\oconc z_0,y\oconc z_1) & 
	\multicolumn{3}{l}{\mbox{\tt NPa}(x), \mbox{\tt Var}(y), 
	\mbox{\tt VI}(z_0, z_1).} \\
\mbox{\tt VI}(x\oconc z_0,y\oconc z_1) & \multicolumn{3}{l}{\mbox{\tt NPd}(x),
    \mbox{\tt Vdr}(y), \mbox{\tt VI}(z_0, z_1).} \\
\mbox{\tt VI}(x,y) & \multicolumn{3}{l}{\mbox{\tt NPa}(x), \mbox{\tt Van}(y).}
\end{array}$$
\end{equation}
%%
This grammar is pretty realistic also with respect to the
constituent structure, about which more below. For simplicity
we have varied the arities of the predicates. Notice in
particular the last two rules. They are the real motor of
the Swiss German infinitive constructions. For we can
derive the following.
%%
\begin{align}
& \mbox{\tt VI}(\mbox{\tt d'chind } z_0, \mbox{\tt laa } z_1) \hrn
    \mbox{\tt VI}(z_0, z_1). \\
& \mbox{\tt VI}(\mbox{\tt em Hans } z_0, \mbox{\tt h\"alfe } z_1) \hrn
    \mbox{\tt VI}(z_0, z_1). \\
& \mbox{\tt VI}(\mbox{\tt d'chind}, \mbox{\tt aastriche}) \hrn . \\
& \mbox{\tt VI}(\mbox{\tt es huus}, \mbox{\tt aastriche}) \hrn . \\
& \mbox{\tt VI}(\mbox{\tt d'chind em Hans }z_0,
    \mbox{\tt laa h\"alfe }z_1) \hrn
    \mbox{\tt VI}(z_0, z_1). \\
& \mbox{\tt VI}(\mbox{\tt em Hans es huus }z_0,
    \mbox{\tt h\"alfe laa }z_1) \hrn
    \mbox{\tt VI}(z_0, z_1).
\end{align}
%%
However, we do not have
%%
\begin{equation}
\mbox{\tt VI}(\mbox{\tt em Hans}, \mbox{\tt laa}) \hrn .
\end{equation}
%%
The sentences of Swiss German as reported in Section~\ref{kap2}.\ref{kap2-6}
are derivable and some further sentences, which are all
grammatical.

Linear LMGs can also be characterized by the vector polynomials
which occur in the rules. We shall illustrate this by way of
example with linear 2--LMGs and here only for the at most
binary rules. We shall begin with the unary rules.
They can make use of these vector polynomials.
%%
\begin{equation}
\begin{split}
\Gi(x_0,x_1) & := \auf x_0,x_1\zu \\
\Gp_X(x_0,x_1) & := \auf x_1, x_0\zu \\
\Gp_F(x_0,x_1) & := \auf x_0x_1, \varepsilon \zu \\
\Gp_G(x_0, x_1) & := \auf x_1x_0, \varepsilon \zu \\
\Gp_H(x_0,x_1) & := \auf \varepsilon, x_0x_1\zu \\
\Gp_K(x_0,x_1) & := \auf \varepsilon, x_1x_0\zu
\end{split}
\end{equation}
%%
Then the following holds.
%%
\begin{align}
\notag
\Gp_X(\Gp_X(x_0,x_1)) & = \Gi(x_0, x_1) \\
\Gp_G(x_0,x_1) & = \Gp_F(\Gp_X(x_0,x_1)) \\
\notag
\Gp_K(x_0,x_1) & = \Gp_H(\Gp_X(x_0,x_1))
\end{align}
%%
This means that one has $\Gp_G$ at one's disposal if one also has
$\Gp_X$ and $\Gp_F$, and that one has $\Gp_F$ if one also has $\Gp_X$
and $\Gp_G$ and so on. With binary rules already the situation
gets quite complicated. Therefore we shall assume that we have all
unary polynomials. A binary vector polynomial is of the form $\auf
p_0(x_0, x_1, y_0, y_1), p_1(x_0, x_1, y_0, y_1)\zu$ such that $q
:= p_0 \conc p_1$ is linear. Given $q$ there exist exactly 5 
choices for $p_0$ and $p_1$, determined exactly by the cut--off 
point. So we only need to list $q$. Here we can assume that in
$q(x_0,x_1,y_0,y_1)$ $x_0$ always appears to the left of $x_1$ and
$y_0$ to the left of $y_1$. Further, one may also assume that
$x_0$ is to the left of $y_0$ (otherwise exchange the $x_i$ with
the $y_i$). After simplification this gives the following polynomials.
%%
\begin{equation}
\begin{split}
q_C(x_0,x_1, y_0,y_1) & := x_0x_1y_0y_1 \\
q_W(x_0,x_1, y_0,y_1) & := x_0y_0x_1y_1 \\
q_Z(x_0,x_1, y_0,y_1) & := x_0y_0y_1x_1
\end{split}
\end{equation}
%%
Let us take a look at $q_W$. From this polynomial we get the
following vector polynomials.
%%
\begin{equation}
\begin{split}
\Gq_{W0}(\auf x_0,x_1\zu,\auf y_0, y_1\zu) & :=
	\auf \varepsilon, x_0y_0x_1y_1\zu \\
\Gq_{W1}(\auf x_0,x_1\zu,\auf y_0, y_1\zu) & :=
	\auf x_0, y_0x_1y_1\zu \\
\Gq_{W2}(\auf x_0,x_1\zu,\auf y_0, y_1\zu) & :=
	\auf x_0y_0, x_1 y_1\zu \\
\Gq_{W3}(\auf x_0,x_1\zu,\auf y_0, y_1\zu) & :=
	\auf x_0y_0x_1, y_1\zu \\
\Gq_{W4}(\auf x_0,x_1\zu,\auf y_0, y_1\zu) & :=
	\auf x_0y_0x_1y_1, \varepsilon \zu
\end{split}
\end{equation}
%%
We say that a linear LMG has polynomial 
basis $Q$ if in the rules of this grammar only vector polynomials 
from $Q$ have been used. It is easy to see that if $\Gq$ is a 
polynomial that can be presented by means of polynomials from $Q$, 
then one may add $\Gq$ to $Q$ without changing the generative 
capacity. Notice also that it does not matter if the polynomial 
contains constants.  If we have, for example,
%%
\begin{equation}
\mbox{\tt X}(\mbox{\tt a}x\mbox{\tt bc}y)
    \hrn \mbox{\tt X}(x), \mbox{\tt Z}(y).
\end{equation}
%%
we can replace this by the following rules.
%%
\begin{align}
\begin{split}
\mbox{\tt X}(uxvwy) & \hrn
    \mbox{\tt A}(u), \mbox{\tt X}(x),
    \mbox{\tt B}(v), \mbox{\tt C}(w),
    \mbox{\tt Z}(y). \\
\mbox{\tt A}(\mbox{\tt a}) & \hrn . \\
\mbox{\tt B}(\mbox{\tt b}) & \hrn . \\
\mbox{\tt C}(\mbox{\tt c}) & \hrn .
\end{split}
\end{align}
%%
This is advantageous in proofs. We bring to the attention of the
reader some properties of languages that can be generated by
linear LMGs. The following is established in 
\shortcite{vijay-weir-joshi:LCFRS}, see also \cite{weir:phd}.
%%
%%%
%%
\begin{prop}[Vijay--Shanker \& Weir \& Joshi]
%%%
\index{Vijay--Shanker, K.}%%%
\index{Weir, David}%%%
\index{Joshi, Aravind}%%%
%%%
\label{prop:linear-semilinear}
Let $G$ be a linear $k$--LMG. Then $L(G)$ is semilinear.
\end{prop}
%%
A special type of linear LMGs are the so--called head grammars.
These grammars have been introduced by Carl Pollard
%%%%
\index{Pollard, Carl}%%
%%%%
in \shortcite{pollard:head}. The strings that are manipulated are
of the form $\vec{x}a\vec{y}$ where $\vec{x}$ and $\vec{y}$
are strings and $a \in A$. One speaks in this connection of
$a$ in the string as the distinguished \textbf{head}.
This head is marked here by underlining it. Strings containing
an underlined occurrence of a letter are called \textbf{marked}.
The following rules for manipulating marked strings are now
admissible. 
%%
\begin{equation}
\begin{split}
h_{C1}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}\uli{a}\vec{w}\vec{y}b\vec{z} \\
h_{C2}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}a\vec{w}\vec{y}\uli{b}\vec{z} \\
h_{L1}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}\uli{a}\vec{y}b\vec{z}\vec{w} \\
h_{L2}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}a\vec{y}\uli{b}\vec{z}\vec{w} \\
h_{R1}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}\vec{y}b\vec{z}\uli{a}\vec{w} \\
h_{R2}(\vec{v}\uli{a}\vec{w}, \vec{y}\uli{b}\vec{z}) &
    := \vec{v}\vec{y}\uli{b}\vec{z}a\vec{w}
\end{split}
\end{equation}
%%
(Actually, this definition is due to \cite{roach:87}, 
%%%
\index{Roach, Kelly}%%%
%%%
who showed that Pollard's definition is weakly equivalent to this 
one.) Notice that the head is not allowed to be empty. In 
\cite{pollard:head} the functions are partial: for example, 
$h_{C1}(\varepsilon, \vec{w})$ is undefined. Subsequently, the 
definition has been changed slightly, basically to allow for empty 
heads. In place of marked strings one takes 2--vectors of strings. 
The marked head is the comma. This leads to the following definition. 
(This is due to \cite{shanker-weir-joshi:coling86}. 
See also \cite{seki}.)
%%
\begin{defn}
%%%
\index{head grammar}%%
%%%
A \textbf{head grammar} is a linear 2--LMG with the following
polynomial basis.
%%
\begin{equation}
\begin{split}
\Gp_{C1}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
    := \auf x_0, x_1y_0y_1\zu \\
\Gp_{C2}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
    := \auf x_0x_1y_0, y_1\zu \\
\Gp_{L1}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
    := \auf x_0, y_0y_1x_1\zu \\
\Gp_{L2}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
    := \auf x_0y_0, y_1x_1\zu \\
\Gp_{R1}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
    := \auf x_0y_0y_1, x_1\zu 
%\\
%\Gp_{R2}(\auf x_0,x_1\zu,\auf y_0,y_1\zu) &
%    := \auf x_0y_0, y_1 x_1\zu
\end{split}
\end{equation}
%%
\end{defn}
%%
It is not difficult to show that the following basis of polynomials 
is sufficient: $\Gp_{C1}$, $\Gp_{C2}$ and 
%%%
\begin{equation}
\Gp_{W}(\auf x_0, x_1\zu, \auf y_0, y_1\zu) := \auf x_0y_0, y_1x_1\zu
\end{equation}
%%%
Notice that in this case there are {\it no\/} extra unary
polynomials. However, some of them can be produced by feeding
empty material. These are exactly the polynomials $\Gi$, $\Gp_F$ 
and $\Gp_H$. The others cannot be produced, since
the order of the component strings must always be respected.
For example, one has
%%
\begin{equation}
\Gp_{C2}(\auf x_0,x_1\zu,\auf \varepsilon, \varepsilon\zu) = \auf x_0x_1,
    \varepsilon\zu = \Gp_F(x_0, x_1)
\end{equation}
%%

We shall now turn to the description of the structures that 
correspond to the trees for CFGs. Recall the definitions of 
Section~\ref{kap1}.\ref{kap1-4}. 
%%
\begin{defn}
\label{defn:context}
%%%
\index{context!$n$--\faul}%%
%%%
A sequence $C = \auf \vec{w}_i : i < n+1\zu$ of strings is called
an $n$--\textbf{context}. A sequence $\Gv = \auf \vec{v}_i : i < n\zu$
\textbf{occurs in} $\vec{x}$ \textbf{in the context} $C$ if
%%
\begin{equation}
\vec{x} = \vec{w}_0\conc\prod_{i < n} \vec{v}_i\vec{w}_{i+1}
\end{equation}
%%
We write $C(\Gv)$ in place of $\vec{x}$.
%%
\end{defn}
%%
Notice that an $n$--sequence of strings can alternatively be
regarded as an $n-1$--context. Let $G$ be a $k$--linear
LMG. If $\vdash_G A(\vec{x}_0, \dotsc, \vec{x}_{k-1})$
then this means that the $k$--tuple $\auf \vec{x}_i : i < k\zu$
is a constituent of category $A$. If $\vdash_G S(\vec{x})$, then 
the derivation will consist in deriving statements of the form 
$A(\auf \vec{y}_i : i < \Xi(A)\zu)$ such that there is an 
$n+1$--context for $\auf \vec{y}_i : i < \Xi(A)\zu$ in 
$\vec{x}$.

The easiest kinds of structures are trees where each nonterminal 
node is assigned a tuple of subwords of the terminal string. Yet, 
we will not follow this approach as it generates stuctures that are 
too artificial. Ideally, we would like to have something analogous to 
constituent structures, where constituents are just appropriate 
subsets of the terminal nodes. 
%%%
\begin{defn}
An \textbf{labelled ordered tree of discontinuity degree} $k$ is a 
%%%
\index{discontinuity degree}%%%
%%%
labelled, ordered tree such that $[x]$ has at most $k$ discontinuous 
pieces.
\end{defn}
%%%
If $G$ is given, the labels are taken from $A$ and $R$, and $A$ is 
the set of labels of leaves, while $R$ is the set of labels for 
nonleaves. Also, if $Q$ is a $k$--ary predicate, it must somehow 
be assigned a unique $k$--tuple of subwords of the terminal string.
To this end, we segment $[x]$ into $k$ continuous parts. The way 
this is done exactly shall be apparent later. Now, if $x$ is 
assigned $B$ and if $[x]$ is the disjoint union of the continuous 
substrings $\vec{y}_i$, $i < k$, and if $\vec{y}_i$ precedes in 
$\vec{y}_j$ in $\vec{x}$ iff $i < j$ then $B(\auf \vec{y}_i : i < k\zu)$.

However, notice that the predicates apply to sequences of substrings. 
This is to say that the linear order is projected from the 
terminal string. Additionally, the division into substrings can be 
read off from the tree (though not from $[x]$ alone). There is, 
however, one snag. Suppose $G$ contains a rule of the form 
%%%
\begin{equation}
\label{rule:nonmon}
A(x_1x_0y_0, y_1) \hrn B(x_0, x_1), C(y_0, y_1)
\end{equation}
%%
Then assuming that we can derive $B(\vec{u}_0, \vec{u}_1)$ and 
$C(\vec{v}_0, \vec{v}_1)$, we can also derive 
$A(\vec{u}_1\, \vec{u}_0\, \vec{v}_0, \vec{v}_1)$. 
However, this means that $\vec{u}_1$ must precede $\vec{u}_0$ 
in the terminal string, which we have excluded. We can prevent 
this by introducing a nonterminal $B^{\ast}$ such that 
$B^{\ast}(x_0, x_1) \dpf B(x_1, x_0)$, and then rewrite the rule as
%%%
\begin{equation}
A(x_0x_1y_0, y_1) \hrn B^{\ast}(x_0, x_1), C(y_0, y_1).
\end{equation}
%%
The problem with the rule \eqref{rule:nonmon} is that it switches 
the order of the $x_i$'s. Rules that do this (or switch the order of 
the $y_i$'s) are called nonmonotone in the sense of the following 
definition.
%%
\begin{defn}
%%%
\index{rule!monotone}%%
\index{literal movement grammar!monotone}%%
%%%
Let $\rho = L \hrn M_0\dotsb M_{n-1}$ be a linear rule,
$L = B(\auf t^j : j < k\zu)$ and
$M_i = A_i(\auf x^j_i : j < k_i\zu)$, $i < n$. $\rho$ is called
\textbf{monotone} of for every $i < n$ and every pair
$j < j' < k_i$ the following holds: if $x_i^j$ occurs in 
$t^q$ and $x_i^{j'}$ in $t^{q'}$ then either $q < q'$ or 
$q = q'$ and $x_i^j$ occurs before $x_i^{j'}$ in the polynomial $t_q$.
An LMG is monotone if all of its rules are.
\end{defn}
%%
Now, for every LCFRS there exists a monotone LCFRS that generates 
the same strings (and modulo lexical rules also the same structures). 
For every predicate $A$ and every permutation $\pi : k \pf k$, 
$k := \Xi(A)$ assume a distinct predicate $A^{\pi}$ with the 
intended interpretation that $A(x_0, \dotsc, x_{k-1})$ iff 
$A^{\pi}(x_{\pi^{-1}(0)}, \dotsc, x_{\pi^{-1}(k-1)})$. Every 
rule $A(\vec{s}) \hrn B_0\dotsc B_{p-1}$ is replaced by all possible 
rules $A^{\pi}(\pi^{-1}(\vec{s})) \hrn B_0 \dotsc B_{p-1}$. 

Let $A \hrn B_0 \dotso B_{p-1}$ be a rule. 
Now put $\rho_i(j) := k$ iff $x^k_i$ is the $j$th variable 
from the variables $x_i^q$, $q < \Xi(B_i)$, which occurs in 
$\prod_{i < \Xi(A)} t_i$, counting from the left. Then 
$A' \hrn B_0^{\rho_0} \dotso B^{\rho_{p-1}}_{p-1}$ will replace 
the rule $A \hrn B_0 \dotso B_{p-1}$, where $A'$ results from 
$A$ by applying the substitution $x_i^j \mapsto x_i^{\rho^{-1}_i(j)}$ 
(while the variables of the $B_i^{\rho}$ remain in the original 
order). This rule is monotone. For example, assume that we have 
the rule
%%%
\begin{equation}
A(x_2y_1x_1, x_0y_2y_0y_3) \hrn B_0(x_0, x_1, x_2), 
C(y_0, y_1, y_2, y_3).
\end{equation}
%%%
Then we put $\rho_0 : 0 \mapsto 2, 1 \mapsto 1, 2 \mapsto 0$, 
and $\rho_1 : 0 \mapsto 2, 1 \mapsto 0, 2 \mapsto 1, 3 \mapsto 3$.
So we get 
%%%
\begin{equation}
A(x_0y_0x_1, x_2y_1y_2y_3) \hrn B^{\rho_0}(x_0, x_1, x_2), 
C^{\rho_1}(y_0, y_1, y_2, y_3).
\end{equation}
%%%
Every terminal rule is monotone. Thus, we can essentially throw 
out all nonmonotone rules. Thus, for nonmonotone LCFRS there is 
a monotone LFCRS generating the same strings. 

We can a derive useful theorem on LCFRSs.
%%%
\begin{defn}
%%%
\index{language!$k$--pumpable}%%
%%%
A language $L$ is called $k$--\textbf{pumpable} if there is a 
constant $p_L$ such that for all $\vec{x}$ of length $\geq p_L$ 
there is a decomposition 
$\vec{x} = \vec{u}_0\prod_{i < k}({\vec{v}_i}{\vec{u}_{i+1}})$
such that 
%%
\begin{equation}
\{\vec{u}_0\prod_{i < k}({\vec{v}_i\,}^n{\vec{u}_i}) : n \in \omega\} 
\subseteq L
\end{equation}
\end{defn}
%%%
\begin{thm}[Groenink]
%%%
\label{thm:2kpump}
\index{Groenink, Annius}%%%
%%%
Suppose that $L$ is a $k$--LCFRL. Then $L$ is $2k$--pumpable.
\end{thm}
%%%
We provide a proof sketch based on derivations. Transform the language 
into a language of signs, by adding the trivial semantics. Then 
let $\GA$ be a sign grammar based on a monotone $k$--LCFRS for it.
Observe that if $\Gt$ is a structure term containing $x$ free 
exactly once, and if $\Gt(\Gu)$ and $\Gu$ are definite 
and unfold to signs of identical category, then with $\Gs(\Gt(\Gu))$ 
also $\Gs(\Gt^n(\Gu))$ is definite for every $n$ (the rule skeleton 
is context free). Now, if $\vec{x}$ is large enough, its structure 
term will be of the form $\Gs(\Gt(\Gu))$ such that $\Gt(\Gu)$ 
and $\Gu$ have the same category. Finally, suppose that the 
grammar is monotone. Then $\Gt^{\varepsilon}$ is a monotone, 
linear polynomial function on $k$--tuples; hence there are 
$\vec{v}_i$, $i < 2k$, such that 
%%%
\begin{equation}
p(x_0, \dotsc, x_{k-1}) = \auf \vec{v}_{2i}x_i\vec{v}_{2i+1} : 
i < k\zu
\end{equation}
%%%

Thus let us now focus on monotone LCFRSs. We shall define the 
structures that are correspond to derivations in monotone LCFRSs, 
and then show how they can be generated using graph grammars.
%%%
\begin{defn}
Suppose that $\GT = \auf T, <, \sqsubset, \ell\zu$ is an ordered labelled 
tree with degree of discontinuity $k$ and $G = \auf A, R, \Xi, S, H\zu$ 
a monotone LCFRS. We say that $\GT$ is a $G$--\textbf{tree} if 
%%%
\begin{dingautolist}{192}
\item $\ell(v) \in A$ iff $v$ is a leaf, 
\item for every nonleaf $v$: there is a set $\{H(v)_i : i < k\}$ 
	of leaves such that $k = \Xi(\ell(v))$, and 
	$H(v)_i \subseteq [x]$ is continuous, 
\item if $v$ has daughters $w_i$, $i < n$, then there is a rule
	$$A(t_0, \ldots, t_{\Xi(A)}) \hrn 
	B_0(x_0^0, \dotsc, x_0^{\Xi(B_0)}), \\ \dotsc, 
	B_{n-1}(x_{n-1}^0, \dotsc, x_{n-1}^{\Xi(B_{n-1})})$$
	such that 
	\begin{itemize}
	\item
	$v$ has label $A$, $w_i$ has label $B_i$ ($i < n$), 
	\item	
	if $t_j = \prod_{i < \nu} x_{g(i)}^{h(i)}$ then $H(v)_j = 
	\bigcup_{i < \nu} H(w_{g(i)})_{h(i)}$.
	\end{itemize}
\end{dingautolist}
\end{defn}
%%
The last clause is somewhat convoluted. It says that whatever 
composition we assume of the leaves associated with $v$, it must 
be compatible with the composition of the $w_i$, and the way the 
polynomials are defined. Since the preterminals are unary, it can 
be shown that $H(v)_i$ is unique for all $v$ and $i$.

The following grammar is called $G^{\heartsuit}$.
%%
\begin{equation}
\begin{array}{llll}
\multicolumn{4}{l}{\mbox{\tt S}(y_0x_0y_1,z_0x_1z_1) \hrn
    \mbox{\tt S}(x_0,x_1),\mbox{\tt X}(y_0,y_1),
    \mbox{\tt Y}(z_0, z_1).} \\
\mbox{\tt S}(\varepsilon, \varepsilon) \hrn . & & & \\
\multicolumn{2}{l}{\mbox{\tt X}(x,y) \hrn \mbox{\tt A}(x), \mbox{\tt B}(y).}
&  
\multicolumn{2}{l}{\mbox{\tt Y}(x,y) \hrn \mbox{\tt C}(x), \mbox{\tt D}(y).} \\
\mbox{\tt A}(\mbox{\tt a}) \hrn  . & & 
\mbox{\tt C}(\mbox{\tt c}) \hrn  . &  \\
\mbox{\tt B}(\mbox{\tt b}) \hrn  . & & 
\mbox{\tt D}(\mbox{\tt d}) \hrn  . & 
\end{array}
\end{equation}
%%
$G^{\heartsuit}$ derives {\mtt aabbccdd} with the structure shown 
in Figure~\ref{fig:disconti}.
%%
\begin{figure}
\begin{center}
\begin{picture}(23,20)
\put(1,1){\makebox(0,0){\tt a}}
\put(4,1){\makebox(0,0){\tt a}}
\put(7,1){\makebox(0,0){\tt b}}
\put(10,1){\makebox(0,0){\tt b}}
\put(13,1){\makebox(0,0){\tt c}}
\put(16,1){\makebox(0,0){\tt c}}
\put(19,1){\makebox(0,0){\tt d}}
\put(22,1){\makebox(0,0){\tt d}}
%%
\multiput(1,1.5)(3,0){8}{\line(0,1){3}}
%%
\put(1,5){\makebox(0,0){\tt A}}
\put(4,5){\makebox(0,0){\tt A}}
\put(7,5){\makebox(0,0){\tt B}}
\put(10,5){\makebox(0,0){\tt B}}
\put(13,5){\makebox(0,0){\tt C}}
\put(16,5){\makebox(0,0){\tt C}}
\put(19,5){\makebox(0,0){\tt D}}
\put(22,5){\makebox(0,0){\tt D}}
%%
\put(1,5.5){\line(0,1){3}}
\put(4,5.5){\line(1,2){1.5}}
\put(7,5.5){\line(-1,2){1.5}}
\put(10,5.5){\line(-3,1){9}}
\put(13,5.5){\line(3,1){9}}
\put(16,5.5){\line(1,2){1.5}}
\put(19,5.5){\line(-1,2){1.5}}
\put(22,5.5){\line(0,1){3}}
%%
\put(1,9){\makebox(0,0){\tt X}}
\put(5.5,9){\makebox(0,0){\tt X}}
\put(17.5,9){\makebox(0,0){\tt Y}}
\put(22,9){\makebox(0,0){\tt Y}}
%%
\put(5.5,9.5){\line(2,1){6}}
\put(17.5,9.5){\line(-2,1){6}}
\put(11.5,13){\makebox(0,0){\tt S}}
%%
\put(1,9.5){\line(3,2){10.5}}
\put(22,9.5){\line(-3,2){10.5}}
\put(11.5,13.5){\line(0,1){3}}
\put(11.5,17){\makebox(0,0){\tt S}}
\end{picture}
\end{center}
\caption{A Structure Tree for $G^{\heartsuit}$}
\label{fig:disconti}
\end{figure}
%%
This tree is not exhaustively ordered. This is the main difference
with CFGs. Notice that the tree does not reflect
the position of the empty constituent. Its segments are found between
the second and the third as well as between the sixth and the
seventh letter. One can define the structure tree also in this way
that it explicitly contains the empty strings. To this end one has
to replace also occurrences of $\varepsilon$ by variables. The rest
is then analogous.

We shall now define a context free graph grammar that generates the 
same structures as a given monotone LCFRS. For the sake of 
simplicity we assume that all predicates are $k$--ary, and that all 
terminal rules are of the form $Y(a, \varepsilon, \dotsc, \varepsilon)$, 
$a \in A$. Monotonicity is not necessary to assume, but makes life 
easier. The only problem 
that discontinuity poses is that constituents cannot be ordered 
with respect to each other in a simple way. The way they are related 
to each other shall be described by means of special matrices.  

Assume that $\vec{u}$ is a string, $C = \auf \vec{v}_i : i < k+1\zu$ 
a $k+1$--context for $\vec{x}_i$, $i < k$, in $\vec{u}$ and 
$D = \auf \vec{w}_i : i < k+1\zu$ a $k+1$--context for $\vec{y}_j$, 
$j < k$, in $\vec{u}$. Now, write $M(C,D) := (\mu_{ij})_{ij}$ for the 
following matrix: 
%%
\begin{equation}
\mu_{pq} = 1 \text{ iff } \prod_{i < p} \vec{v}_i\vec{x}_i 
\text{ is a prefix of } \vec{w}_0\conc \prod_{i < q} \vec{y}_i\vec{w}_{i+1} 
\end{equation}
%%%
We call $M(C,D)$ the \textbf{order scheme of} $C$ and $D$. 
Intuitively, the order scheme tells us which of the $\vec{x}_p$ 
precede which of the $\vec{y}_q$ in $\vec{u}$.  We say that $C$ and 
$D$ \textbf{overlap} if there are $i, j < k$ such that the (occurrences 
of) $\vec{x}_p$ and $\vec{y}_q$ overlap. This is the case iff 
$\mu_{pq} = \mu_{qp} = 0$.
%%%
\index{overlap}%%
%%%
\begin{lem}
Assume that $\GB$ is a labelled ordered tree for a monotone LCFRS 
with yield $\vec{u}$. Further, let $x$ and $y$ be nodes. Then $\ell(x)$ 
and $\ell(y)$ overlap iff $x$ and $y$ are comparable. 
\end{lem}
%%%
Notice that in general $\mu_{qp} = 1 - \mu_{pq}$ in the nonoverlapping 
case, which is what we shall assume from now on. We illustrate this with 
an example. Figure~\ref{fig:order} shows some 2--schemes for monotone 
rules together with the orders which define them. (We omit the vector 
arrows; the ordering is defined by `is to the left of in the string'.) 
%%
\begin{figure}
$$\begin{array}{ccc}
\left(\begin{array}{ll}
1 & 1 \\
1 & 1
\end{array}\right) &
\left(\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right) &
\left(\begin{array}{ll}
1 & 1 \\
0 & 0
\end{array}\right) \\
x_0x_1y_0y_1 &
x_0y_0x_1y_1 &
x_0y_0y_1x_1 \\
\\
%%
\left(\begin{array}{ll}
0 & 1 \\
0 & 1
\end{array}\right) &
\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right) &
\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right) \\
y_0x_0x_1y_1 &
y_0x_0y_1x_1 &
y_0y_1x_0x_1
\end{array}$$
\caption{Order Schemes}
\label{fig:order}
\end{figure}
%%
For every $k$--scheme $M$ let $\xi_M$ be a relation.
Now we define our graph grammar. $N^{\circ} := \{A^{\circ} 
: A \in N\}$ is a set of fresh nonterminals. The set of vertex 
colours $F_V := N \cup N^{\circ} \cup A$, the set of terminal vertex
colours is $F_V^T := N  \cup A$, the set of edge
colours is $\{<\} \cup \{\xi_M : M \mbox{ a $k$--scheme}\}$.
(As we will see immediately, $\sqsubset$ is the relation which
fits to the relation $\xi_{\BI}$ where $\BI = (1)_{ij}$ is
the matrix which consists only of 1s.) The start graph is the
one--element graph $\GS$ which has one edge. The vertex has 
colour $S$, the edge has only one colour, $\xi_{K}$, where 
$K = (\kappa_{pq})_{pq}$ with $\kappa_{pq} = 1$ iff $p < q$. 
For every rule $\rho$ we add a graph replacement rule. Let
%%
\begin{equation}
\rho = B(\auf t^j : j < k\zu) \hrn A_0(\auf x_0^j : j < k\zu) 
\dotsb A_{n-1}(\auf x_{n-1}^j : j < k \zu)
\end{equation}
%%
be given. Let $p := \prod_{i < k} t^i$ be the characteristic
polynomial of the rule. Then the graph replacement $\rho^{\gamma}$
replaces the node $B^{\circ}$ by the following graph.
%%
\begin{equation}
\begin{array}{l}
\begin{picture}(12,10)
\put(2,3){\makebox(0,0){$\bullet$}}
\put(2,2){\makebox(0,0){$v_0$}}
\put(2,.5){\makebox(0,0){$A_0^{\circ}$}}
\put(2,3){\line(2,1){6}}
\put(5,3){\makebox(0,0){$\bullet$}}
\put(5,2){\makebox(0,0){$v_1$}}
\put(5,.5){\makebox(0,0){$A_1^{\circ}$}}
\put(5,3){\line(1,1){3}}
\put(8,3){\makebox(0,0){$\dotsb$}}
\put(11,3){\makebox(0,0){$\bullet$}}
\put(11,2){\makebox(0,0){$x_{n-1}$}}
\put(11,.5){\makebox(0,0){$A_{n-1}^{\circ}$}}
\put(11,3){\line(-1,1){3}}
\put(8,6){\makebox(0,0){$\bullet$}}
\put(8,7){\makebox(0,0){$w$}}
\put(8,8.5){\makebox(0,0){$B$}}
\end{picture}
\end{array}
\end{equation}
%%
Furthermore, between the nodes $v_i$ and $w_j$, $i \neq j$, the 
following relations hold (which are not shown in the picture).
Put $\mu(i,j)_{i'j'} := 1$ if $x_i^{i'}$ is to the left of
$x_j^{j'}$ in $p$. Otherwise, put $\mu(i,j)_{i'j'} := 0$. 
Then put $H_{ij} := (\mu(i,j)_{i'j'})_{i'j'}$. The relation 
from $v_i$ to $v_j$ is $\xi_{H_{ij}}$. Notice that by definition 
always either $x_i^{i'}$ is to the left of $x_j^{j'}$ or to the 
right of it. Hence the relation between $v_j$ and $v_i$ is exactly 
$\xi_{1 - H}$. This is relevant insofar as it allows us to 
concentrate on one colour functional only: $\GI\GI$. Now supppose 
that $w$ is in relation $\xi_M$ to a node $u$. (So, there is an 
edge of colour $\xi_M$ from $v$ to $u$.) We need to determine the 
relation (there is only one) from $v_i$ to $u$. This is $\xi_N$, 
where $N = (\nu_{pq})_{pq}$ and $\nu_{pq} = 1$ iff 
$\mu_{p'q} = 1$, where $x_i^p$ occurs in $t^{p'}$. 
The map that sends $M$ to $N$ and $<$ to $<$ is the desired 
colour functional $\GI\GI$. The other functionals can be 
straightforwardly defined.

There is a possibility of defining structure in some restricted cases,
namely always when the right hand sides do not contain a variable
twice. This differs from linear grammars in that variables are
still allowed to occur several times on the left, but only once
on the right. An example is the grammar
%%
\begin{equation}
\mbox{\tt S}(xx) \hrn \mbox{\tt S}(x).; \quad
    \mbox{\tt S}(\mbox{\tt a}) \hrn .
\end{equation}
%%
The notion of structure that has been defined above can be transferred 
to this grammar. We simply do as if the first rule was of this form
%%
\begin{equation}
\mbox{\tt S}(xy) \hrn \mbox{\tt S}(x), \mbox{\tt S}(y).
\end{equation}
%%
where it is clear that $x$ and $y$ always represent the same string.
In this way we get the structure tree for {\tt aaaaaaaa} shown 
in Figure~\ref{fig:exp}.
%%
\begin{figure}
\begin{center}
\begin{picture}(23,20)
\put(1,1){\makebox(0,0){\tt a}}
\put(4,1){\makebox(0,0){\tt a}}
\put(7,1){\makebox(0,0){\tt a}}
\put(10,1){\makebox(0,0){\tt a}}
\put(13,1){\makebox(0,0){\tt a}}
\put(16,1){\makebox(0,0){\tt a}}
\put(19,1){\makebox(0,0){\tt a}}
\put(22,1){\makebox(0,0){\tt a}}
%%
\multiput(1,1.5)(3,0){8}{\line(0,1){3}}
%%
\put(1,5){\makebox(0,0){\tt S}}
\put(4,5){\makebox(0,0){\tt S}}
\put(7,5){\makebox(0,0){\tt S}}
\put(10,5){\makebox(0,0){\tt S}}
\put(13,5){\makebox(0,0){\tt S}}
\put(16,5){\makebox(0,0){\tt S}}
\put(19,5){\makebox(0,0){\tt S}}
\put(22,5){\makebox(0,0){\tt S}}
%%
\put(1,5.5){\line(1,2){1.5}}
\put(4,5.5){\line(-1,2){1.5}}
\put(7,5.5){\line(1,2){1.5}}
\put(10,5.5){\line(-1,2){1.5}}
\put(13,5.5){\line(1,2){1.5}}
\put(16,5.5){\line(-1,2){1.5}}
\put(19,5.5){\line(1,2){1.5}}
\put(22,5.5){\line(-1,2){1.5}}
%%
\put(2.5,9){\makebox(0,0){\tt S}}
\put(8.5,9){\makebox(0,0){\tt S}}
\put(14.5,9){\makebox(0,0){\tt S}}
\put(20.5,9){\makebox(0,0){\tt S}}
%%
\put(2.5,9.5){\line(1,1){3}}
\put(8.5,9.5){\line(-1,1){3}}
\put(14.5,9.5){\line(1,1){3}}
\put(20.5,9.5){\line(-1,1){3}}
%%
\put(5.5,13){\makebox(0,0){\tt S}}
\put(17.5,13){\makebox(0,0){\tt S}}
%%
\put(5.5,13.5){\line(2,1){6}}
\put(17.5,13.5){\line(-2,1){6}}
\put(11.5,17){\makebox(0,0){\tt S}}
%%
\end{picture}
\end{center}
\caption{An Exponentially Growing Tree}
\label{fig:exp}
\end{figure}
%%

{\it Notes on this section.} In \cite{seki}, a slightly more 
general type of grammars than the LCFRSs is considered, which are 
called {\it Multiple Context Free Grammars} (MCFGs). In our
terminology, MCFGs maybe additionally upward deleting. In \cite{seki} 
weak equivalence between MCFGs and LCFRs is shown. See also 
%%%
\index{Seki, Hiroyuki}%%%
\index{Matsumura, Takashi}%%%
\index{Fujii, Mamoru}%%%
\index{Kasami, Tadao}%%
%%%%
\cite{kasami:mcfg}. \cite{michaelis:minimalism} 
%%%
\index{Michaelis, Jens}%%%
%%%
also defines monotone rules for MCFGs and shows that any MCFL can be  
generated by a monotone MCFG. For the relevance of these 
grammars in parsing see \cite{villemonte:parsing,villemonte:mcfg}. 
%%%
\index{Villemonte de la Clergerie, Eric}%%%
%%%
In his paper \shortcite{stabler:minimalism},
%%%
\index{Stabler, Edward P.}%%
%%%
Edward Stabler describes a formalisation of minimalist grammars akin
to Noam Chomsky's 
%%%
\index{Chomsky, Noam}%%%
%%%
Minimalist Program (outlined in \cite{chomsky:minimalist}). Subsequently, 
in \cite{michaelis:minimalism,michaelis:lacl98,michaelis:lacl01} and
\cite{harkema:minimalism} 
%%%
\index{Harkema, Henk}%%%
%%%
it is shown that the languages
generated by this formalism are exactly those that can be generated
by simple LMGs, or, for that matter, by LCFRSs.
%%
\vplatz
\exercise
Show that the derivation $\Gamma'$ is determined by $\Gamma$ up
to renaming of variables.
%%
\vplatz
\exercise
Prove Proposition~\ref{prop:linear-semilinear}.
%%%
\vplatz
\exercise
Let $A_k := \{\mbox{\mtt a}_i : i < k\}$, and let 
$W_k := \{\prod_{i < k}\mbox{\mtt a}_i^n : n \in \omega\}$. 
Show that $W_k$ is a $m$--LCFRL iff $k \leq 2m$.
%%
\vplatz
\exercise
Determine the graph grammar $\gamma G^{\heartsuit}$.
%%
\vplatz
\exercise
Show the following. {\it Let $N = \{x_i : i < k\} \cup \{y_i : i < k\}$
and $<$ a linear ordering on $N$ with $x_i  < x_j$ as well as $y_i < y_j$
for all $i < j < k$. Then if $m_{ij} = 1$ iff
$x_i < y_j$ then $M = (m_{ij})_{ij}$ is a $k$--scheme.
Conversely: let $M$ be a $k$--scheme and $<$ defined by
(1) $x_i < x_j$ iff $i < j$, (2) $y_i < y_j$ iff
$i< j$, (3) $x_i < y_j$ iff $m_{ij} = 1$. Then
$<$ is a linear ordering. The correspondence between orderings
and schemes is biunique.}
%%
\vplatz
\exercise
Show the following: {\it If in a linear $k$--LMG all structure trees
are exhaustively ordered, the generated tree set is context free.}
%%
