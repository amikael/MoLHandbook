\chapter{Syntax}

The theory of syntax addresses three strongly interconnected ranges of facts.
The first of these is the combinatorical possibilities of words. It is very
clear that {\it The boys ate} is a sentence of ordinary English, while four
other permutations of these three elements, {\it *The ate boys, *Ate boys the,
  *Boys the ate}, and {\it *Boys ate the}, are outside the bounds of ordinary
English. The remaining one, {\it ?Ate the boys}, is harder to pass judgment
on, but it seems clear that its stylistic value is very different from that of
the first sentence. Similarly, speakers of English will strongly agree that
{\it The boys eat} and {\it The boy eats} are ordinary sentences of the
language, while {\it *The boy eat} and {\it *The boy ates} are not, a highly
generalizable observation that justifies the statement, familiar to all from
school grammar, that predicates agree with their subjects in person and
number.  \index{English}

In most, though not necessarily all, cases it is relatively easy to construct
pairs of sentences, one grammatical and the other not, that bring into sharp
relief how a particular rule or constraint operates or fails to operate. There
are areas of grammar such as `weak crossover' or `heavy NP shift' which are
{\it weak} in the sense that the contrast is less visible and obvious than in
the examples above, but even if we raise the bar very high, there are plenty
of significant contrasts left for a theory of syntax to account for. On the
whole, the development of syntax is not crucially impacted by the weaker
examples, especially as there are generally other languages where phenomena
marginal in one language, such as resumptive pronouns in English, can be
observed in unequivocal examples, and often in far richer detail. 

The second range of facts concerns the internal structure of sentences.
Generally there is a verb, which takes a subject, and often an object,
sometimes a second (indirect) object, and there can be other dependents,
modifiers, and so on.  Here the problem is twofold: first, it is not at all
evident what primitive notions one should consider (though the grammatical
tradition of millennia offers some rather clear guidelines), and second, even
if a set of primitive grammatical notions is agreed upon, it is far from clear
which word or combination of words fits which notion in any given sentence. 

The third range of facts concerns the fit, or lack thereof, between what is
being said and what is seen in the world. In Chapter~6, we will view sentences
as broadly analogous to formulas and inquire about their interpretation in
various model structures. Here we begin with a word of caution: clearly,
people are often capable of assigning meaning to grammatically ill-formed
utterances, and conversely, they can have trouble interpreting perfectly
well-formed ones. But even with this caveat, there is a great deal of
correlation between the native speaker's judgments of grammaticalness and
their ability to interpret what is being said, so a program of research that
focuses on meaning and treats grammaticality as a by-product may still make
sense.

In fact, much of modern syntax is an attempt, one way or the other, to do away
with separate mechanisms in accounting for these three ranges of facts.  In
Section~5.1, we will discuss {\it combinatorical} approaches that put the
emphasis on the way words combine. Although in formal theories of grammar the
focus of research activity historically fell in this category, the
presentation here can be kept brief because most of this material is now a
standard part of the computer science curriculum, and the reader will find
both classical introductions such as Salomaa (1973) and modern monographic
treatments such as Kracht (2003).  In Section~5.2, we turn to {\it
  grammatical} approaches that put the emphasis on the grammatical primitives;
prominent examples are {\it dependency grammar} (Tesni\`ere 1959),
\nocite{Tesnie1re:1959} {\it tagmemics} \cite{Brend:1976}, {\it case grammar}
(Fillmore 1968), and {\it relational grammar} (Perlmutter 1983), as well as
classical P\={a}\d{n}inian {\it morphosyntax} and its modern variants. These
theories, it is fair to say, have received much less attention in the
mathematical literature than their actual importance in the development of
linguistic thought would warrant.  In Section~5.3, we discuss {\it
  semantics-driven} theories of syntax, in particular the issues of frame
semantics and knowledge representation. In Section~5.4, we take up weighted
models of syntax, which extend the reach of the theory to another range of
facts, the {\it weight} a given string of words has. We will present this
theory in its full generality, permitting as special cases both standard
formal language theory, where the weights 1 and 0 are used to distinguish
grammatical from ungrammatical, the extension \cite{Chomsky:1967} where
weights between 1 and 0 are used to represent intermediate degrees of
grammaticality, and the {\it probabilistic} theory, which plays a central role
in the applications. In Section~5.5, we discuss weighted regular languages,
giving an asymptotic characterization of these over a one-letter
alphabet. Evidence of syntactic complexity that comes from external sources
such as difficulty of parsing or acquisition is discussed in Section~5.6.

\section{Combinatorical theories} 

Given some fixed set of vocabulary items $\Sigma$, a central question is to
characterize the subset $L \subset \Sigma^*$ that speakers take to be
`ordinary' or `grammatical' or `well-formed' (we use these notions
interchangeably) by means of a formal system. One significant practical
difficulty is that $\Sigma$, as traditionally understood, collects together
all words from the language, and this is a very large set. For English, we
already have $| \Sigma | \gg 10^6$ if we restrict ourselves to words attested
in print, and potentially infinite, if we permit all well-formed, but not
necessarily attested, words. In Section~5.1.1, we discuss how to define classes
of words by distributional equivalence and thereby replace the set $\Sigma$ of
words by a considerably smaller set $C$ of (strict) {\it categories}. In
Section~5.1.2, we introduce {\it categorial grammar} as a means of
characterizing the set of well-formed expressions, and in Section~5.1.3, we
turn to constituent structure.  \index{English}

\subsection{Reducing vocabulary complexity}

Since the vocabulary of natural languages is large, potentially infinite (see
Corollary 4.4.1), grammarians have throughout the ages relied on a system of
{\it lexical categories} or {\it parts of speech} to simplify matters.
\index{lexical category} \index{part of speech, POS} Originally, these were
defined by similarity of meaning (e.g. {\it adze} and {\it ax}) or similarity
of grammatical function (e.g. both can serve as the subject or object of a
verb).  Both these methods were deemed inadequate by the structuralists:

\begin{quote} 
The school grammar tells us, for instance, that a noun is `the name of a
person, place, or thing'. This definition presupposes more philosophical and
scientific knowledge than the human race can command, and implies, further,
that the form-classes of a language agree with the classifications that would
be made by a philosopher or scientist. Is fire, for instance, a thing? For
over a century, physicists have believed it to be an action or process rather
than a thing: under this view, the verb burn is more appropriate than the noun
fire. Our language supplies the adjective hot, the noun heat, and the verb to
heat, for what physicists believe to be a movement of particles in a body.
... Class meanings, like all other meanings, elude the linguist's power of
definition, and in general do not coincide with the meanings of strictly
defined technical terms. To accept definitions of meaning, which at best are
makeshifts, in place of an identification in formal terms, is to abandon
scientific discourse.  (Bloomfield 1933 Sec. 16.2)
\end{quote}

\noindent 
The method preferred by the structuralists, formalized in contemporary terms
by \newcite{Myhill:1957} and \newcite{Nerode:1958}, relies on similarity of
combinatorical potential. Given a word $w$, we call its {\bf distribution} the
set of all pairs of strings $\alpha,\beta$ such that $\alpha w \beta \in L$.
\index{distribution|textbf} Two words $u, v$ have the same distribution iff

\begin{equation}
\alpha u \beta \in L \Leftrightarrow \alpha v \beta \in L
\end{equation}

\noindent 
{\bf Strict lexical categories} are defined as the equivalence classes $C$ of
(5.1). In Section~3.4 we already introduced a variant of this definition with
$u,v$ arbitrary strings (multiword sequences), and we note here that for any
strings $x,x'$ from class $c_1$ and $y,y'$ from class $c_2$, their
concatenation $xy$ will belong in the same distributional equivalence class as
$x'y'$. Therefore, concatenation is a well-defined binary operation among the
equivalence classes, which is obviously associative since concatenation was
associative.  This operation turns $C$ into a semigroup and, with the addition
of the empty word that serves as the identity, a monoid over $C$ called the
{\bf syntactic semigroup} or {\bf syntactic monoid} associated with $L$.
\index{syntactic semigroup} \index{syntactic monoid} 

The algebraic investigation of syntax is greatly facilitated by the
introduction of these structures, which remain invariant under trivial changes
such as renaming the lexical categories. Needless to say, algebraic structures
bring with themselves the whole apparatus of universal algebra, in particular
{\bf homomorphisms}, \index{homomorphism|textbf} which are defined as mappings
that preserve the operations (not just the semigroup product, but in monoids
also the identity, conceptualized as a nullary operation); {\bf
  substructures}, \index{substructure|textbf} defined as subsets closed under
the operations; and {\bf direct products},\index{direct product|textbf}
defined componentwise.  We shall also avail ourselves of some notions and
theorems more specific to the theory of semigroups and (semi)automata, in
particular {\bf divisors},\index{divisor|textbf} which are defined as
homomorphic images of subsemigroups/sub(semi)automata (denoted $A \prec B$ --
when $A$ is a divisor of $B$, we also say $B$ is a {\bf cover} of
A),\index{cover|textbf} {\it cascade products}, and the powerful Krohn-Rhodes
theory. But before we turn to the general case in Section 5.6, here we
concentrate on the special case where $u,v$ are words, here construed as atomic
symbols of the alphabet rather than as having (phonological or morphological)
internal structure.

By {\bf lexical categorization (category assignment)} we mean the coarsest one
to one (one to many) mapping $f: \Sigma \rightarrow C$ that respects
distributional equivalence. The system of strict categories $C$ employed here
is slightly different from the traditional linguistic notion of lexical
categories, in that we do not permit words to have multiple categories. Where
linguists would assign a word such as {\it run} two categories, Noun (as in
{\it This was a successful run}) and Verb (as in {\it Run along!}), $f$ maps
{\it run} to the unique category NounVerb. Also, the finer distinctions
relegated in linguistics to the level of subcategories, in particular
differences among the range of optional and obligatory complements, are here
seen as affecting the category.  Verbs with a single obligatory argument slot
are called {\bf intransitive}, those with two are called {\bf transitive}, and
those with three are called {\bf ditransitive}.  \index{intransitive|textbf}
\index{transitive|textbf} \index{ditransitive|textbf} As far as (5.1) is
concerned, these are clearly distinct: compare {\it She ate something}
(transitive verb) to {\it *She hiccuped something} (intransitive verb used in
a transitive context). Finally, changes in inflection, viewed by linguists as
leaving the category unchanged, have the effect of changing the strict
categories defined by (1): for example, {\it f(eat)} $\neq$ {\it f(eats})
because e.g. {\it John eat} $\not\in L$ while {\it John eats} $\in L$.
\index{lexical category} \index{strict category|textbf}

Altogether, the system $C$ of strict categories used here is obtained from the
traditionally defined set of lexical categories or {\it parts of speech} (POS)
by (i) making inflection part of the category, (ii) elevating subcategorial
distinctions to full categorial status, and (iii) taking the Boolean atoms of
the resulting system.  Linguistics generally uses a set of only a dozen or so
basic lexical categories, presumed to be common to all languages: the major
classes are Noun, Verb, Adjective, Adverb, and perhaps Preposition (or
Postposition, depending on language) -- given the prominence assigned to these
in school grammars, these will all be familiar to the reader. There is less
agreement concerning minor categories such as Determiner, Pronoun, Quantifier,
etc., and many grammarians permit {\it syncategorematic} elements that are
exempted from the domain of $f$.  By extending the discussion to inflectional
distinctions, we blow up this basic set to a few hundred or a few thousand,
depending on the complexity of the inflectional morphology of the language in
question \cite{Spencer:1998}.  While in principle the move to Boolean atoms
could blow this up exponentially, in practice only very few combinations are
actually attested, so this step is far less important than the (additive)
effect of using strict subcategories. \index{part of speech, POS}
\index{syncategoremata}

For English, well-developed sets of categories (usually called {\it tags} or
{\it POS tags} in \index{tagset} \index{POS tag} corpus linguistics) include
the Brown Corpus tagset comprising 226 categories \cite{Greene:1971}, the LOB
Corpus tagset comprising 153 categories \cite{Johansson:1986}, and the CLAWS2
and C6 tagsets with over 160 categories \cite{Leech:1994}. These numbers are
slightly extended by adding subcategory distinctions within nouns (such as
count noun vs. mass noun, simple vs.  relational, etc.), adjectives
(attributive vs. predicative), and adverbs (manner, time, place, cause,
etc.). A far more significant extension comes from adding subcategory
distinctions for verbs. For example, Levin (1993) distinguished 945
subcategories for English verbs, with no two of these having exactly the same
distribution. The differences are often expressed in terms of {\it selectional
  restrictions}.\index{selectional restriction} For example {\it murder} and
{\it kill} differ in the requirement imposed on the subject: the former
requires a human (compare {\it The assassin murdered him} to {\it *The
  accident murdered him}), while the latter imposes no such restriction ({\it
  The assassin/accident killed him}). \index{English}

But when all is said and done, the move from the specific lexical items in
$\Sigma$ to the strict categories in $C$ brings a great deal of
simplification, especially as it collapses some very large (potentially
infinite) sets such as that of numbers or personal names. Altogether, the
method brings down the size of the alphabet to that of an abridged dictionary,
perhaps $10^3$--$10^4$ strict categories, sometimes called {\it preterminals}
\index{preterminal} in recognition of the fact that they do not correspond
well to either the terminals or the nonterminals in string-rewriting systems.
In this regard, we will follow the practice of contemporary linguistics rather
than that of early formal language theory and think of terminals as either
words or (strict) categories, whichever is more convenient. 

\subsection{Categorial grammar}

Once we collapsed words with the exact same distribution, the next task is to
endow the set of (pre)terminals with some kind of grammatical structure that
will regulate their combinatorical potential. One particularly attractive
method would be to assign each (pre)terminal $p \in C $ a word $s(p)$ from a
free group $G$ with generators $g_0, g_1, \ldots , g_k$ and define a string of
preterminals $p_{i_1}p_{i_2}...p_{i_r}$ to belong in $L$ iff
$s(p_{i_1})s(p_{i_2})...s(p_{i_r})$, as a group-theoretical product, is equal
to a distinguished generator $g_0$ (cf. Example 2.2.2). For reasons that will
become clear from the following discussion, this method is more attractive in
the Abelian than in the general (noncommutative) case, and the latter will
require conventions that go beyond what is familiar from group theory. 

\smallskip
\noindent
{\bf Example 5.1.1} Arithmetical expressions. Let us collect arithmetic
expressions that contain both variables and numbers in the formal language
$A$. For example $(1.3-x)*(1.33+x) \in A$, but $9)(x \not\in A$.  The major
categories, with infinitely many elements, will be Var and Num, and there will
be smaller categories Bin for binary operators $+,-,:,*$ and Uni for the unary
operator $-$. In addition, we need one-member categories OpenPar and ClosePar.
(To eliminate the multiple category assignment of minus, instead of Bin and Uni
we could use StrictBin for $+,:,*$ and BinUni for $-$, but this would only
complicate matters here.)

Since Num and Var give well-formed expressions as they are, we take s(Num) =
s(Var) = $g_0$. Members of StrictBin take well-formed expressions both on the
left and the right, so we take s(StrictBin) = $g_0^{-1}$ -- this will work out
nicely since $g_0 \cdot g_0^{-1} \cdot g_0 = g_0$. (Notice that the system 
slightly overgenerates, permitting the operator to come in Polish or reverse 
Polish order as well -- we will return to this matter later.) 
The case of the minus sign is more complicated: when it is a binary operator,
it is behaving the same way as the other operators and thus should get the
signature s($-$)=$g_0^{-1}$, but when it is a unary operator it can be prefixed
to well-formed expressions and yields a well-formed expression, making it
s($-$) = $I_L$ (left unit). Finally, OpenPar creates an expression that turns
well-formed expressions into ill-formed expressions requiring a ClosePar. To
handle this requires a second generator $g_1$, and we take
s(OpenPar) = $g_1g_0^{-1}$ and s(ClosePar) = $g_1^{-1}g_0$. 

\smallskip
\noindent
{\bf Discussion.} This toy example already displays many of the peculiarities
of categorial grammar writing. First, whenever a preterminal $w$ belongs to
multiple categories, we need to assign it multiple values s($w$). In the
language of arithmetic, this is a rare exception, but in natural language the
situation is encountered quite frequently. The categorial signature assignment
s thus has to be taken as a one-to-many relation rather than a function, and
the key definition of grammaticality has to be modified so that as long as
there is at least one permitted assignment that reduces to $g_0$, the string
is taken to be grammatical. In a string of $r$ words that are all $d$-way
ambiguous as far as their categorial assignment is concerned, we therefore
need to examine $d^r$ products to see if any of them reduce to $g_0$. 

The main difficulty concerns the left unit. In a group, left units are
indistinguishable from right units, and the unit will commute with every
element even if the group itself is not commutative. However, in syntax,
elements that preserve grammaticality can still have order restrictions:
consider, for example, the discourse particle {\it well}.  Ordinarily, if
$\alpha$ is a sentence of English, so is {\it Well,} $\alpha$, but the string
$\alpha$, {\it well} will not be grammatical. To address this issue,
categorial grammars formally distinguish two kinds of {\bf cancellation}, {\bf
  left} and {\bf right}. While in arithmetic $3*(5/3)$ is indistinguishable
from $(5/3)*3$, in categorial grammar we will make the distinction between
$A/B$, a syntactic unit that requires a {\it following B} to make it $A$, and
$B \backslash A$, a syntactic unit that requires a {\it preceding B} to make
it $A$. If the category of a full sentence is $T$, the category of {\it well}
is $T/T$, meaning that it requires something of category $T$ following it to
yield $T$. This is in sharp contrast with group theory, where $g_ig_i^{-1}$
and $g_i^{-1}g_i$ would cancel equally (see Section~6.3.2 for further
details).  By a {\bf cancellation structure} we shall mean the minimal set of
formal expressions generated from a finite set of basic categories $P$ and
closed under the operations $(p \backslash q)$ and
$(p/q)$.\index{cancellation|textbf}

Once cancellation order is controlled, we are in a good position to capture
syntactic conventions about arithmetic operators. In standard notation,
strictly binary operators, such as +, take one argument on the left and one on
the right, so their type is $(g \backslash g)/g$. In reverse Polish notation,
their type is $g \backslash (g \backslash g)$, and in Polish notation
$(g/g)/g$.  Left units, such as the unary minus operator, can no longer attach
anywhere, so expressions like $(23.54-)$ are now ruled out as long as the type
of the unary minus is $g/g$, demanding a well-formed expression to its right.
But the syntax of natural language still presents some difficulties, even with
unary operators like {\it well}. Sentences like {\it Well, well, ..., well,
  John went home} look increasingly strange as the number of {\it well}s
increases.  This effect has no counterpart in (directional) multiplication: if
$I_L$ is a left unit, multiplying by it many times will not change the outcome
in any way. We return to this matter in Section~5.4, where we discuss {\it
  valuations} e.g.  in terms of degree of grammaticalness, but here we ignore
this complication and summarize the preceding discussion in two definitions.

A {\bf unidirectional categorial grammar} is given by a set of preterminals
$P$ and a mapping s:$P \rightarrow 2^G$, where $G$ is a free Abelian group
with a distinguished generator $g$. We say that the string $p_1 p_2 \ldots
p_n$ is accepted by the grammar iff there are elements of $G$ $a_1, a_2,
\ldots a_n$ such that $a_i \in s(p_i)$ and $a_1 a_2 \ldots a_n = g$.
\index{categorial grammar!unidirectional|textbf}

A {\bf bidirectional categorial grammar} or just {\bf categorial grammar} is
given by a set of preterminals $P$ and a mapping s:$P \rightarrow 2^G$, where
$G$ is a cancellation structure with a distinguished generator $g$. We say
that the string $p_1 p_2 \ldots p_n$ is accepted by the grammar iff there are
elements of $G$ $a_1, a_2, \ldots a_n$ such that $a_i \in s(p_i)$ and there is
a bracketing of $a_1 a_2 \ldots a_n$ that cancels to $g$ by a series of left-
and right-cancellations sanctioned by the rules $q(q \backslash p)=(p/q)q=p$.
\index{categorial grammar|textbf}

Categorial grammar (CG) offers an extremely flexible and intuitive method for
sorting out complex syntactic situations: schoolchildren spend years mastering
the complexities that we can now succinctly capture in a handful of categorial
signature assignments. What is particularly attractive about the system is
that the syntax ties to the semantics in exactly the desired manner: the order
of cancellations corresponds exactly to the order of evaluation.  Take for
example $3/2/2$. The categorial signatures are $g,g \backslash g/g,g,g
\backslash g/g,g$ so the product is $g$, making the expression well-formed.
If we begin by evaluating from the left, we first get $3/2=1.5$, and
subsequently $1.5/2=0.75$. If we begin by evaluating from the right, we get
$2/2=1$, and subsequently $3/1=3$.  Remarkably, the same kind of ambiguity can
be observed in natural language. Consider, following Montague (1970a),

\begin{equation} 
\mbox{Every man loves a woman}
\end{equation}

\noindent 
To keep the notation close to Montague's original, the distinguished generator
$g_0$ will be written $T$, and the second generator $g_1$ is written $E$.
(Anticipating developments in Section~6.3, $T$ is mnemonic for $t$ruth value
and $E$ for $e$ntity.) The categorial signatures s are given as follows.
Common nouns such as {\it man} and {\it woman} are assigned the signature $T
\backslash E$.  Quantifiers and determiners have signature $E/(T \backslash
E)$, so that noun phrases come out as $E/(T \backslash E) \cdot (T \backslash
E) = E$.  Finally, transitive verbs such as {\it loves} have signature $E
\backslash T/E$ (i.e. they require an $E$ on both sides to produce the
distinguished generator $T$). As in the arithmetic example, the product s({\it
  every})s({\it man})s({\it loves})s({\it a})s({\it woman})= $E/(T \backslash
E) \cdot T \backslash E \cdot E \backslash T/E \cdot E/(T \backslash E) \cdot
T \backslash E$ can be computed in two essentially different orders, and these
correspond to the two {\it readings} of (5.2), namely

\begin{eqnarray}
\forall x \mbox{man}(x) \exists y \mbox{woman}(y) \mbox{loves}(x,y)\\
\exists y \mbox{woman}(y) \forall x \mbox{man}(x) \mbox{loves}(x,y)
\end{eqnarray}

\noindent 

It would be trivial to extend this grammar fragment\footnote{The presentation
  here is not faithful to the classic Montague (1973) `PTQ fragment' -- see
  Section~6.2 for details.} to cover proper nouns such as {\it John} by taking
s({\it John}) = $E$, and intransitive verbs such as {\it sleeps} by taking
s({\it sleeps}) = $E \backslash T$. This would correctly derive sentences such
as {\it Every man sleeps} or {\it A woman loves John}.  To add prepositions
and prepositional phrases requires another generator $P$. By assigning
prepositions such as {\it to} the signature $P/E$ and to ditransitives such as
{\it gives} the signature $E \backslash T/E/P$, we obtain derivations for
sentences like {\it John gave a book to every woman}.  Prefixing adjectives
like {\it lazy} to common nouns leaves the category of the construction
unchanged: {\it lazy man} has essentially the same distribution as {\it man}.
For this reason, we assign adjectives the signature $(T \backslash E)/(T
\backslash E)$. The same method is available for {\it adadjectives} like {\it
  very}. Since these leave the category of the following adjective unchanged,
we assign them to the category $((T \backslash E)/(T \backslash E))/((T
\backslash E)/(T \backslash E))$.  \index{adjective} \index{adadjective}

\smallskip
\noindent
{\bf Exercise 5.1}. Consider a programming or scripting language with which
you are familiar, such as Fortran, C, Perl, or Python. Write a categorial
grammar that captures its syntax.

\subsection{Phrase structure}

It is a remarkable fact about natural language that some of the more complex
(in linguistic terminology, {\it derived}) constructions from $\Sigma^*$
clearly fit in some of the equivalence classes of lexical items defined by
(5.1).  For example, proper nouns such as {\it John}, and noun phrases such as
{\it every man}, can be substituted for one another in virtually every context
such as {\it Bill visited~$\underline{\ \ }$}. Comparing {\it John didn't go
  home yesterday evening} to {\it ?Every man didn't go home yesterday evening}
(to which speakers strongly prefer {\it No man went home yesterday evening})
makes clear that there are exceptions, but the distributional similarity is so
strong that our preference is to assume identity and fix the exceptions by a
subsequent rule or a more highly ranked constraint.

Not only do proper nouns show near-perfect distributional equivalence with
noun phrases, but the same phenomenon, the existence of distributionally
equivalent lexical entries, can be observed with most, if not all, derived
constructions. The phenomenon is so pervasive that it has its own name, {\it
  lexicality}. Informally, the principle says that for any grammatical
position or role that can be filled by a phrase, there is a single-word
equivalent that can serve in the same position or role.\index{lexicality} In
the special case where the equivalence is between a construction and one of
its components, this component is called the {\bf lexical head} of the
construction. A good example is adjectival modification: {\it lazy old man} is
equivalent to its lexical head {\it man}.  Constructions that contain their
own lexical heads are called {\bf endocentric} and the rest are called {\bf
  exocentric}. Noun phrases formed by determiners or quantifiers are a good
example of the latter. For example, neither {\it every} nor {\it woman} shows
distributional equivalence, or even broad distributional similarity, to {\it
  every woman} since the full construction can appear freely in subject or
object position, while {\it *Every sleeps, *Woman sleeps, *John saw every} or
{\it *John saw woman} are ungrammatical, and conversely, nouns can undergo
adjectival modification while noun phrases cannot ({\it *lazy every
  woman}). When quantifiers undergo similar modification, as in {\it nearly
  every}, the semantics makes it clear that these do not attach to the whole
construction, we have {\it (nearly every) woman} rather than {\it nearly
  (every woman)}.\index{endocentric construction}\index{exocentric construction}

In endocentric constructions, it is nearly always the case that the nonhead
constituents are entirely optional. This phenomenon, though not quite as
general as lexicality, has its own name: {\it optionality}.  For constructions
composed of two parts, optionality follows from the definition: if $uv$ is
equivalent to $v$, this means $u$ was optional. For larger constructions, the
only exceptions seem to be due to agreement phenomena: for example, {\it Tom,
  Dick, and Harry} is equivalent to {\it Dick and Harry} but not to any smaller
substring since the construction (e.g. in subject position) demands plural
agreement. Even exocentric constructions, where there is no clear candidate
for head or for deletable elements, show evidence for not being composed of
parts of equal importance, and we commonly find the term {\it head} extended
to the most important component. \index{head}

To the extent that there seems to be something of an Artinian condition (no
infinite descending chain) on natural language constructions, we may want to
inquire whether there is a Noetherian condition (no infinite ascending chain)
as well. A construction whose head is some lexical category $c$ is said to be
a {\bf projection} of $c$: the idea is that we obtain more and more complex
constructions by successively adjoining more and more material to the head
lexical entry. Can this process terminate in some sense? Linguistics has
traditionally recognized {\bf phrases} as maximal projections (i.e. as
constructions that can no longer be extended in nontrivial ways). The most
important example is the noun phrase, which is effectively closed off from
further development by a determiner or quantifier. Once this is in place,
there is no further adjective, numeral, or other modifier that can be added
from the left (compare {\it *three the books, *three every books} to {\it the
  three books, every three books}) and only relative clauses are possible from
the right ({\it the three books that I saw yesterday}). Once such a {\it
  that-}clause is in place, again there is no room for different kinds of
modifications. Further relative clauses are still possible ({\it the three
  books that I saw yesterday that you bought today}), but no other kind of
element is. Other notable examples include the verb phrase (VP), the
prepositional phrase (PP), the adjectival phrase (AP), and the adverbial
phrase (AdvP) -- since this covers all major categories, it is commonly
assumed that every construction is part of a maximal (phrasal) construction
that can be further extended only by the trivial means of coordination.
\index{projection} \index{maximal projection} \index{phrase|textbf}
\index{verb phrase, VP} \index{prepositional phrase, PP}
\index{adjectival phrase, AP} \index{adverbial phrase, AdvP}\index{dependent}

Another observation connects nonheads (also called {\it dependents}) to
phrases: in most constructions, dependents are freely substitutable for their
maximal projections. Thus, where a construction has a dependent adjective,
such as {\it traditional} in {\it the traditional dish}, it can also have a
full adjectival phrase, as in {\it the exceedingly traditional dish}. The
phenomenon that dependents are maximal projections is common enough to have a
name of its own: {\it maximality}.

There are two formally rather different systems of syntactic description built
on these insights. The first, {\it dependency grammar} (DG), uses directed
graphs where arcs always run from dependents to heads (in some works, the
opposite convention is used), and the second, {\it X-bar theory}, uses
string-rewriting techniques. Historically, dependency grammar has not been
very concerned with word order, its main focus being the relationships that
can obtain between a head and its dependents. String rewriting, on the other
hand, does not really have the apparatus to deal with grammatical function,
valence, case, and similar matters occupying the dependency grammarian, its
primary focus being the hierarchical buildup of sentence structure. In the
remainder of this section, we concentrate on X-bar theory, leaving case,
agreement, valence, and related issues to Section~5.2.

The key to hierarchical structure is provided by optionality, which enables us
to gradually reduce complex constructions to simpler ones. This is the method
of {\it immediate constituent analysis} (ICA), \index{immediate constituent analysis, ICA} \index{constituents} 
which relies on extending the relation
(5.1) from $\Sigma$ to $\Sigma^*$.  Recall Definition 3.4.2: two strings
$\gamma, \delta$ are distributionally equivalent whenever

\begin{equation}
\alpha \gamma \beta \in L \Leftrightarrow \alpha \delta \beta \in L
\end{equation}

\noindent 
The key insight, due to Chomsky (1956), was to notice that the typical
reduction step in ICA does not require distributional equivalence, just
substitutability in positive contexts:

\begin{equation}
\alpha \gamma \beta \in L \Rightarrow \alpha \delta \beta \in L
\end{equation}

\noindent 
To make this concrete, consider the original example from Wells (1947): {\it
  The king of England opened Parliament}.\nocite{Wells:1947} We wish to find
how this sentence is built from parts and recursively how the parts are built
from smaller parts, etc. ICA begins with locating the structural break between
{\it England} and {\it opened} (rather than, say, between {\it of} and {\it
  England}) based on the observation that {\it opened Parliament} can be
substituted by {\it slept} without loss of grammaticality and {\it The king of
  England} can be similarly substituted by {\it the king}. It is clear that
the substitution works well only in one direction, that of simplification. For
example, the construction {\it a bed rarely slept in} will not tolerate
substitution in the other direction.  The famous rule

\begin {equation}
\mbox{S} \rightarrow \mbox{NP VP}
\end{equation} 

\noindent 
captures exactly this step of the ICA. The sentence is analyzed in two 
constituents, a (subject) NP {\it The king of England} and a verb phrase 
{\it opened Parliament}. 

In the original notation of Wells (1947), the constituents were described by
boundary symbols $|,||,|||,\ldots$ of ever-increasing strength, so that the
sentence would come out {\it The $|$ king $||$ of $|$ England $|||$ opened $|$
  Parliament}, but this has been superseded by {\it
  bracketing},\index{bracketing} which would make the example come out as {\it
  (((The king) (of England)) (opened Parliament)).}  In general, to formalize
ICA by string rewriting, we need to adjoin new {\it nonterminal} symbols to the
(pre)terminals found in $C$ or $\Sigma$ so as to cover constructions
(multiword constituents).  Lexicality means that the new symbols can be
patterned after the ones used for categories. Traditionally, the same symbols,
with superscript bars (hence the name X-bar), are reused so that e.g. nouns are
N, bare noun phrases are $\overline{\mbox{N}}$, and full noun phrases are
$\overline{\overline{\mbox{N}}}$. We leave open the issue of whether there is
exactly one bar level between the lexicon and maximal phrases and use the
notation $X^M$ to denote a maximal projection of $X$.  (For many minor lexical
categories $X^M=X$; i.e. no complex phrases of that type can be built.)  If we
attach the nonterminal to the opening parenthesis of each constituent, we speak
of {\bf labeled bracketings}.\index{labeled bracketing|textbf} Just as
scanning codes (see Section~3.4) offer a simple linearized version of an
association relation, (labeled) bracketings offer a simple linearization of
parse trees, and many questions about tree structures can be recast in terms
of the code strings (Chomsky and Sch\"{u}tzenberger 1963).

Since (5.5) defines an equivalence relation, $L$ itself is the union of some
equivalence classes. To the extent that sentences are freely substitutable for
one another, $L$ is covered by a single symbol $S$, the start symbol of
string-rewriting systems. Lexicality in this case is traditionally taken to
mean that $S$ is a projection of the main verb -- in more modern accounts it
is often some abstract property of the verb, such as its tense marking, that
is taken to be the head of the entire sentence. Either way, rule (5.7) amounts
to the statement that the NP on the left (the subject) completes the VP and
thus S has one more bar than VP.

In a string-rewriting formalism, maximality means that only rules of the type
$X^n \rightarrow Y_1^M \ldots Y_l^M X^{n-1} Z_1^M \ldots Z_r^M$ are possible.
Counterexamples are not easy to find but they exist; consider infinitival
clause complements to verbs. Some verbs, such as {\it want}, can take both
subjectful and subjectless clauses as their complement: compare {\it John
  wanted Bill to win} and {\it John wanted to win}. Other verbs, such as{\it
  try}, only take subjectless clauses: {\it John tried to win, *John tried Bill
  to win}.  By analogy to tensed sentences, the tenseless {\it Bill to win}
has one more bar level than {\it to win} since the subject {\it Bill} adds
one bar. If this is so, the rule expanding {\it try}-type VPs has one fewer
bar on the clausal complement than the rule expanding {\it want}-type verbs,
meaning that maximality is violated in the former. Altogether, maximality
remains one of the fascinating near-truths about syntax, and trying to come to
grips with this and similar counterexamples remains an active area of
research.

Given the special role that coordination plays in the system, one technique
of great practical significance is {\bf extending} context-free grammars by
permitting rules of the form $A \rightarrow r$, where $r$ is any regular
expression over the alphabet composed of both terminals and nonterminals: for
any string $w$ that matches $r$, we say that A can be rewritten as $w$.  It is
easy to see that this extension does not change the generative capacity of the
system (if a language can be described by means of an extended CFG, it can also
be defined by an ordinary CFG), yet the perspicuity of the system is
significantly increased.  \index{context-free grammar, CFG!extended}

\smallskip
\noindent
{\bf Exercise 5.2} Prove that extended CFGs only generate CFLs. 

\smallskip
\noindent 
All in all, (extended) CFGs offer an extremely transparent and flexible method
of syntactic analysis. Several rather different-looking grammatical traditions
are equivalent, at least in some formulation, to CFGs: Postal
(1964)\nocite{Postal:1964} argued this for tagmemics (Pike
1967),\nocite{Pike:1967}\nocite{Bar-Hillel:1960} and Bar-Hillel et al (1960)
made the argument for categorial grammars. Yet CFGs can be faulted both for
over- and undergeneration. On the one hand, CFGs can easily handle the problem
of balanced parentheses of arbitrary depth.  This is obviously helpful
inasmuch as CFGs could not have gained their prominence as the primary means
of syntactic analysis for programming languages without the ability to
correctly handle parenthesized expressions of arbitrary depth, but in the
analysis of natural languages, arbitrary depth constructions are rarely
encountered, if at all. On the other hand, there is a class of {\it copying}
phenomena, ranging from cross-serial dependencies (see Section~5.2.2) to
constructions such as {\it Freezing cold or no freezing cold (we will go
  barefoot)}, that is outside the CFL domain, see 
\newcite{Manaster-Ramer:1986}.

Exploring combinatorical mechanisms that transcend the limitations of CFGs has
been, and continues to be, a major avenue of research in mathematical
linguistics. The general problem of {\it discontinuous constituents}, i.e.
\index{tree adjoining grammar, TAG}\index{head grammar}
\index{combinatory categorial grammar, CCG}\index{constituents}\index{discontinuous constituents}%
constituents that are interrupted by material from other
constituents, was noted early by the developers of ICA -- for a current
summary, see Ojeda (2006b).\nocite{Ojeda:2006} The fact that cross-serial
dependencies are problematic for CFGs was noted by Postal (1964), who cited
Mohawk [MOH] in this regard -- later work concentrated on Germanic languages,
in particular Dutch and Swiss German.  The first formalism to address the
issue was {\it tree adjoining grammar} (TAG -- for a modern summary, see
\nocite{Joshi:2003} Joshi 2003) and, perhaps even closer to the original
spirit of ICA, {\it head grammars} \cite{Pollard:1984}. Among categorial
grammars, {\it combinatory categorial grammar} (CCG) provided the same kind of
extension to discontinuities; see \newcite{Steedman:2001}. (Note that where it
does not clash with established mathematical usage, we distinguish {\it
  combinatorial}, `pertaining to combinators', from {\it combinatorical}
`pertaining to combinatorics' -- CCG uses combinators.) Readers interested in
further details are directed to Chapter~5 of \newcite{Kracht:2003}, which
discusses all key developments in this area.  \index{Mohawk [MOH]}

\section{Grammatical theories}

The approaches that put the emphasis on string, tree, or category manipulation
are rather new (less than a hundred years old) compared with traditional
notions such as subject, object, or case, which grammarians have invoked for
millennia in accounting for syntactic regularities. The primary impulse of the
combinatorical theories is to do away with these and similar notions entirely;
for example, Chomsky (1981:59) uses (5.7) to define subject as ``NP of S"
(i.e.  as the NP that appears in the rule rewriting S), predicate as ``VP of
S", and object as ``NP of VP". Such definitions imply that there is no
explanatory role for these notions in grammar, that they are purely
epiphenomenal, serving at best as convenient abbreviations for constituents in
some frequently seen combinatorical configurations. Yet grammarians not only
persist in using these notions but in fact the class of theories that rely
crucially on them, what we call `grammatical theories' for lack of a better
umbrella term, has undergone intense development in the past forty years.

The first generative model to give primacy to a grammatical notion of case
over the combinatorical notions of constituency and category was {\it case
  grammar} \cite{Fillmore:1968}, and we will cover {\it role and reference
  grammar} \cite{Valin:2005}, and {\it relational grammar}
\cite{Perlmutter:1983}, as well as classical P\={a}\d{n}inian {\it
  morphosyntax} and its modern variants (Ostler 1979, Kiparsky 1987, Smith
1996).  \nocite{Kiparsky:1987} \nocite{Ostler:1979} \nocite{Smith:1996} Over
the years, even mainstream combinatorical theory (Chomsky 1981, 1995) came to
incorporate modules of {\it case theory} and {\it theta theory} that employ
such grammatical devices.

With the notable exception of {\it lexical functional grammar} (LFG; see
Bresnan 1982)\nocite{Bresnan:1982}\nocite{Chomsky:1981}\nocite{Chomsky:1995}
and a variety of unification grammars, which came mathematically fully
articulated, the formal theory has not always kept up with the insights gained
from the grammatical work. In Section~5.2.1, we take the first steps toward
formalizing some of the key ideas, starting with dependency, agreement,
surface case, and government. Deep cases and direct linking are discussed in
Section~5.2.2,\index{lexical functional grammar, LFG} and grammatical
functions and indirect linking are discussed in Section~5.3.

\subsection{Dependency} 

Classical grammar has imbued our culture to such an extent that many of its
technical notions, such as {\it subject, object}, or {\it predicate}, are
applied almost unthinkingly in scientific discourse. So far, we have used
these terms without much reflection, essentially in the same somewhat vague
but nevertheless widely approved sense as they are used by all people past
grammar school. To see what motivates the use of such devices, consider first
sentence pairs such as

\begin{eqnarray}
\mbox{The farmer killed the duckling}\\
\mbox{The duckling was killed by the farmer}
\end{eqnarray}

\noindent 
Clearly there is a very close paraphrase relationship between (5.8), known as
an {\it active} sentence, \index{active} \index{passive} and (5.9), known as a
{\it passive}: it seems impossible to imagine a state of affairs in which one
is true and the other is false. There is more to active/passive pairs than
semantic relatedness; the constructions themselves show deeper parallelism.
Whatever selectional relationship obtains between the active verb and its
object is carried over to the passive verb and its subject, and whatever
relationship obtains between the active verb and the subject is replicated
between the passive verb and the agentive {\it by}-phrase. 

For Chomsky (1957) and much of the subsequent tradition of transformational
generative syntax, these observations were taken as indicative of a need to
treat (5.8) as basic and (5.9) as derived, obtained from (5.8) by means of a
{\it passive transformation} \index{passive transformation} that rearranges
the constituents of the active construction and supplies the requisite
grammatical formatives to yield the passive. Unfortunately, the derivational
analysis creates as many problems as it solves. In particular, we find actives
with no passive counterparts, such as {\it John resembles Bill} and {\it *Bill
  is resembled by John.} These require some special mechanism to block the
passive transformation.  Even more problematic are passives with no active
counterparts, such as {\it *Everyone said John to be honest/John was said to be
  honest by everyone}, since these call for an obligatory application of the
transformation and thus rely on an abstract {\it deep structure.} This is
extremely challenging from the perspective of language acquisition since the
learner has to reverse-engineer the path from deep structure to surface based
on an opaque surface structure. \index{opacity, syntactic}

That something of a more abstract sort than surface word order or constituency
is required can hardly be doubted, especially if we consider a wider range of
alternations. Compare, for example, {\it The butcher cuts the meat/The meat
cuts easily} to {\it Kelly adores French fabrics/*French fabrics adore easily}
or {\it Jack sprayed the wall with paint/Jack sprayed paint on the wall} to
{\it June covered the baby with a blanket/*June covered the blanket over the
baby}. (The examples are from \cite{Levin:1993}, where a broad range of
similar alternations are discussed.) Traditional grammatical theories, having
been developed on languages that show overt case marking, generally use an
abstract version of {\it case} as the key mechanism to deal with the
subtleties of syntax. 

What is case? Structuralist theories of syntax such as \newcite{Jakobson:1936}
took case to be at the confluence of three major domains: morphology,
semantics, and syntax. From the morphological standpoint, {\it surface} cases
are noun affixes common to all but the most isolating of languages. The
details of case systems vary greatly, from languages such as Hindi that have
only two, nominative and oblique (plus a vestigial vocative), to languages
such as Hungarian with seventeen and Tsez [DDO] with over sixty
morphologically distinct cases. The typical {\it nominative/accusative} system
will include locative cases, accusative, dative, and perhaps some more
specialized cases, such as instrumental and genitive (by convention, all
nonnominative cases except the vocative are called oblique).  Latin, which
lacks an instrumental case but has a vocative and two locatives, or Russian,
are good examples of the typical pattern. \index{case} \index{oblique}
\index{accusative} \index{locative} \index{vocative} \index{dative}
\index{instrumental} \index{genitive} 
\index{nominative/accusative case pattern} \index{Hindi} \index{Hungarian} \index{Russian} \index{Latin}
\index{Tsez [DDO]}

From the semantic standpoint, cases correlate with the role the nouns or NPs
marked by them play in the sentence. In particular, vocative marks the one the
sentence is addressed to, locatives describe where the action or event
described by the sentence takes place, the dative is associated to the
recipient or beneficiary, the instrumental to instruments, and genitive to
possession.  The correlation between morphological marking and semantic
content is far from perfect. For example, in many languages, the same dative
morphology will express both recipients and experiencers. Much of traditional
syntax is concerned with enumerating the discrepancies, e.g. the `dative of
separation' that occurs with some verbs instead of the expected ablative, the
`genitive of material' which denotes a relationship of `being composed of' as
in {\it a bar of gold} rather than a relationship of possession, and so forth
-- we will return to this matter in Section~5.3.

Finally, from the syntactic side, cases correlate with grammatical function:
subjects are typically marked with the nominative case, objects with the
accusative case, indirect objects with the dative case, and so forth.  Again it
has been recognized from the earliest times that the correlation between
grammatical function and case marking is imperfect. On the one hand, we find
what appears to be the same functional relation expressed by different cases,
and on the other, we find that one and the same case (e.g. the accusative) can
serve in different functions. Even so, the syntactic impact of case is
undeniable. In particular, it is quite clear that in languages with overt case
marking, the noun phrases that carry these marks can generally be permuted much
more freely than in languages such as English that lack overt case marking. For
example, in Latin, we can have {\it anaticulam agricola occisit} or any of the
other five permutations of {\it duckling.ACC farmer.NOM kill.PAST} and the
meaning of the sentence remains the same as that of (5.8). \index{Latin}

Here we formalize some essential aspects of grammatical theories, while
leaving many issues open. From a combinatorical perspective, the easiest of
these is {\it agreement (concord)}, which obtains between two elements of a
construction.  \index{agreement|textbf} \index{concord} The minimum
requirement for agreement is a set of two words $u$ and $v$ and a paradigmatic
dimension $D$ that can take at least two values $D_1, D_2$.  For example, the
words can be the subject and the predicate, and the dimension can be number,
taking the values singular and plural. With the standard {\it glossing}
notation (see Section~2.3), where forms are given as $x.D$ with $x$ being a
stem and $D$ some paradigmatic value, we have the following definition.

\smallskip
\noindent
{\bf Definition 5.2.1} If for all values $D_i$ the strings $\alpha u.D_i \beta
v.D_i \gamma \in L$ while for all $i \neq j$ we have $\alpha u.D_i \beta v.D_j
\gamma \not\in L$, we say that $u$ and $v$ {\bf agree} in $D$. 

\smallskip
\noindent
In a string-rewriting formalism, agreement is handled by means of complex
symbols: instead of (7) we write $S \rightarrow NP.D_i VP.D_i$ (the . is used
here as part of the morphemic gloss rather than as a pure concatenation
marker). Since words can agree in more than one dimension (e.g. the subject
and the predicate will generally agree both in number and in person), a rule
schema like this can be thought of as subsuming as many rules as there are
$D_i$. This brings to light the phenomenon of {\it internal agreement}, when a
construction as a whole agrees with some part of it; e.g. {\it boys} and {\it
  lazy boys} are both plural, and {\it boy} and {\it lazy boy} are both
singular.  Since constructions typically agree with their heads, the head
feature convention (HFC; see Gazdar et al. 1985) \nocite{Gazdar:1985}
\index{Head Feature Convention, HFC} stipulates that internal agreement
between constructions and their heads is automatic and it is suspension,
rather than enforcement, of this rule that requires a special rule.

Note that more than two words can participate in agreement. For example, in
Georgian, the predicate agrees not just with the subject but with the object
(and in certain circumstances the indirect object) as well -- for a fuller
discussion, see Anderson (1992 Sec. 6.1). The definition naturally extends to
situations where one of the words carries the feature inherently rather than
through explicit morphological marking. For example, in Russian, adjectives
agree in gender with the nouns they modify. The nouns do not exhibit gender
variation: they are feminine, masculine, or neuter, and have no means of
assuming a different value for gender as they could for number. In the typical
case of concord it is not clear which of the two $D_i$ is the cause and which
is the effect. Traditionally we say that the choice of e.g.  first-person
subject forces first-person verbal morphology on the predicate, but we could
equally well say that the choice of first-person verbal morphology forces the
use of a first-person subject. In the Russian case described above, the
direction of the value assignment is clear: only the adjective has a range of
values, so agreement comes from the noun imposing its value on the adjective
and not the other way around. Such cases are therefore also called {\it
government}. 

\smallskip
\noindent
{\bf Definition 5.2.2} If in a construction $\alpha u.D_i \beta v.D_i \gamma
\in L$ the value $D_i$ is an inherent lexical property of $u$ ($v$), we say
that $u$ governs $v$ ($v$ governs $u$) if $\alpha u.D_i \beta v.D_j \gamma
\not\in L$ ($\alpha u.D_j \beta v.D_i \gamma \not\in L$) for any $j \neq i.$
\index{government|textbf}

\smallskip\noindent In dependency grammar, the key diagnostic for assuming a
dependency link between two items is agreement. In the special case of
government, the direction of the link is set by convention so that it runs
from the dependent to the head (governor). Since verbs can impose case marking
on their complements, verbs are the heads, and not the other way around, a
conclusion that in phrase structure grammar would be reached from a different
fact, the optionality of the dependents. The characterization of the various
dependency relations that can obtain between verbs and their dependents is a
primary concern of both {\it case grammar}, which uses an inventory of
abstract {\it deep cases} to describe them, and {\it relational grammar},
which uses the grammatical functions subject (called `1' in RG), object (`2'),
indirect object (`3'), as well as more deep case-like functions such as
Benefactive and Locative. Other relations of note are {\it modification}, as
obtains between an adjective and a noun or an adverbial and a verb, {\it
  possession} between nouns and nouns, and {\it coreference} between pronouns
and nouns.  \index{possession} \index{coreference} \index{modification}

Once an inventory of relations is fixed, \index{dependency grammar|textbf} it
is natural to formulate {\bf dependency grammar} in terms of directed graphs:
words or higher constituents are vertices and the relations are the edges. A
closely related formalism is that of {\it tagmemics}, which uses abstract
construction types (called tagmemes) that have empty {\it slots} that require
{\it fillers}. \index{tagmemics} To this simple dependency apparatus, {\bf
  relational grammar} adds an ordered set of {\it strata}: conceptually each
stratum corresponds to a stage in the derivation.  \index{relational grammar|textbf} \index{stratum} Here we go further, permitting edges to run
directly between nodes and other edges as in traditional {\it sentence
  diagrams} (Reed and Kellog 1878, Kolln 1994, Klammer and Schultz 1996).
\index{sentence diagram} \nocite{Reed:1878} \nocite{Kolln:1994}
\nocite{Klammer:1996} We take the graph visualization as secondary, and the
primary formalism is given as an algebraic system.  For reasons of wider
applicability (see in particular Section~5.3) rather than defining a single
system, we define a metasystem MIL that can be instantiated differently
depending on the choice of atoms and operations.\index{MIL|textbf}

\bigskip 
\noindent 
{\bf Definition 5.2.3} (i) The atomic elements of MIL form a finite set $A$. 

\smallskip
\noindent  
(ii) The primitive operations of MIL are \&, =, and perhaps also finitely many
binary operations $P_1, P_2, \ldots, P_n$. \& will also be denoted by $P_0$,
and we use $H$ as a (metalanguage) variable ranging over the $P_i
(i=0,1,\ldots,n)$.

\smallskip 
\noindent 
(iii) If $p$ and $q$ are arbitrary elements, $Hpq$ will be an element, and
$p=q$ is an (elementary) statement. The only predicate of MIL is `='.
$x,y,z,\ldots$ will be (metalanguage) variables ranging over elements. 
  
\smallskip
\noindent  
(iv) The system is defined inductively: the only elements, operations, and
statements of MIL are those resulting from the iterated application of (iii). 
  
\smallskip
\noindent  
(v) The axioms of MIL (using prefix notation for operations and infix for 
equality) are
\begin{eqnarray*}
 x = x\\
 Hx\&yz = \&HxyHxz\\
 \&xx = x\\
 H\&xyz = \&HxzHyz\\
 \&xy = \&yx\\
\end{eqnarray*}


\smallskip
\noindent  
(vi) The rules of deduction are
\begin{eqnarray*}
\frac{x=y}{y=x} \; \; \; \; \; \; \; \; \frac{x = y,y = z}{x=z} \; \; \; \; \; \; \; \; \frac{x = y}{Hxz = Hyz} \; \; \; \; \; \; \; \; \frac{x = y}{Hzx = Hzy}
\end{eqnarray*}

\smallskip 
\noindent 
The elements of the algebraic structure can be thought of as {\bf dependency
  diagrams}. These are directed labelnode hypergraphs of a special kind,
differing from ordinary graphs only in that edges can also run to/from other
edges, not just nodes. What the operations specify are not the diagrams, just
formulaic descriptions of them. Different formulas can describe the same
object, depending on the order in which the graph was built up. The
equivalence `=' makes it possible to define a conjunctive normal form with
respect to \&.  To put it in other words, MIL is a free algebra over a finite
set $A$ generated by the binary operations $P_0, P_1, \ldots, P_n$ satisfying
the equations (v).  Since the rules of deduction in (vi) make = compatible
with the operations, = is a congruence, and its classes can be represented by
terms in conjunctive normal form. \& corresponds to union of subgraphs, with
concomitant unification of identical atomic nodes.

To apply the formalism to dependency grammar, we must take words (or
morphemes) as the primitive elements and the set of deep cases and other
relations as the primitive operations. To apply it to relational grammar, we
need to use a different primitive operation $P_{i,c_j}$ for each relation $i$
and each stratum $c_j$. To connect MIL formulas in normal form to the more
standard graph notation, depict $P_iab$ as a directed graph with vertices $a$
and $b$ and a directed edge labeled $P_i$ running from $a$ to $b$. Note that
a nested formula such as $P_ia(P_jbc)$ does not have a trivial graph-theoretic
equivalent since the edge $P_i$ now runs from the vertex $a$ to the {\it
edge} $P_jab$ as in (5.10.i). This frees the formalism from the spurious nodes
introduced in \newcite{Gaifman:1965}, making it perhaps more faithful to the
original grammatical ideas. 

\begin{equation}
\end{equation}

\noindent
(i)\vbox{\xymatrix{ & b \ar[dd]^(.3){P_j} \\ a\ar[r]_{P_i} &{} \\ & c}}%
\hbox{\phantom{ww}}(ii)%
\vbox{\xymatrix{{} \\ a\ar[r]^{P_i} & b \\
           c\ar[r]^{P_j} & d}}\phantom{ww}(iii)%
\vbox{\xymatrix{ {} \\ a &  b \ar[l]_{P_i} \ar[r]^{P_j} &  c}}\phantom{ww}(iv)%
\vbox{\xymatrix{{} \\ a\ar[r] & b \\
          a\ar[r] & d}}

\medskip
\noindent
To build more complex dependency graphs, the operation \& is interpreted as
(graph-theoretic) union: expressions like $\&P_iabP_jcd$ are as depicted in
(5.10.ii).  Expressions like $\&P_iabP_jac$ correspond to structures like
(5.10.iii) -- since atoms have to be unique, structures like (5.10.iv) cannot
be formed.  The axioms in (v) and the rules of deduction in (vi) serve to make
indistinguishable those structures that differ only in the order in which they
were built up. The temporal aspect of the derivation, to the extent it is
taken to be relevant (in particular in relational grammar), is modeled through
explicit temporal indexing of the edges.

An intuitively very appealing way of formulating dependency theories is to
invoke a notion of {\it valences} in analogy with chemical valences.
\index{valence} Under this conception, grammatical constructions are viewed as
molecules composed of atoms (words or morphemes) that each have a definite
number of slots that can be filled by other atoms or molecules. One key issue
is the strength of such valences. Consider for example

\begin{eqnarray}
\mbox{[John]}_1 \mbox{ rented} \mbox{ [the room]}_2 
\mbox{ [from a slumlord]}_3 \mbox{ [for a barbershop]}_4  \nonumber \\
\mbox{[for the first three months]}_5 
\mbox{ [for fifteen hundred dollars]}_6 
\end{eqnarray}

\noindent There are a total of six complements here, and most of them are
optional: leaving their slots unfilled leaves a less informative but still
evidently grammatical sentence such as {\it John rented}. In English at least,
the subject (complement 1) is obligatory {\it (*rented a room)}, and we find
many verbs such as {\it admit} or {\it transcend} that positively require an
object (complement 2) as well: consider {\it *John admits} or {\it *John
  transcended}. Ditransitive verbs require an indirect object (complement 3)
as well; compare {\it *John gave, *John gave Bill}, and {\it *John gave a last
  chance} to {\it John gave Bill a last chance}.  These constructions are
perhaps analogous to radicals such as CH$_3$, which are not found in nature
(because they are so highly reactive that the valence gets filled almost
immediately).  One point where the chemical analogy may break down is the
existence of situations where one slot is filled by more than one filler.
Superficially, this is what appears in (5.11) with complements 4--6, which are
all prepositional {\it for}-phrases -- the notion of deep cases is introduced
precisely with the goal of separating such complements from one another, see
Section~5.2.3 for further details.  P\={a}\d{n}ini distinguishes six deep
cases: Agent, Goal, Recipient, Instrument, Locative, and Source (see
\nocite{Staal:1967} Staal 1967). The exact inventory is still heavily
debated.\index{P\={a}\d{n}ini}

\subsection{Linking}

In the Western tradition of grammar, action sentences like (5.8 and (5.9) are
viewed as prototypical, and for the moment we will stay with them.  Such
sentences always have a main verb, and here we assume that verbs come with a
{\bf case frame} that specifies (i) the grammatical relation that the verb has
to its dependents, (ii) the case or other marking that expresses this
relation, (iii) the syntactic category of the complements, (iv) the obligatory
or optional nature of the complements, and (v) the subcategorization
restrictions the head places on the complements. The primary goal of syntax,
at least in regard to the prototypical class of sentences, is to specify how
the various entities (NPs) named in the sentence are linked in the argument
slots.

Well-developed grammatical theories that rely on case frames include classical
DG, head-driven phrase-structure grammar (HPSG; see Pollard and Sag 1987),
\nocite{Pollard:1987} lexical-functional grammar (LFG; see Bresnan et
al. 1982),\nocite{Bresnan:1982} case grammar \cite{Fillmore:1968}, role and
reference grammar (Foley and van Valin 1984, van Valin
2005),\nocite{Foley:1984}\nocite{Valin:2005} relational grammar
\cite{Perlmutter:1983}, as well as classical P\={a}\d{n}inian morphosyntax and
its modern variants (Ostler 1979, Kiparsky 1987, Smith 1996).
\nocite{Ostler:1979} \nocite{Kiparsky:1987} \nocite{Smith:1996} This is not to
say that all these theories are identical or even highly similar. All that is
claimed here is that a linking mechanism, expressed one way or another, is an
integral part of their functioning. (Note also that many would take exception
to calling this structure a {\it case} frame, preferring, often for reasons
that make a lot of sense internal to one theoretical position or another,
names such as {\it subcategorization frame, lexical form}, or {\it thematic
  grid}.)

Consider again the sentences (5.8) and (5.9). In the active case, we simply
say that the transitive frame shared by {\it kill} and many transitive verbs
requires a subject and an object NP (or perhaps an agent and a patient NP,
depending on whether our terminology is more grammatically or semantically
inspired). As long as we accept the notion that in Latin the relationship of
subjecthood (or agency) that holds between {\it the farmer} and the act of
killing is expressed by overt case marking, while in English the same
relationship is expressed by word order, the essence of the analysis remains
constant across languages.  This is very satisfactory, both in terms of
separating meaning from form (e.g.  for the purposes of machine translation)
and in terms of capturing variation across languages in typological terms.

In the passive, the central part of the construction is the verbal complex
{\it was killed}, which also has two slots in its frame, but this time the
second one is optional. The first slot is that of the subject (a passive
experiencer rather than an active agent) and must be filled by an NP, while
the second, that of the agent, can be left open or be filled by a {\it
  by}-phrase. There is no sense that (5.9) is derived from (5.8); these are
independent constructions related only to the extent that their main verbs are
related. Unlike syntax, which is expected to be fairly regular in its
operation, the lexicon, being the repository of all that needs to be
memorized, is expected to be storing a great deal of idiosyncratic material,
so the appearance of idiom chunks such as {\it was said to be} that have
their own case frames (in this instance, that of a subject NP and a
predicative AP, both obligatory) comes as no surprise.

We still want to say that {\it resemble} is an exception to the lexical
(redundancy) rule that connects transitive verbs {\it V-ed} to their passive
form {\it was V-en}, and the case frame mechanism offers an excellent
opportunity to do so. Instead of taking the verb as a regular transitive
(which demands an accusative object), we treat the second complement as
nonaccusative (e.g. goal) and thereby exempt the verb from the passive rule
that operates on true transitives. Such an analysis is strongly supported
cross-linguistically in that in many other languages with overt case endings,
the second complement is indeed not accusative.

If we connect any specific theory of syntax (among the many variants sketched
so far) to a theory of morphological realization rules (see Section~4.2), we
obtain something approaching an end-to-end theory of action sentences,
starting with their logical form and going ultimately to their phonetic
realization. In a somewhat confusing manner, both the overall directionality
from meaning to form and a specific theory of syntax within transformational
grammar that uses this direction are called {\it generative semantics.} (The
reverse direction, progressing from syntactic form to some kind of meaning
representation, is called {\it interpretative semantics} and will be discussed
in Chapter~6.) Many theories, such as HPSG (Pollard and Sag 1987) or role and
reference grammar (RRG; see van Valin 2005), are purposely kept neutral
between the two directions.  \nocite{Pollard:1987} \nocite{Valin:2005}


To get a richer description of syntax, we need to supplement case frames by a
number of other mechanisms, in particular by rules for dealing with free
adverbials and other material outside the case frame. In (5.11), many
grammarians would argue that the goal adverbial complement 4, {\it for a
barbershop}, is outside the core argument structure of {\it rent}: everything
can be said to happen with some goal in view so this is a free adverbial
attached to the periphery rather than the core of the structure. Other
supplementary mechanisms include language-particular or universal word order
rules, rules that govern the placement of dependents of nouns (adjectives,
possessors, determiners), the placement of their dependents, and so forth. 

Even with these additions, the case frame system has a clearly finitistic
flavor.  For every slot, we need to specify (i) the grammatical relation,
taken from a small (typically $\leq 6$) inventory, (ii) the case or other
marking that expresses the relation, again taken from a small (typically $\leq
20$) inventory, (iii) the syntactic category, typically a maximal major
category, again taken from a small (typically $\leq 5$) inventory, (iv)
whether the complement is obligatory or optional, a binary choice, and (v)
subcategorization restrictions, generally again only a few dozen, say $\leq
50$, choices. Since a verb can never have more than five slots, the total
number of case frames is limited to $(6 \cdot 20 \cdot 5 \cdot 2 \cdot 50)^5$,
a number that at $7.7 \cdot 10^{23}$ is slightly larger than Avogadro's number
but still finite. The size of this upper bound (which could be improved
considerably e.g. by noting that only a handful of verbs subcategorize for
four or five complements) gives an indication of the phenomenon we noted in
Section~5.1.1 that nominal, adjectival, and other categories have only a few
subcategories, while verbs have orders of magnitude more.

One case of particular interest to modern syntax is whether the frame can
become recursive: what happens when verbs take other verbs as their
complement?  The most frequent cases, auxiliary verbs and modals such as {\it
  have} or {\it can}, are relatively easy to handle because they require main
verbs (rather than auxiliaries) to complement them, which blocks off the
recursion in short order. There is a less frequently seen class of verbs
(sometimes called {\it control} or {\it raising} verbs)\index{control verb}\index{raising verb} that take infinitival complements such as {\it
  try} and {\it continue} that recurse freely: both {\it John tries to
  continue to run} and {\it John continues to try to run} are perfectly
grammatical (and mean different things).  Some of these verbs, such as {\it
  persuade} and {\it want}, take sentential infinitival clauses that have
subjects ({\it John persuaded/wanted Bill to run}), and the others are
restricted to infinitival VPs ({\it *John tried/continued Bill to run}). To
resolve the apparent violation of maximality (see Section~5.1.3), it is
tempting to pattern the latter after the former, assuming simply that the
subjectless cases already have their subject slot filled by the subject of the
matrix verb by some process of sharing arguments (dependents). The issue is
complicated by the fact that the behavior of such verbs is not homogeneous in
regard to the use of dummy subjects: compare {\it There continues to be an
  issue} to {\it *There tries to be an issue} -- we return to the matter in
Section~5.2.3.

Clause (v) of the case frame definition expresses the fact that verbs can
`reach down' to the subcategory of their complements (we have seen an example
in Section~5.1, {\it kill} vs. {\it murder}) and if some verbal complements
are themselves endowed with a case frame, as must be the case for infinitival
complements, it is a question for recursive constructions of this sort as to
how far the matrix verb can control the complement of a complement of a
complement.  The answer is that such control can go to arbitrary depth, as in
{\it Which books did your friends say your parents thought your neighbor
  complained were/*was too expensive?} This phenomenon, as
\newcite{Gazdar:1981} has shown, is still easily within the reach of
CFGs. There are other significant cases where this is no longer true, the most
important being {\it cross-serial dependencies}, which we will illustrate here
on Dutch using a set of verbs such as {\it see}, {\it make}, {\it help,
  \ldots} that all can, in addition to the subject NP, take an infinitival
sentence complement \cite{Huybregts:1976}.  To make the word order phenomena
clear, we restrict ourselves to subordinate clauses beginning with {\it dat}
'that' and leave it to the reader to supply a main clause such as {\it It is
  impossible~$\underline{\ \ }$}.  The verbs in question can thus participate
in constructions such as \index{Dutch}

\begin{eqnarray}
\mbox{... dat Jan de kinderen zag zwemmen}\\
\mbox{ that Jan the child.PL see.PAST swim.INF}\nonumber \\
\mbox{ that Jan saw the children swim}\nonumber\\
\mbox{ }\nonumber\\
\mbox{... dat Piet de kinderen hielp zwemmen}\nonumber \\
\mbox{ that Piet the child.PL help.PAST swim.INF}\nonumber \\
\mbox{ that Piet helped the children swim}\nonumber\\
\mbox{ }\nonumber\\
\mbox{... dat Marie de kinderen liet zwemmen}\nonumber \\
\mbox{ that Marie the child.PL make.PAST swim.INF}\nonumber \\
\mbox{ that Marie made the children swim}\nonumber
\end{eqnarray}

\noindent
Once we begin to recursively substitute these constructions in one another,
two things become evident. First, that there is no apparent limit to this
process. Second, the insertion proceeds at two different sites: both
subjects and verbs get stacked left to right. 

\begin{eqnarray}
\mbox{ }\\
\mbox{... dat Jan Piet de kinderen zag helpen zwemmen}\nonumber \\
\mbox{that Jan Piet the child.PL see.PAST help.INF swim.INF}\nonumber \\
\mbox{that Jan saw Piet help the children swim}\nonumber\\
\mbox{ }\nonumber\\
\mbox{... dat Jan Piet Marie de kinderen zag helpen laten zwemmen}\nonumber \\
\mbox{that Jan Piet Marie the child.PL see PAST help.INF make.INF swim.INF}\nonumber \\
\mbox{that Jan saw Piet help Marie make the children swim}\nonumber
\end{eqnarray}

\noindent 
The case frames of verbs like {\it see} contain a subject slot, to be filled
by a nominative NP, and an infinitival complement slot to be filled by an
S.INF, which again is subject to the rule (5.7) that rewrites it as a subject
NP and an (infinitival) VP.INF, and the latter can be filled by an
intransitive V.INF. (Mentioning the INF in these rules is redundant since the
Head feature Convention will enforce the inheritance of INF from S to VP to
V.) In the DG formalism, the innermost arrow runs from the dependent (subject)
{\it child.PL} to the head {\it swim.INF}. This arrow, corresponding to the
innermost VP, is a dependent of the matrix verb {\it make}, which has another
dependent, the subject {\it Marie}, yielding the complete infinitival sentence
{\it Marie make the children swim} by (7).  Progressing inside out, the next
matrix verb is {\it help}, with subject {\it Piet} and complement {\it Marie
  make the children swim}, yielding {\it Piet help Marie make the children
  swim}, and the process can be iterated further.  In a standard CFG,
recursive application of the rule S.INF $\rightarrow$ NP V S.INF, terminated
by an application of S.INF $\rightarrow$ V.INF, would yield the bracketing [NP
  V [NP V [NP V]]] as in {\it Piet help Marie make children swim}, which would
be entirely satisfactory for English. But Germanic word order does not
cooperate: we need to supplement the phrase structure rules either by some
verb second (V2) transformation \cite{Besten:1985}, by a {\it wrapping}
mechanism (Bach 1980, Pollard 1984),\nocite{Bach:1980}\nocite{Pollard:1984} or
by a separate linear precedence mechanism to which we turn now.

Here the fact that DG is generally silent on word order is helpful: all that 
needs to be said is stating the typologically relevant generalization (namely
that Dutch is V2 in subordinate clauses) to get the required word order. One
way to formulate generalizations about word order is to state them as
constraints on linear precedence. In the Dutch case at hand, we can simply say
$NP \prec V$ (noun phrases precede verbs) within structures dominated by an
infinitival $S$. This technique, called {\it immediate dominance/linear
precedence}, or ID/LP for short, \index{immediate dominance/linear precedence,
  IDLP}
is the one used in generalized phrase structure grammar (GPSG)
\index{generalized phrase structure grammar, GPSG} \cite{Gazdar:1985}. Another
possible technique is based on {\it nearness}, a weakening of the relation of
immediate adjacency.  If no morphological marking is available to signal the
fact that two words are strongly related, the best stand-in is to insist that
they appear next to each other, or at least as close as possible.  English
uses this device for two rather different functions. In {\it parataxis},
adjacency signals coordination, as in {\it Tom Dick and Harry}, where logical
calculi would require {\it Tom and Dick and Harry.} \index{parataxis} The
other case is signaling subjecthood: the only structural position where a
subject can appear is the one immediately before the verb.\index{English}

Modern theories of morphosyntax (Ostler 1979, Kiparsky 1987, Smith 1996),
which differ from P\={a}\d{n}ini chiefly in their ambition to also handle
analytic languages such as English, have enlarged the inventory of {\it
  linkers} from morphological devices such as case marking, prepositions, and
agreement to include positionally defined relationships as well.  {\it Direct}
theories of linking offer a straightforward mechanism to capture the basic
intuition with which we started, that cases mediate between semantic,
morphological, and syntactic generalizations. Their central device is the deep
case {\it (k\={a}raka)}, which serves to link complements to the verb both in
sentences and in nominal constructions derived from verbs. The derivation
starts by selecting a verb with the appropriate tense marking, some nominals
with the appropriate number marking, and the deep cases -- the latter will be
realized ({\it abhihita}, `spoken') by morphological and structural devices
biuniquely but heterogeneously across verbs. We have biuniqueness in any fixed
construction since every deep case that appears there will be realized by a
single linker and every linker realizes some deep case, and we have
heterogeneity in that there is no requirement for the realization to be
constant across verbs. For example, the accusative (surface) case can serve to
realize Goal or Patient (but not both in any given sentence or
nominalization), and Goal can also be realized by the instrumental case.

Such theories are `direct' because they proceed from the arguments of the
verb, defined semantically, to the linkers, which are visible on the surface,
using only one intermediary, the deep cases. In contrast, {\it indirect}
theories invoke two sets of intermediaries, deep cases and grammatical
functions (subject, object, etc.), as well. Relational grammar does not use a
separate apparatus for these two (grammatical functions and deep cases are
intermingled) but does permit derivational strata, which make the overall
theory indirect. The same is true of case grammar, which in its original form
(Fillmore 1968) was clearly intended as a direct theory but used
transformations, which brought strata with them. The case frame definition
left room for using {\it both} grammatical functions and deep cases, and so
far there is little to recommend the traditional notion of grammatical
function.  Yet there are some remarkable phenomena that point at differences
not easily explained without reference to subjects and objects. Consider first
the behavior of reflexives: {\it John shaved himself} is obviously grammatical
and {\it *Himself shaved John} is obviously not, yet in both cases the Agent
and the Patient of the shaving are both {\it John}, so the difference in
acceptability can hardly be attributed to a difference in the semantics.

To further complicate matters, there is a third set of primitives, called {\it
  thematic roles} or {\it theta roles}, \index{thematic role} \index{theta role} that are regularly invoked in classifying verbal arguments. These are
intended as fully semantical, expressing generalizations that follow from the
meaning of verbs. For example, if $V$ is an action and $NP$ refers to the
Agent of this action, then $NP$ intends $V$ to happen. Comparing {\it John
  accidentally killed the pedestrian} to {\it *John accidentally murdered the
  pedestrian} shows that under this definition of Agent (the names used for
thematic roles largely overlap the names used for deep cases), {\it kill} does
not require an Agent but {\it murder} does.  The theories of linking discussed
so far distinguish between deep and surface cases and keep both of these
distinct from both grammatical functions (subject, object, indirect object)
and thematic roles to the extent they admit them. Direct theories that 
refer to thematic roles use them purely as abbreviatory devices to distinguish 
different classes of verbs with different lexical entailments, while indirect 
theories, to which we now turn, permit combinatorical statements that refer 
to more than one set of primitives. 

\subsection{Valency}

In Section~5.1 we already considered the informal notion of {\it valency} as a
means of stating grammatical regularities. Here we consider a more formal, and
in key respects more general, mechanism for syntactic computations, the use of
algebraic structures as a means of regulating the computation. We illustrate
the way these formal systems are intended to be used by well-known linguistic
phenomena, such as the possessor-possessed relationship, but we do not intend
these illustrations to be exemplary in the linguistic sense. In many cases,
different analyses of the same phenomenon are also available, often within the
confines of the same formal systems.

In many languages, such as Latin or German, the {\it genitive}\index{genitive}
case is affixed to a noun to indicate that it is the possessor of another noun
(or NP) within the same sentence. The relationship of possession is to be
construed as a rather loose one, ranging from `being a physical part of', as
in {\it John's hand}; to `ownership', as in {\it John's book}; to `being
closely associated to', as in {\it John's boss}; and to `being loosely
associated to', as in {\it John's cop}, who can be the cop that always tickets
John at a particular intersection, the cop that John always talks about, the
cop John always calls when a fight breaks out in the bar, and so on.

Many linguists, starting with P\={a}\d{n}ini,\index{P\={a}\d{n}ini} would
argue that the genitive is not even a case, given that it expresses a relation
between two nouns rather than a noun and verb. It is also true that the same
possessor-possessed relation is expressed in many languages by suffixation on
the head of the construction, the possessed element, rather than on the
modifier (the possessor). For our purposes, the main fact of note is that the
possessor can show agreement with the possessed in head-marking languages. For
example, Hungarian {\it az \'{e}n k\"{o}nyvem}, {\it a te k\"{o}nyved}, {\it
  az \H{o} k\"{o}nyve} `my book, your book, his book'. By Definition 5.2.1,
this means that in the pure case we have some forms $u.D_i$ and $v.D_i$ that
cooccur in some context $\alpha\underline{\ \ }\beta\underline{\ \ }\gamma$,
while for $i\neq j, \alpha u.D_i \beta v.D_j \gamma \not\in L$.  (Not every
case is pure since often the paradigmatic distinction is not entirely visible
on the form, such as English {\it you}, which can be second-person singular or
plural, or Hungarian {\it az \H{o}}, which in the possessive construction can
be third-person singular or plural possessor alike.  As there are many
languages where all paradigmatic forms are distinct, we can safely ignore this
complication here.)

Since there are only a finite number of cases to consider, almost any method,
including tabulation (listing), would suffice to handle agreement phenomena
like this. For example, if the paradigm has four slots and we denote $u.D_i
(v.D_i)$ by {\it a,b,c,d} (resp. $a',b',c',d'$), the four forms $\alpha a
\beta a' \gamma, \alpha b \beta b' \gamma, \alpha c \beta c' \gamma, \alpha d
\beta d' \gamma$ are admissible, while $\alpha a \beta b' \gamma$ and the
other eleven nonagreeing forms are not. Yet linguistics rejects this method
since the intrinsic complexity of such a list would be the exact same as that
of listing the cyclic permutation $\alpha a \beta b' \gamma, \alpha b \beta c'
\gamma, \alpha c \beta d' \gamma$, $\alpha d \beta a' \gamma$, and the latter
is clearly unattested among natural languages. The preferred method is to
formulate a rough rule that generates $\alpha u \beta v \gamma$ and supplement
this by a further condition stipulating agreement between $u$ and $v$. 

One important method of stipulating such conditions is to map preterminals
onto some algebraic structure that offers some direct means of checking them.
For example, given a free group $F$ over four generators {\it a, b, c, d} and
their inverses $a'=a^{-1}, b'=b^{-1}, c'=c^{-1}, d'=d^{-1}$, if $\alpha,
\beta,$ and $\gamma$ are mapped on the unit $e$ of $F$ and
$a,\ldots,d,a',\ldots,d'$ onto themselves, the acceptable forms, the ones with
proper agreement, will be exactly the ones whose image is mapped onto $e$.
What makes cancellation in a group such an attractive model of valency is that
it is easily typed. When talking about valence informally, we always mean
finding an {\it appropriate} filler for some slot.  By assigning each slot an
independent group element and each appropriate filler its inverse, we can
guarantee both that slots will require filling and that different slots
receive different fillers. 

Given that agreement in natural languages is independent of word order, the
fact that in a group we always have $aa^{-1}=a^{-1}a=e$ is useful, and so is
the fact that we can take the group used for checking agreement to be the
direct product of simpler cyclic groups used for checking agreement in any
particular paradigmatic dimension. But in situations where the filling of
valences is order-dependent, using structures other than groups and using
decomposition methods other than direct products may make more sense. Before
turning to these, let us first consider other potential pitfalls of modeling
valences with cancellation. 

A strong argument against a strict cancellation model of valency may be a case
where a single slot (negative valence) is filled by more than one filler
(positive valence), as in coordinated constructions, such as {\it John and
  Mary won}, or conversely, cases where a single filler fills more than one
slot.  Consider, for example, the accusativus cum infinitivo (ACI)
construction in Latin, as in {\it Ad portum se aiebat ire} `He said he
(himself) was going to the harbor'. Evidently the reflexive pronoun {\it se}
`himself.ACC' acts both as the subject of {\it say} and the subject of {\it
  go.}\index{Latin}\index{accusativus cum infinitivo, ACI} On the whole, the
scope of ACI in Latin is remarkably large: almost every verb that can take an
object can also take an infinitival construction as an object.  Unlike in
English or Dutch, where {\it He wanted/helped/watched the children to swim}
are possible but {\it *He said/judged/delighted the children to swim} are not,
in Latin sentences such as {\it Thales dixit aquam esse initium rerum,
  Karthaginem delendam esse censeo}, and {\it Gaudeo te salvum advenisse} are
easily formed.  Another potential case of a single filler filling multiple
slots comes from the class of control verbs discussed in Section~5.2.2: in
{\it John tried to run}, arguably the same filler, {\it John}, acts as the
subject of {\it try} and {\it run} at the same time. Even more interesting are
cases of {\it object control} such as {\it John asked Bill to run}, where {\it
  Bill} is both the object of {\it ask} and the subject of {\it run} --
compare {\it John promised Bill to run.}

To extend valency to coordination, we need a device that will act as a
multiplexer of sorts, taking two syntactic objects with one positive valence
each and returning a single compound object with one positive valence.
Problems arise only in cases where the compound filler can appear in positions
that are closed off for elementary fillers: clearly {\it John and Mary hated
each other} is grammatical, while neither {\it *John hated each other} nor {\it
*Mary hated each other} are, and it is hard to conceive of the former as being
in any sense the coordination of the latter two.  Once a multiplexer is
available, it or its dual could be used for unifying slots as well, and
expressing general statements like `in object control verbs, the object of the
matrix verb is shared with the subject of the embedded verb' would be easy.

Yet, just as with coordination, there remain some strong doubts whether using
argument sharing as a conceptual model for apparently shared fillers is truly
appropriate. For example, {\it I hate to smoke} means that I dislike cigarette
smoke, while {\it I hate that I smoke} means that I actually like cigarette
smoke (but dislike my own weakness of not being able to give it up).  The
standard method of formalizing argument sharing is by means of shared
variables: if we treat transitive verbs as two-place predicates, {\tt
  try(Someone, Something), hate(Someone, Something)}, and intransitives as
one-place predicates, {\tt run(Someone)}, or as two-place predicates with
implicit defaults, {\tt smoke(Someone,Cigarettes)}, we can write {\it John
  tried to run} as {\tt x=John, try(x,run(x))} and {\it I hate to smoke} as
{\tt x=I, hate(x}, {\tt smoke(x,Cigarettes))}, but this representation is
clearly more appropriate for {\it I hate it that I smoke}.

How, then, is {\it I hate to smoke} to be represented? One possibility is to
treat `hating-of-smoking' as a compound verb that has only one valence and
relegate the process of forming such verbal complexes to the lexicon, where
other valency-changing processes, such as forming passives or causatives, are
also known to operate. In syntax, then, we can retain a pure form of valence
theory that assigns a valence of $+1$ to every NP, including coordinated
forms, and a valence of $-n$ to $n$-place predicates.  However, the situation
is further complicated by free adverbials and other adjunct NPs that do not 
fill any slot associated to the verbal arguments -- this is what makes typing 
the slots and fillers essential.\index{adjunct}

In the commutative case, we can restate matters using additive rather than
multiplicative inverses and direct sums rather than direct products. We will
say that a predicate has a {\bf valence} vector $(x_1,\ldots,x_k)$, where the
$x_i$ are $0$ or $-1$ and the basis dimensions correspond to grammatical
\index{valence|textbf} functions or deep cases -- for the sake of
concreteness, we will use Agent, Goal, Recipient, Instrument, Locative, and
Source in this order. The valence vector of {\it rent} will thus be
$(-1,-1,0,-1,-1,-1)$, and for our example (5.11), {\it John rented the room
  from a slumlord for a barbershop for the first three months for fifteen
  hundred dollars}, we have five NPs that can fill the slots: {\it John} as
the Agent, with valence vector $(1,0,0,0,0,0)$; {\it the room} as the Goal,
with valence vector $(0,1,0,0,0,0)$; {\it a slumlord} as the Source, with
valence vector $(0,0,0,0,0,1)$; {\it the first three months} as the Location,
with valence vector $(0,0,0,0,1,0)$; and {\it fifteen hundred dollars} as the
Instrument, with valence vector $(0,0,0,1,0,0)$. The purpose clause, {\it for
  a barbershop}, is floating freely (the Goal is the immediate result of the
renting action in view, namely {\it the room}) and thus has valence vector
$(0,0,0,0,0,0)$.  As long as the valence vectors sum to 0, we accept the
sentence, seemingly irrespective of phrase order; the reason why {\it *John
  the room rented} is unacceptable is that in English only the NP in the
immediate preverbal position can be the Agent and only the NP in immediate
postverbal position can be the Goal. In languages where deep cases are
morphologically marked (by surface cases), the expectation is that the clauses
can be freely permuted as long as their morphological marking is preserved.


%uta1lom pe1tert elmenni la1tni, *uta1lom pe1tert elmenni, uta1lom pe1tert
%control: John tries to kill Bill, Bill tries to be killed by John. 
%dative subjects in German vs. Icelandic (van valin 2005)

\section{Semantics-driven theories}

In the most extreme form of semantics-driven theories of syntax, the relation
that connects the form and the meaning of sentences is one of causation: a
sentence has a given form {\it because} this is the best way to express some
idea or state of affairs. Given the bewildering variety of ways different
languages can express the same thought, this appears a hopelessly naive
approach, yet a great deal of the methods and motivation are shared between
the semantics-driven and the more `pure' theories of syntax discussed so far.
To solve the problem that different languages use different constructions to
express the same idea, all that is needed is a parametric theory of syntax and
a slight relaxation of direct causality. Instead of saying that a sentence has
a given form because this is the best way to express an idea, we now say that
it has this form because this is the best {\it given} the language-specific
setting of the parameters.

To give an example, consider transitive sentences of the (5.8) type.
Semantics-driven theories maintain that the structure of thought is universal:
there is an act of killing, with {\it the farmer} as the agent and {\it the
  duckling} as the patient. Following Greenberg (1963),\nocite{Greenberg:1963}
we divide languages in six types corresponding into the six possible
permutations of Subject, Object, and Verb.  English is an SOV language, so the
sentence comes out as above. In Japanese, an SOV language, we get {\it
  Hyakusyoo wa ahiru o korosita `As for farmer, duck killed'}, while in Irish,
a VSO language, we get {\it Mhairiagh an feirmeoir an lacha}, and so forth.
Given that word order in different languages can take any value (according to
Hawkins (1983), 45\% of languages are SOV, 42\% SVO, 9\% VSO, 3\% VOS, 0.9\%
OVS, and 0.1\% are OSV), proponents of the Sapir-Whorf hypothesis must explain
why the majority of languages do not follow the natural order (whichever
that may be).  \index{Japanese} \index{Irish} \nocite{Hawkins:1983}
\index{Sapir-Whorf hypothesis}

The research program of eliminating nonparametric variation from syntax is
common to many theories, including some of the most pure combinatorical
theories, such as classical transformational grammar and modern minimalist
syntax. The standard form (see Chomsky and Lasnik 1993)\nocite{Chomsky:1993}
is often called {\it principles and parameters}\index{principles and
  parameters} theory, assuming a common core of grammatical principles that
operate in all languages, and a finite set of binary parameters that govern
them (so that altogether there are only finitely many core systems).  This is
an extremely attractive model, but one that faces many serious technical
difficulties that come to light as soon as we attempt to flesh out the
typological system.\index{split systems} First of all, there are many
languages that are {\it split} (display a mixture of the pure types). For
instance, German, while widely regarded as an SOV or V2 (verb second)
language, displays a pure SVO construction and has several different word
orders both in main and in subordinate clauses. Traditional descriptions of
German word order, such as Drach (1937),\nocite{Drach:1937} therefore employ a
linear structure composed of separate structural positions or {\it topological
  fields}, a notion that has little typological generality. Even if we could
separate out the parochial from the universal factors in such cases, there is
a larger problem: the terms that we use in establishing our typology may have
no traction over the actual variety of
languages.\nocite{Schachter:1976}\index{German}

The marked word orders VOS, OVS, and OSV account for less than 5\% of the
world's languages. However, about one language in ten has no identifiable
subject category at all according to Schachter's (1976) criteria: a prime
example is Acehnese [ACE] (see Durie 1987).\nocite{Durie:1987}\index{Acehnese [ACE]} For many languages that fall outside the Greenberg system entirely, a
different typological distinction between {\it ergative} and {\it accusative}
languages is invoked. As the basic pattern is very strange for those whose
only exposure has been to the 90\% of languages that are accusative, we give
an example here from Warlpiri [WBP] \cite{Hale:1983}. \index{Warlpiri [WBP]}
The sentences generally considered simplest are the intransitives. These
involve just a single argument:

\begin{eqnarray}
\mbox{The baby cried}\\
\mbox{NP.NOM  V.PAST}\nonumber
\end{eqnarray}

\noindent and in the familiar {\it accusative} languages that have overt case
marking, this argument, the subject, appears with the nominative case.
\index{accusative} Transitives, the next simplest case, require two arguments.
We repeat example (5.8) here with the relevant pseudogloss:

\begin{eqnarray}
\mbox{The farmer killed the duckling}\\
\mbox{NP.NOM     V.PAST    NP.ACC}\nonumber
\end{eqnarray}

\noindent
That the glosses in (5.14) and (5.15) are not entirely fictitious even for
English is clear from the use of oblique pronouns {\it him/her} in the object
position as opposed to the nominative {\it he/she} in the subject position.
The Warlpiri pattern is rather different:

\begin{eqnarray}
\mbox{Kurdu ka wangka-mi}\\
\mbox{NP.ABS AUX speak.NONPAST}\nonumber\\
\mbox{The child is crying}\nonumber\\
\nonumber\\
\mbox{Ngarrka-nguku ka wawirri panti-rni}\\
\mbox{man.ERG AUX kangaroo.ABS spear.NONPAST}\nonumber\\
\mbox{The man is spearing the kangaroo}\nonumber
\end{eqnarray}

\noindent 
What is striking about this pattern (which is the basic pattern of languages
called {\it ergative} in typology) is that the default case marking (called
{\it absolutive} and realized by zero morphology in many ergative languages,
just as nominative is realized by zero in many accusative languages) appears
with what from the accusative perspective we would consider the object of the
transitive construction, and it is the subject that receives the overt case
marking.  As this example shows, it is far from trivial to put to use
grammatical primitives such as {\it subject} or {\it object} even for their
stated purpose of grammatical description -- ergative languages group together
the subjects of intransitives with the objects of transitives.

\smallskip
\noindent
{\bf Example 5.3.1} The basic accusative pattern. In defining the accusative
and ergative patterns, we put to use the algebraic apparatus we started to
develop in Section~5.1.1.  For both cases, we will have nominal elements in
the category $n$ {\it (John, the baby)}, purely intransitive verbs {\it
  (sleeps, walks)}, purely transitive verbs {\it (loves/kills)}, and verbs
that have both kinds of subcategorization {\it (eats, sees)}, which we place
in categories $i$, $t$, and $d$, respectively. To simplify matters, we shall
ignore person/number agreement but obviously not case marking. For the
relevant case suffixes, we will use $N$ (Nominative) $A$ (Accusative), $B$
(aBsolutive), and $E$ (Ergative), and since these always attach to nominal
elements we will treat them as subcategories of $n$, writing simply $A$
instead of the morphemic gloss {\it n.A}. The {\bf accusative} pattern is
given by a finite language $X$ that will have the strings corresponding to
intransitive and transitive sentences, with the appropriate case markings {\it
  Ni, NtA, Nd, NdA} and all their permutations (free word order).

The syntactic monoid $C_X$ has two distinguished elements: the empty string
(whose equivalence class we denote by $I$ since it is the multiplicative
identity) and a large grab-bag equivalence class $U$ for all strings with more
than one verbal or more than two nominal elements. Members of this class are
all cases of {\it unrecoverable ungrammaticality}
\index{ungrammaticality!unrecoverable} in the sense that no further element
can be added to create a grammatical sentence. Other strings, such as $NA$,
are also ungrammatical, but in a recoverable way: adding $d$ or $t$ to the
string will create a grammatical sentence, and in fact it simplifies matters
to arrange all equivalence classes in layers running from the innermost $(I)$
to the outermost $(U)$ in accordance with how many further elements it takes
to make them grammatical. In the innermost layer, we find two classes,
represented by $t$ and $A$, respectively (these require two further elements
to complete a sentence), in the next layer we find five classes, represented
by $Nt, i, NA, N$, and $d$, respectively (these require only one further
element), and in the outermost layer we find two classes represented by $Ni$
and $Nd$, respectively (both require zero additional elements, but they are not
in the same class since for one $A$ can still be added). Altogether, the four
basic grammatical strings in the accusative pattern (or five, if we declare
the empty string grammatical) and their permutations yield a total of eleven
syntactic congruence classes i.e. a syntactic monoid with eleven elements.

\smallskip
\noindent
{\bf Discussion} The progression from inner to outer layers is a way of
recapitulating the idea of valency \index{valence} discussed in Section~5.2.1, and
serves to show an essential property of the system, namely that no simple
sentence ever has a (semigroup-theoretic) inverse: once valencies are filled
in, there is no return. Complex sentences (with more than one verb) may still
show periodic behavior (by filling in a verbal dependent we may end up with
more open valences than when we started out), but the syntactic semigroup built
on simple sentences will always be aperiodic.  Since this result is clearly
independent of the simplifying assumption of free word order made above, we
state it as a separate postulate.

\medskip
\noindent
{\bf Postulate 5.3.1} In any natural language, the syntactic monoid associated
to the language of simple sentences is aperiodic. 

\smallskip
\noindent
In Chapter~2, we already emphasized the importance of Chomsky's (1965)
observation that natural languages are noncounting in the informal sense that
they do not rely on arithmetic notions. Postulate 5.3.1 provides a more formal
statement to the same effect, and in Section~5.5 we shall offer a formal
definition of {\it noncounting} that is, perhaps, easier to falsify
empirically than Postulate 5.3.1, but as Yli-Jyr\"{a}
(2003)\nocite{Yli-Jyra2:2003} notes, even a large-scale wide-coverage grammar
of English \cite{Voutilainen:1994} can be modified to avoid the Kleene $^*$
operator altogether.  Note, however, that the statement is clearly relevant
only for syntax since phonology evidently shows periodic patterns, e.g.  in
the placement of stress (typically binary, sometimes ternary), as discussed in
Section~4.1.  This runs counter to the generally unspoken but nevertheless
very real prejudice of early generative grammar that phonology is easier than
syntax.

Returning to the ergative pattern for a moment, the language $Y$ will have
{\it Bi, EtB, Bd, EdB}, and their permutations. It is a trivial exercise to
verify that the monoid $C_Y$ is isomorphic to $C_X$. By mapping $N$ to $B$ and
$A$ to $E$, the accusative set of patterns is mapped on the ergative set in a
concatenation-preserving manner. However, this is an isomorphism that fails to
preserve meaning: {\it The man is spearing the kangaroo} would become {\it The
  kangaroo is spearing the man}, and it is easy to see that in fact no
meaning-preserving isomorphism exists between the two patterns. We return to
this matter in Chapter~6, where we investigate not just the stringsets but
rather more complex structures that contain both the words and their meanings.

The study of the ergative pattern makes it abundantly clear that the overall
plan of using a few examples and counterexamples to help navigate the
typological decision tree is very hard to put into practice.  Compared with
stress typology (see Section~4.1.4), the situation is rather dismal. The
empirical correlates to the primitive notions are unclear, and the system,
however organized, is riddled with split cases. For example Dyirbal [DBL] has
an accusative system in the first and second persons, ergative in the third
person \cite{Schmidt:1985}, and many Indo-Iranian languages are ergative in
perfect and accusative in imperfect aspects and so on.\index{Dyirbal [DBL]}

This gives rise to the suspicion, shared by many in the field of artificial
intelligence (AI), that mainstream linguistics goes about the whole matter the
wrong way, invoking too many hard to define and hard to apply grammatical
intermediaries in what should be a straightforward mapping from thought to
expression. The goal should be simply to specify the {\it language of thought}
and explain syntactic complexities, to the extent we care about them, by means
of parochial imperfections in natural languages.  This line of thought goes
back at least to the great rationalist thinkers of the 17th century, in
particular Descartes, Pascal, and Leibniz.\index{language of thought}
Early AI research such as Quillian's (1969) Teachable Language Comprehender
\nocite{Quillian:1969} and Anderson and Bower's (1973) Human Associative
Memory and the subsequent ACT-R \nocite{Anderson:1973} took quite seriously
the task of accounting for human long-term memory, including performance
effects such as reaction times.  

In modern research, the mental aspects of the language of thought are
generally downplayed, with many researchers being downright hostile to the
idea of looking for brain activity correlates to formulaic descriptions of
thoughts or ideas. Rather, to eliminate any reference to the mental aspects,
emphasis is placed on what the thoughts or ideas are about, in particular on
real-world objects (entities) and assertions about them.
\index{artificial intelligence, AI}

In this view, the task of {\it knowledge representation} (KR)
\index{knowledge representation, KR} begins with hierarchically organizing the entities that
are typically expressed by common nouns.  The central organizational method is
Aristotelian, with increasingly special species subsumed under increasingly
general genera. Economy is achieved by a system of {\it default inheritance}
whereby lower nodes on the hierarchy inherit the properties of the higher
nodes unless specified otherwise. Thus, we need not list the property of
breathing with Baby Shamu since she is a killer whale, killer whales are
dolphins, dolphins are mammals, mammals are animals, and animals breathe.
However, the inference that fishes breathe has to be blocked by explicitly
stating that fishes do {\it not} breathe air, that indeed breathing water
through gills is part of their definition. (Going further, the default fish
rule needs to be suspended for lungfish and other fish species that can in
fact breathe air.) The creation of systems of logic that can sustain
nonmonotonic inferences (overriding default conclusions in special cases) is
an important thread in modern AI and philosophical logic (see Ginsberg 1986a,
Antonelli 1999). \nocite{Antonelli:1999}

Linguists have long used this technique to hierarchically classify lexical
entries, in particular verbs. At the top of the hierarchy, we find the {\bf
  telic/atelic} distinction that roughly corresponds to the count/mass
distinction among nouns. Telic verb phrases denote events that have a definite
endpoint, while atelic VPs lack this; compare {\it John cleaned the dishes for
  an hour} to {\it *John recognized Bill for an hour}.  Next, atelic events
are divided into {\it states} and {\it activities}, telic events into {\it
  achievements} and {\it accomplishments}; see \newcite{Vendler:1967},
\newcite{Dowty:1979}, and \newcite{Verkuyl:1993}.  \index{aktionsart}
\index{telic} \index{atelic} The situation is greatly complicated by the
existence of other largely orthogonal classification schemes such as {\it
  stage-level} and {\it individual-level} predicates\nocite{Kratzer:1995} (for
a modern summary, see Kratzer 1995) or other semantically motivated groups
such as {\it verbs of motion} or {\it psych verbs}.  \newcite{Levin:1993}
offers a rich selection of grouping criteria but refrains from hierarchically
organizing the data and thus avoids the need to specify exactly what aspects
of the competing cross-classification schemes inherit and to what extent the
different flavors can be mixed.

Just as common nouns correspond to entities, adjectives correspond to
properties.  Unlike entities, which are taken to be an atomic type $E$,
properties are taken as functions from entities to truth values
$T$.\index{truth value} Under this analysis, the meaning of an adjective $a$
is simply the set of entities $N$ such that $a(n)$ is true, and $n$ enjoys
property $a$ just in case $n \in N$.  A key aspect of this model is the
division of properties that an entity has into {\it accidental} and {\it
  essential} properties. Continuing with the example above, not only do fishes
breathe water but this is essential to their being fishes. The fact that they
generally have iridescent scales is accidental: we can imagine fishes without
this property, and indeed we find many, such as sharks or eels, that have no
such scales.  What is accidental in one kind of entity may be essential in
another, and to keep track of which is which \newcite{Minsky:1975} introduced
the notion of {\it frames.} These are not to be confused with the case frames
introduced in Section~5.2 above -- KR frames apply to nouns, case frames to
verbs.

For example, the {\it dog} frame will contain slots for {\it name} and {\it
owner}, which are essential to the definition of dogs as cultural constructs
in modern urban life, even though they would not be relevant for a biological
definition.  That it is indeed the cultural, rather than the biological,
definition of dogs that is relevant to concerns of natural language
understanding can be seen from the following examples: {\it Rover has
diarrhea. The owner has a hard time complying with cleanup regulations/*The
coenzyme Q10 overdose is evident.} Once {\it Rover} has been introduced to the
discourse, referring to {\it the owner} proceeds smoothly because the former
is a definite (singular) entity and as such implies a definite owner. For
people outside the medical and veterinary professions, the relationship between
dogs, diarrhea, and coenzyme Q10 is anything but evident, and introducing {\it
coenzyme Q10 overdose} with the definite article {\it the} is infelicitous. 

For another example, consider the dictionary definition of {\it cup} as {\it a
  usually open bowl-shaped drinking vessel with or without a handle}. What
does this really mean? Clearly, every individual object is with or without a
handle, and it is just as true of the genus {\it fish} that it comes with or
without a handle as it is of the genus {\it cup}. Yet there is something right
about the definition: {\it I found a cup/*fish but the handle was broken}
shows the same effortless move from {\it cup} (but not from {\it fish}) to
{\it handle} that we had from {\it dog} to {\it owner}. So the {\it cup} frame
must contain a slot for {\it handle} meaning that having a handle, or not, is
an essential property of cups.

Frames extend more or less naturally to events. To avoid confusion with case
frames, we call the frame representation of event objects {\it scripts}, as
has been standard in KR since the work of \newcite{Schank:1977}. The original
intention was to use scripts as repositories of commonsense procedural
knowledge: what to do in a restaurant, what happens during a marriage
ceremony, etc. Scripts have actors fulfilling specific roles, e.g. that of the
waiter or the best man, and decompose the prototypical action in a series of
more elementary sub-scripts such as `presenting the menu' or `giving the bride
away'. There are some linguistically better motivated models, in particular
{\it discourse representation theory}, that rely on lexically stored
commonsense knowledge, but their scope is more modest, being concerned
primarily with the introduction of new entities {\it (the owner, the best
  man)} in the discourse.  Also, there are more systematic studies of {\sl
  ritual}, in particular in the Indian tradition (Staal
1982,1989),\nocite{Staal:1982,Staal:1989} but their cross-fertilization with
the Western KR tradition has been minimal so far.
\index{discourse representation theory, DRT}\index{ritual}

The AI program, then, offers specific solutions to many issues surrounding the
representation of nouns, adjectives, and verbs, with both active and stative
verbs freely used as elementary subunits in scripts. In fact, these subunits
are not viewed as atomic: {\it conceptual dependency} \cite{Schank:1972} is a
representational theory that extends ideas familiar from dependency grammar
down to the level of mental language. To apply the MIL formalism of
Section~5.2 to Conceptual Dependency, we take the primitive objects to be the
atomic symbols PP, PA, and AA (which in CD correspond roughly to nouns,
adjectives, and adverbs), ATRANS, PTRANS, MTRANS, GRASP, PROPEL, MOVE, INGEST,
EXPEL, ATTEND, SPEAK, MBUILD, and DO (which correspond roughly to verbs), and
choose the $P_i$ as CAUSE, BI-CAUSE, MOBJECT, INSTRUMENT, ENABLES, RESULTS,
INITIATES, and REASON (which correspond roughly to cases).\index{conceptual dependency, CD|textbf}

\smallskip
\noindent
{\bf Exercise 5.2} Consider all the English example sentences in 
Chapter~6 and write the equivalent graphs and formulas.  

\smallskip \noindent This skeletal picture would need to be supplemented by a
host of additional axioms to recapitulate the exact combinatorical
possibilities of CD, e.g. the notion that objects need to be AT-LOC before
they can PTRANS out of it \cite{Schank:1973}. We will not pursue CD in this
detail because the representations preferred in linguistics tend to use a
slightly different set of primitives; e.g. for {\it John gave the book to
  Bill} we could have DO(John,CAUSE(HAVE(Bill,book))) as the underlying
semantic structure \cite{Jackendoff:1972}. The explicit use of implicit
illocutionary primitives such as DO in this example is particularly
characteristic of the {\it generative semantics} school \cite{Harris:1995}.
\index{generative semantics}

An important version of the same broad program has been pursued by Wierzbicka
(1972, 1980, 1985, 1992) and the NSM school \cite{Goddard:2002}. The set of
primitives is broader, including pronouns {\it I}, {\it YOU}, {\it SOMEONE},
{\it SOMETHING/ THING;} determiners {\it THIS}, THE SAME, OTHER; quantifiers
ONE, TWO, SOME, ALL, MANY/MUCH; adjectives {\it GOOD}, BAD, BIG, SMALL, TRUE;
adadjectives VERY, MORE; verbs {\it THINK}, {\it KNOW}, {\it WANT}, {\it
  FEEL}, SEE, DO, HEAR, {\it SAY}, HAPPEN, MOVE, TOUCH, LIVE, DIE; adverbials
{\it WHERE/PLACE}, HERE, ABOVE, BELOW, FAR, NEAR, SIDE, INSIDE, TOUCHING,
WHEN/ TIME, NOW, BEFORE, AFTER, A LONG TIME, A SHORT TIME, FOR SOME TIME;
connectives {\it NOT}, MAYBE, CAN, BECAUSE, IF; relations KIND OF, PART OF,
THERE IS/EXIST, HAVE, LIKE; and a handful of nouns: BODY, WORDS, MOMENT,
PEOPLE (elements in italics were included in the early work, the rest were
added since the 1990s). Unlike many of the early AI researchers whose work
aimed at immediate algorithmization, Wierzbicka and the NSM school, with a
commendable lack of pretense, eschew formalization and operate entirely with
natural language paraphrases such as the following definition (Wierzbicka
1992:36) of {\it
  soul}:\nocite{Wierzbicka:1972}\nocite{Wierzbicka:1980}\nocite{Wierzbicka:1985}\nocite{Wierzbicka:1992}

\begin{quote}
one of two parts of a person\\
one cannot see it\\
it is part of another world\\
good beings are part of that world\\
things are not part of that world\\
because of this part a person can be a good person\\
\end{quote}

\noindent 
To recapitulate this analysis in a more formal framework, we would need to
introduce two worlds, one with visible things and one without, a conceptual
model of persons with parts in both worlds, goodness and visibility (or the
lack thereof) as essential properties of the subclass of persons and the
superclass of beings, and so forth. This kind of conceptual modeling of the
{\it folk theory} or {\it naive theory} behind conceptual entities fits very
well in the style of logical analysis undertaken in AI \cite{Hayes:1978},
except perhaps for the issue of uniqueness: in AI it is commonly assumed that
there will be a unique correct solution reflecting the fundamental nature of
reality, while the NSM school assumes that such definitions may show
considerable variation across languages and cultures. 

An even larger set of 2851 primitives, the Longman Defining Vocabulary (LDV),
is used throughout the {\it Longman Dictionary of Contemporary English}
(LDOCE). For the reductionist, the NSM list already offers some tempting
targets: do we really need LIVE and DIE as primitives given the availability
of NOT?  Do we need TOUCH and TOUCHING? Because the LDV list contains a large
number of cultural constructs, often trivially definable in terms of one
another (e.g.  Monday, Tuesday, ..., Sunday are all listed), it is clearly
not a candidate list for the primitive entities in a presumably genetically
transmitted universal internal language of thought. Yet LDOCE performs the
bulk of the work we expect from a reductionist program: it covers over a
hundred thousand word and phrase senses, and clearly every word of English
ever encountered is definable in terms of those defined there. Thus, the more
radical reductionist programs such as CD or NSM only need to cover the 2851
LDV entries; the rest of English will follow from these (assuming of course
that the reductions will not implicitly rely on background knowledge).

\smallskip
\noindent
{\bf Exercise 5.3} Pick ten words randomly from LDV and define them in 
terms of the CD or NSM primitives. 

\smallskip \noindent Given the large number of elusive but nevertheless
attractive generalizations that link semantical notions such as volition or
telicity to syntactic notions such as case or subjecthood, one may wonder why
most linguists consider the task of explaining syntax from semantics hopeless.
One particularly strong reason is that natural language has an immense number
of {\it constructions} \index{construction} that are fixed both in form and
meaning, but the relationship between the two is arbitrary. We already
discussed in Chapter~3 the arbitrariness of signs: the meaning of words is not
predictable from the sounds of which they are composed.  As it turns out, the
meaning of syntactic constructions is also not predictable from the words of
which they are composed.  Consider arithmetic proportions such as
$3:5=6:10$. In English, we say {\it 3 is to 5 as 6 is to 10}, and the
construction extends well beyond arithmetic. We can say {\it London is to
  England as Berlin is to Germany}. Notice that the key predicate {\it is to}
does not exist in isolation; the only way a phrase like {\it Joe is to Bill}
can appear is as part of the {\it X is to Y as Z is to W} construction. There
appears to be no rational calculus whereby the overall meaning $X:Y=Z:W$ could
be derived from the meaning of the function words {\it as}, {\it is}, {\it
  to}, especially as there must be other factors that decide whether we solve
{\it London is to England as Z is to the US} by {\it New York} (analogy based
on the biggest city) or {\it Washington} (analogy based on capital). The
number of such constructions is so large and their range is so varied (see the
entry {\it snowclones} in {\tt http://www.languagelog.org}) that some theories
of grammar, in particular {\it construction grammar} (Fillmore and Kay 1997,
Kay 2002),\nocite{Fillmore:1997}\nocite{Kay:2002} take these as the
fundamental building blocks of syntax, relegating more abstract rules such as
(5.7) to the status of curiosities.  \index{construction grammar}

Faced with the immense variety of constructions, most linguists today
subscribe to the {\it autonomy of syntax} thesis that both the combinatorical
properties of words and the grammatical descriptions of sentences are
independent of their meaning. In this view, it is necessary to make the
relationship between form and meaning the subject of a separate field of
inquiry. This field, what most linguists would call semantics proper, will be
discussed in Chapter~6. Here we are concerned with the issue of developing a
formal theory of semantics-driven syntax, leaving it to the future to decide
how far such a program can actually get us. As is clear from the foregoing,
the formal theory begins with a set of conceptual primitives such as LIVE and
MOVE, which are used in three settings: first, there are some {\it axioms}
connecting these to each other, e.g. that good things are not the same as bad
things; second, there are some dictionary definitions or {\it meaning
postulates} that connect the vast majority of lexical items to those few we
consider primitive; and third, some kind of {\it inner syntax} that regulates
how conceptual representations, both primitive and derived, combine with one
another. 

In most versions of the theory, in particular in the AI/KR approach, the
primitives come equipped with a frame not so different from the grammarian's
case frame: primitives are modeled as functions with a definite signature,
both in terms of having a fixed arity (number of arguments) and in terms of
having type restrictions imposed both on their input arguments and on their
output. The inner syntax does little more than type-checking: well-formed
conceptual representations correspond to well-typed programs evaluating to a
few distinguished types such as T (truth value) for sentences and E (entity)
for noun phrases. To obtain predictions about the combinatorical possibilities
of words, it is necessary to couple this inner syntax to some statements about
word order ( e.g. whether an adjective precedes or follows the noun it
modifies) and to morphology (e.g. whether a property is signaled by
word-internal processes or by adding a separate word). The coupling is
language-specific, while the inner syntax is assumed to be universal.

While the AI/KR approach is using function arguments for the slot/filler
mechanism, the NSM work is more suggestive of categorial grammar in this
regard. For example, a primitive such as VERY could be treated as a function
that takes an adjective and returns an adjective or as an adadjective that
combines with a following adjective to yield another adjective. These two
approaches are not incompatible: in full generality, the former corresponds to
lambda calculus and the latter to combinatory logic. However, both of these
are Turing-equivalent, while there is no reason to suppose that syntax, either
alone or in combination with semantics, goes beyond the context-sensitive
(linear bounded) domain.

\smallskip
\noindent
{\bf Exercise 5.4} 
Write a context-sensitive grammar describing the set of first-order formulas
where no quantifier is dangling (every quantifier binds at least one
variable). 

\smallskip
\noindent
{\bf Exercise 5.5$^*$} Write an indexed grammar describing the same set.

\smallskip 
\noindent 
In terms of the Chomsky hierarchy, context-sensitive grammars provide an upper
bound on the complexity of natural languages, and many take examples like
(5.13) as indicative of a lower bound, namely that natural languages cannot be
properly described by context-free grammars.  Finding {\it mildly}
context-sensitive grammars that cover all such examples without sacrificing
polynomial parsability has been a focal point of research in mathematical
linguistics since the 1980s, with special attention on linear and partially
linear versions of indexed grammars -- for a good summary, see
\newcite{Kracht:2003}.  The larger issue of whether structured sets of
examples such as (5.13) can actually provide lower bounds remains
unsettled. The first such set of examples, used in \newcite{Chomsky:1957} to
demonstrate that English is not finite state (regular), was attacked
almost immediately \cite{Yngve:1961} on the grounds that as the length of such
examples increases so does the uncertainty about their grammaticality. We
return to this issue in Section~5.4 from the perspective of weighted
languages.

\section{Weighted theories}

Both grammatical and semantics-driven theories rely on devices, such as case,
grammatical function, thematic role, or frame slot, that are in many
constructions obscured from the view of the language learner and must be
inferred by indirect means. To the extent that combinatorical theories rely on
deletion (e.g. the use of {\it traces} in modern transformational theory),
they are open to the same charge of multiplying entities beyond necessity.
\index{trace} Since the proper choice of such partially observable devices is
anything but clear, it makes a great deal of sense to attempt to bring as much
observable evidence to bear as possible. One important range of facts that is
becoming increasingly accessible with the advance of computers and the growth
of on-line material is the frequency of various word strings: though equally
grammatical, {\it Hippopotami are graceful} has one Google hit, while {\it
  What's for lunch?} has over 500,000. The discrepancy is large enough to lend
some plausibility to the assumption that a child learning English will likely
encounter the second, but not the first, early on.  Given the paucity of {\it
  NP's for lunch}, a theory of language acquisition that relies on direct
memorization (example-based learning) for the contracted copular {\it 's} in
questions that have the {\it wh}-element in situ is far more credible than one
that would crucially rely on the availability of examples like {\it ?Fruit's
  for lunch} to the language learner.

To the extent that syntactic theories are incapable of accommodating such
facts, the combinatorical statement of the problem as a membership problem is
incomplete.  We address this defect by introducing {\bf weighted languages},
defined as mappings $f$ from strings $w \in \Sigma^*$ to values in a semiring
$R$. $f(w)$ is called the {\bf weight} of
$w$.\index{language!weighted|textbf}\index{weight|textbf} Our primary example
of $R$ will be the set of nonnegative reals ${\Bbb R}^+$ endowed with the
usual operations, but the overall framework carries as special cases both
standard formal language theory (with $R$ taken as ${\Bbb B}$, the Boolean
semiring with two elements) and theories that rely on degrees of grammaticality
\cite{Chomsky:1967}, as well as theories that use weights in ${\Bbb N}$ to
count the number of ways a string can be derived.\nocite{Chomsky:1963} When
the weights taken over the set of all strings sum to 1, we will talk of {\bf
  probabilistic languages} and write $P$ instead of $f$.
\index{language!probabilistic|textbf}

By {\it language modeling} we mean the development of models with the goal of
approximating the pattern of frequencies observed in natural language. This
includes not just probabilistic theories but all the theories discussed so
far, as these can be interpreted as offering a crude 0-1 approximation of the
actually observable pattern. To be sure, most theories of syntax were not
designed with this kind of explanatory burden in mind, and many grammarians
actually disdain frequency counts, be they from Google or from better
organized corpora. But the overall question of how successfully one (weighted)
language {\it approximates} another remains valid for the standard
(unweighted) case, and the mathematical theory of {\it density} developed in
Section~5.4.1 covers both. As we shall see, the central case is when density
equals zero, and in Section~5.4.2 we describe some finer measures of
approximation that are useful within the zero density domain. In Section~5.4.3,
we introduce the general notion of weighted rules and discuss their
interpretation in sociolinguistics as {\it variable rules}. In Section~5.5, we
turn to weighted regular languages and discuss the main devices for generating
them, weighted finite state automata/transducers and hidden Markov models.
The larger issues of bringing external data to bear on syntax, be it from
paraphrase or translation, from language acquisition and learnability, from
the study of dialects and historical phases of the language, or from
performance considerations such as parsing speed or memory issues, will all
have to be rethought in the probabilistic setting, a matter we shall turn to
in Section~5.6.

\subsection{Approximation}

Here we first investigate what it means in general for one (weighted) language
to approximate another. Given $f: \Sigma^* \rightarrow {\Bbb R}^+$ and a
threshold $\varepsilon \geq 0$, the {\it niveau sets} $f_{\varepsilon}^+ = \{w
\in \Sigma^* | f(w) > \varepsilon\}, f_{\varepsilon}^- = \{w \in \Sigma^* |
f(w) < \varepsilon\}, f_{\varepsilon}^0 = \{w \in \Sigma^* | f(w) =
\varepsilon\}$ are ordinary (nonweighted) formal languages over $\Sigma$.  In
engineering applications, it is convenient to restrict the mapping to strictly
positive values. Even though a naive frequentist would assign zero to any
string $w$ that has not been observed in some very large sample, it is hard to
entirely guarantee that $w$ will never be seen (e.g. as a result of a typo),
and it is best not to let the model be driven to a singularity by such random
noise. As we shall see in Chapter~9, a great deal of engineering effort is
spent on deriving reasonable nonzero estimates for zero observed frequency
cases. Such estimates, depending on the status of $w$, can differ from one
another by many orders of magnitude. For example, Saul and Pereira
(1997)\nocite{Saul:1997} estimate the ratio of the probabilities P(colorless
green ideas sleep furiously)/P(furiously sleep ideas green colorless) to be
about $2 \cdot 10^5$. This suggests that with a good probabilistic model of
English we may have a broad range from which to choose a threshold
$\varepsilon$ such that $f_\varepsilon^+$ ($f_{\varepsilon}^-$) approximates
the set of grammatical (ungrammatical) strings.

But if $P$ is a probability measure, then $P_{\varepsilon}^+$ will be finite
for any $ \varepsilon$. Since all niveau sets will be either finite or
cofinite, all niveau sets are regular, rendering our primary navigation aid,
the Chomsky hierarchy, rather useless for probabilistic languages. This is not
to say that there is no way to generalize finite automata, CFGs, TAGs, or even
Turing machines to the probabilistic case (to the contrary, such
generalizations are readily available) but rather to say that studying their
niveau sets, which is generally the focal point of the analysis of
probabilistic systems, suggests that the regular case will be the only one
that matters.  Since the set of grammatical strings could in principle have
nonregular characteristics, while the niveau sets we use for approximation
are of necessity regular, our driving example will be the approximation of the
Dyck language $D_1$ over a two-letter alphabet. As $D_1$ has infinite index
(for $i\neq j, a^i$ and $a^j$ always have different distributions), it cannot
be described by a finite automaton, so no regular approximation can ever be
perfect.  The language $D_1^1$ of matched parentheses of depth one, as given
by the CFG $S\rightarrow aTb | SS | \lambda, T \rightarrow ab | abT,$ can be
easily described without this CFG by a finite automaton, and so could be the
language $D_1^2$ of matched parentheses of depth at most two, and so forth. In
general, we define a {\bf bounded counter} of depth $k$ as a finite automaton
having states $\{0,1, \ldots,k-1\}$ and with transitions under $a (b)$ always
increasing (decreasing) state number, except in state $k-1$ (resp. 0) where
$a$ (resp. $b$) keeps the automaton looping over the same
state.\index{bounded counter|textbf}
It is intuitively clear that with increasing $k$ the $D_1^k$
get increasingly close to $D_1$: our concern here is to capture this intuition
in a formal system.

As in classical analysis, the general problem of approximating an arbitrary
weighted language $f$ by a series of weighted languages $f_k$ reduces to the
special case of approximating zero by a series. We will say that the $f_k$
{\bf tend to} $f$ (denoted $f_k \rightarrow f$) if the symmetric differences
$(f \setminus f_k) \cup (f_k \setminus f) \rightarrow 0$ (here $0$ is the
language in which every string has weight 0). We discuss a variety of measures
$\mu$ for quantitative comparison. These will all assign numerical values
between zero and plus infinity, and if $U \subseteq V$ or $f \leq g$ we will
have $\mu(U) \leq \mu(V)$ or $\mu(f) \leq \mu(g)$. Some of them are true
measures in the sense of measure theory; others are just useful figures of
merit.  We begin with weighted languages over a one-letter alphabet and see
how a simple quantitative measure, {\it density}, can be properly generalized
for $k$-letter alphabets.  Eilenberg (1974:225) defines the {\bf density} of
a language $L$ over a one-letter alphabet as \[\lim_{n \rightarrow \infty}
\frac{| \{ \alpha \in L | |\alpha| < n \} |}{n}\] if this limit exists.
This definition can be generalized for languages over a $k$-letter alphabet
$\Sigma$ in a straightforward manner: if we arrange the elements of $\Sigma^*$
in a sequence $\phi$ and collect the first $n$ members of $\phi$ in the sets
$\Sigma_n$,

\begin{equation}
\lim_{n \rightarrow \infty} \frac{| L \cap \Sigma_n |}{n} = \rho_{\phi}(L)
\end{equation}

\noindent
can be interpreted as the density of $L$ when it exists. Since this definition
is not independent of the choice of the ordering $\phi$, we need to select a
canonical ordering. We will call an ordering $\psi$ {\bf length-compatible} if
$|\psi(n)| \leq |\psi(m)|$ follows from $n<m$. It is easily seen that for
arbitrary alphabet $\Sigma$ and language $L$, if $\phi$ is a length-compatible
ordering of $\Sigma^*$ and the limit in (5.18) exists, then it exists and has
the same value for any other length-compatible ordering $\psi$. In such cases,
we can in fact restrict attention to the subsequence of (5.18) given by
$\Sigma^0, \Sigma^1, \Sigma^2, \ldots$. If we denote the number of strings of
length $n$ in $L$ by $r_n$, {\bf natural density} $\nu$ can be defined by
\index{density!natural|textbf}

\begin{equation}
\nu(L) = \lim_{n \rightarrow \infty} \frac{\sum_{i=0}^n r_i}{\sum_{i=0}^n k^i}
\end{equation}

\smallskip
\noindent
To define density by (5.19) over $k$-letter alphabets for $k > 1$ would have
considerable drawbacks since this expression fails to converge for some simple
languages, such as the one containing all and only strings of even length
\cite{Berstel:1973}. To avoid these problems, we introduce the generating
function $d(z)=\sum_{n=0}^{\infty}r_nz^n$ and define the {\bf Abel density}
$\rho$ by 
\begin{equation} 
\rho(L) = \lim_{z \rightarrow 1} (1-z)d(z/k)
\end{equation} 

\smallskip
\noindent
if this limit exists. \index{density!Abel|textbf} A classical theorem of Hardy
and Littlewood asserts that whenever the limit (5.19) exists, (5.20) will also
exist and have the same value, so our definition is conservative. (We use the
name Abel density because we replaced the Ces\`aro summation implicit in
Berstel's and Eilenberg's definition with Abel summation.)

For weighted languages, the number of strings $r_n$ is replaced by summed
weights $R_n = \sum_{|w|=n} f(w)$ of the strings of length $n$, otherwise the
definition in (5.20) can be left intact. As long as the individual values
$f(w)$ cannot exceed 1, the Abel density will never be less than zero or more
than one irrespective of whether the total sum of weights converges or not.
As Berstel notes (1973:346), natural density will not always exist for regular
languages, even for relatively simple ones such as the language of even
length strings over a two-letter alphabet. Abel density does not suffer from
this problem, as the following theorem shows. 

\smallskip
\noindent
{\bf Theorem 5.4.1} Let $L$ be a regular language over some $k$-letter alphabet
$\Sigma$. The Abel density $\rho(L)$ defined in (5.20) always exists and is
the same as the natural density whenever the latter exists. The Abel density
of a regular language is always a rational number between 0 and 1.

\smallskip
\noindent 
{\bf Proof} Since $r_n \leq k^n$, $d(z) \leq 1/(1-kz)$ so $\rho(L) \leq 1$
will always hold. The transition matrix $A$ associated with the finite
deterministic automaton accepting $L$ has column sums $k$, so $B=A/k$ is
stochastic. Define $H(z)$ as $(1-z)(E-zB)^{-1}$.  The limiting matrix
$H=\lim_{z \rightarrow 1} H(z)$ always exists, and the density of $L$ is
simply $\vec{v}H\vec{e}_i$, where the $j$th component of $\vec{v}$ is 1 of the
$j$th state is an accepting state (and 0 otherwise), and the initial state is
the $i$th. Since $H(z)$ is a rational function of $z$ and the rational
coefficients of $B,$ and its values are computed at the rational point $z=1$,
every coefficient of $H$ is rational and so is the density. 

This is not to say that nonregular languages will always have a density. For
example, the context-sensitive language $\{a^i | 4^n \leq i < 2 \cdot 4^n, n
\geq 0\}$ can be shown not to have Abel density over the one-letter alphabet
$\{a\}$.  When it exists, Abel density is always additive because of the
absolute convergence of the power series in $z=1$. 

The proof makes clear that shifting the initial state to $i'$ will mean only
that we have to compute $\vec{v}H\vec{e}_{i'}$ with the same limiting matrix
$H$, so density is a bilinear function of the (weighted) choice of initial and
final states. Because of this, density is more naturally associated to {\bf
  semiautomata},\index{semiautomaton} also called {\bf state
  machines},\index{state machine} which are defined as FSA but without
specifying initial or accepting states, than to FSA proper. The density vector
$H\vec{e}_i$ can be easily computed if the graph of the finite deterministic
automaton accepting L is strongly connected. In this case the Perron-Frobenius
theorem can be applied to show that the eigenvalue $k$ of the transition
matrix has multiplicity 1, and the density vector is simply the eigenvector
corresponding to $k$ normed so that the sum of the components is 1.  If this
condition does not hold, the states of the automaton have to be partitioned
into strongly connected equivalence classes. Such a class is {\bf final} if no
other class can be reached from it, otherwise it is {\bf
  transient}.\index{transient state|textbf}\index{final state|textbf}

\medskip\noindent
{\bf Theorem 5.4.2} 
The segment corresponding to a final class in the overall density vector is
a scalar multiple of the density vector computed for the class in question. 
Those components of the density vector that correspond to states in some
final class are strictly positive, and those that correspond to states in
the transient class are 0. 

\smallskip\noindent {\bf Proof} By a suitable rearrangement of the rows and
columns of the transition matrix $A,$ $B=A/k$ can be decomposed into blocks
$D_i$, which appear in the diagonal, a block $C$, which corresponds to
transient states and occupies the right lowermost position in the diagonal of
blocks, and blocks $S_i$ appearing in the rows of the $D_i$ and the columns of
$C$.  The column norm of $C$ is less than 1, so $E-C$ can be inverted, and its
contribution to the limiting matrix is 0. The column sum vectors of $S_i$ can
be expressed as linear combinations of the row vectors of $E-C$, and the
scalar factors in the theorem are simply the $n$th coefficients in these
expressions, where $n$ is the number of the initial state. Moreover, since
$(E-C)^{-1}=\sum_{i=1}^{\infty} C^i$ holds, all these scalars will be strictly
positive. By the Perron-Frobenius theorem, the density vectors corresponding
to the (irreducible) $D_i$ are strictly positive.

\medskip We will say that a language $L$ over $\Sigma$ is {\bf blocked off} by
a string $\beta$ if $L\beta \Sigma^* \cap L = \emptyset$. $L$ is {\bf
vulnerable} if it can be blocked off by finitely many strings; i.e. iff
$$\exists \beta_1, \ldots, \beta_s \forall \alpha \in L \exists \beta \in
\{\beta_1,\ldots,\beta_s\} \forall \gamma \in \Sigma^* \alpha\beta\gamma
\not\in L$$ \index{blocking off|textbf}\index{vulnerability|textbf}


\medskip\noindent
{\bf Theorem 5.4.3} For a language $L$ accepted by some finite deterministic
automaton $A$, the following are equivalent:
\begin{itemize}
\item[(i)] $\rho(L)=0$.
\item[(ii)] The accepting states of $A$ are transient.
\item[(iii) ] $ L$ is vulnerable. 
\end{itemize}

\smallskip
\noindent
{\bf Proof} (iii) $\Rightarrow$ (i). If $\rho (L) > 0, A$ 
has accepting states in some final class by Theorem 5.4.2. If $\alpha \in L$
brings $A$ in such a state, then no $\beta \in V^*$ can take $A$
out of this class, and by strong connectedness there is a $\gamma \in V^*$
that takes it back to the accepting state, i.e. $\alpha\beta\gamma \in
L$. Thus, $\alpha$ cannot be blocked off.

(i) $\Rightarrow$ (ii). This is  a direct consequence of Theorem 5.4.2.

(ii) $\Rightarrow$ (iii). If the accepting states of $A$ are transient,
then for every such state $i$ there exists a string $\beta_i$ that takes the
automaton in some state in a final class. Since such classes cannot be left
and contain no accepting states, the strings $\beta_i$ block off the language.

\medskip\noindent
{\bf Theorem 5.4.4} Probabilistic languages have zero density.

\smallskip
\noindent
{\bf Proof} We divide the language into three disjoint sets of strings
depending on whether $f(w)=0$, $0 < f(w) <\varepsilon$, or $f(w) \geq
\varepsilon$ holds.  The first of these, $f_0^0$, obviously does not
contribute to density. The third, $f_\varepsilon^{0+}$, can have at most
$1/\varepsilon$ members, and will thus have $R_n = 0$ for $n$ greater than the
longest of these. In other words, the generating function for these is a
polynomial, of necessity bounded in the neighborhood of 1, so the limit in
(5.20) is 0. Thus only words with weight $0 < f(w) <\varepsilon$ can
contribute to density, but their contribution is $ \leq \varepsilon$, and we
are free to choose $\varepsilon$ as small as we wish.

\medskip\noindent
{\bf Theorem 5.4.5} Hidden Markov weighted languages have zero density.

\smallskip
\noindent
The proof of this theorem is deferred to Section~5.5 where hidden Markov
models are defined -- we stated the result here because it provides additional
motivation for the study of zero density languages, to which we turn now.

\subsection{Zero density}


In Theorem 5.4.3, blocking off corresponds to the intuitive notion that certain
errors are nonrecoverable: once we have said something that arrests the
grammatical development of the sentence (generally any few words of nonsense
will do), no amount of work will suffice to get back to the language {\it
  within} the same sentence. One needs an explicit pause and restart. There
are some sentence-initial strings that are harder to block, e.g. whenever we
gear up for explicit quotation; {\it And then, believe it or not, he said} is
easily followed by any nonsense string and can still be terminated
grammatically by {\it whatever that means}. But such sentence-initial strings
can still be blocked off by better crafted $\beta$s; e.g. those that
explicitly close the quotation and subsequently introduce unrecoverable
ungrammaticality.

It is of course highly debatable whether natural languages can be construed as
regular stringsets, but to the extent they can, Theorem 5.4.3 applies and zero
density follows. Clearly any length-delimited subset, e.g. English sentences
with less than 300 words (which contains the bulk of the data), will be
regular, so to escape Theorem 5.4.3 one would need a strong (infinite
cardinality, nonzero density) set of counterexamples to demonstrate that the
entire stringset is not zero density. No such set has ever been proposed. Even
if we accept as grammatical, without reservation, for arbitrary length, the
kind of center-embedded or crossed constructions that have been proposed in the
literature, these carry very strong conditions on the category of elements
they can contain. For example, the Dutch crossed constructions must begin with
a specified formative {\it dat} followed by NPs followed by infinitival VPs --
this alone is sufficient to guarantee that they have zero density.

Since the most important weighted languages, probabilistic languages, and
natural languages all have zero density, it is of great importance to
introduce finer quantitative measures. We will consider three alternatives:
{\it Bernoulli density, combinatorial density}, and {\it saturation}.  Aside
from finite lists, the simplest class of weighted languages is {\it Bernoulli
  languages} over $k$ letters $a_1, \cdots , a_k$ that have positive
probabilities $p_1, \cdots, p_k$ summing to one: the weight $f$ of a string
$a_{i_1}a_{i_2} \cdots a_{i_r}$ is defined as $p_{i_1}p_{i_2} \cdots p_{i_r}$.
Note that this is a probabilistic {\it process} but not a probabilistic
language. For any $n >0 $ the probabilities of the strings of length exactly
$n$ sum to 1, so the overall probabilities diverge (a trivial problem that we
will fix in Example 5.5.3 by penalizing nontermination at each stage).
Beauquier and Thimonier (1986) take Bernoulli languages as their starting
point and define the {\bf Bernoulli density} $\delta$ of a language by the
weights of its prefixes (minimal left factors):
\index{density!Bernoulli|textbf}

\begin{equation}
\delta(L) = \sum_{\alpha \in Pref(L)} f(\alpha)
\end{equation}

\smallskip
\noindent
where $\textit{Pref}(L)$ contains all those strings in $L$ that have no left
factors in $L$. In the equiprobable case, for languages where every word is a
prefix, this coincides with the {\bf combinatorial density}
$\kappa(L)=\sum_{n=1}^{\infty} r_n/k^n$. Finally, the {\bf saturation}
$\sigma(L)$ of a language $L$ over a $k$-letter alphabet is given by the
reciprocal of the convergence radius of $d(z/k)$ -- this again generalizes
trivially to arbitrary weighted languages.
\index{density!combinatorial|textbf} \index{saturation|textbf}

Although clearly inspired by Bernoulli languages, Bernoulli density as a formal
construct is meaningful for any weighted or probabilistic language, though it
may be infinitely large. Combinatorial density is restricted to weighted
languages and languages with Abel density 0. For $k >1$ if $\rho(L)>0$, the
terms in $\kappa$ will converge to this value so combinatorial density itself
will diverge. As for saturation, we have the following theorem.

\medskip\noindent
{\bf Theorem 5.4.6} If $\rho(L) > 0$, then $\sigma(L)=1$. If $\rho(L) =0$ and
$L$ is regular, then $\sigma(L) < 1$. If $L \subset L'$, then $\sigma(L) \leq
\sigma(L')$. $\sigma(L)=0$ iff $L$ is finite. 

\bigskip\noindent
{\bf Proof} If $\lim_{z \rightarrow 1} (1-z)d(z/k) > 0$, then 
$d(z/k)$ tends to infinity in $z=1$, and since it is convergent inside the 
unit circle, $\sigma$ must be 1. If $L$ is regular, $d(z/k)$ is rational
(since it is the result of matrix inversion). Therefore if it is bounded in 
$z=1$, it has to be convergent on a disk properly containing the unit circle.
If $L_1 \subseteq L_2,$ then $d_1(z/k) \leq d_2(z/k)$, and since the Taylor
coefficients are nonnegative, it is sufficient to look for singularities on
the positive half-line. There $d_1(z/k)$ must be convergent if $d_2(z/k)$
is convergent, so $\sigma (L_1) \leq \sigma (L_2)$. Finally, if $d(z/k)$ is 
convergent on the whole plane, then $f(1)=\sum_{n=0}^{\infty} r_n < \infty$,
so $L$ must be finite.

\medskip\noindent 
{\bf Discussion} 
Which of these three measures is more advantageous depends on the situation.
Neither Bernoulli nor combinatorial density is invariant under multiplication
with an arbitrary string, but for Abel density and for saturation we have
$\rho(L)=\rho(\alpha L)=\rho(L\alpha)$ and $\sigma(L)=\sigma(\alpha L)=
\sigma(L\alpha)$ for any string $\alpha$ and for any $L$, not just those with
zero density. While (5.20) does not always yield a numerical value, Bernoulli
density always exists. Although this suggests that $\delta$ would be a better
candidate for a basic measure in quantitative comparisons than $\rho$, 
there is an important consideration that points in the other direction: while
Bernoulli density is only an exterior measure, additive only for languages
closed under right multiplication, Abel density is always additive. If $L$ is
not closed, there is an $\alpha \in L$ and a $\beta \in V^*$ such that
$\alpha\beta \not\in L$.  Either $\alpha$ is a prefix or it contains a left
factor $\alpha_0 \in L$ that is.  Consider the two-member language
$X=\{\alpha_0,\alpha\beta\}$:

\[\delta(X) = p(\alpha_0) \not= p(\alpha_0) + p(\alpha\beta) =
\delta(X \cap L) + \delta(X \setminus L) \]

\smallskip
\noindent
Thus, by Caratheodory's theorem, $L$ cannot be measurable. Note also that
for languages closed under right multiplication $r_{n+1} \geq kr_n$, so the
coefficients in $d(z/k)=\sum_{n=0}^{\infty} r_nz^n/k^n$ are nondecreasing.
Therefore the coefficients of the Taylor expansion of $(1-z)d(z/k)$ are
nonnegative, and the Abel density $\rho$ also exists.

Now we are in a better position to return to our motivating example, the
approximation of the Dyck language $D_1$ by the languages $D_1^k$ that contain
only matching parentheses of depth $k$ or less. The number of strings of
length $2n$ in $D_1$ is given by $r_{2n} = {2n\choose n}/(n+1)$, so
$d(z)=(1-\sqrt{1-4z^2})/2z^2$ and thus $\rho(D_1) = 0$. This makes all three
finer measures discussed so far usable: taking the left and the right
parenthesis equiprobable, $\delta(D_1)=1/2^2+1/2^4+1/2^6+\ldots=1/3$,
$\kappa(D_1) \approx 0.968513$, and $\sigma(D_1)=1$.  Using Bernoulli density,
it is clear that the $D_1^k$ approximate $D_1$: the shortest string in $D_1
\setminus D_1^k$ has length $2k+2$ and $\delta(D_1 \setminus D_1^k)=3/4^k$,
which tends to zero as $k$ tends to infinity. Using saturation, it is equally
clear that the $D_1^k$ do not approximate $D_1$: no matter how large $k$ we
choose, the convergence radius of the differences remains 1. 

\bigskip
\noindent
{\bf Exercise 5.6} Is $D_1^k$ defined by a bounded counter of depth $k$?
Compute $\kappa(D_1 \setminus D_1^k)$.

\bigskip
\noindent
One class of weighted languages that deserves special attention is when the
weights are set to be equal to the number of different derivational histories
of the string.  In the case of CFGs, we can obtain the generating function
$d(z)$ that counts the number of occurrences by solving a set of algebraic
equations that can be directly read off of the rules of the grammar (Chomsky
and Sch\"utzenberger 1963). For example, the grammar $S\rightarrow aSb | SS |
\lambda$ generates $D_1$, and the generating function associated with the
grammar will satisfy the functional equation $d(z)=z^2 d(z)+d^2(z) +1$. Thus
$d(z)$ will have its first singularity in $\sqrt{3} > 1$, so the language is
supersaturated: its saturatedness $2\sqrt{3}$ can be interpreted as the degree
of its ambiguity.  \nocite{Chomsky:1963} \nocite{Beauquier:1986}

If strings of length $n$ are generated by some CFG approximately $a_n$ times,
then $a_{n+m}\approx a_na_m$ because context-freeness makes disjoint subtrees
in the generation tree independent.  Therefore, $a_n \approx c^n$ and the base
$c$ is a good measure of ambiguity. By the Cauchy-Hadamard theorem,
$\sigma=\limsup \root n \of a_n = c$. Note also that in the unambiguous case,
$\log(\sigma) = \limsup \log(a_n)/n$ can be interpreted as the {\it channel
capacity} of the grammar \cite{Kuich:1970}.  In the unambiguous case as well
as in the case of context-free languages where weights are given by the degree
of ambiguity, the generating functions corresponding to the nonterminals
satisfy a system of algebraic equations, and therefore $d(z)$ will have its
first singularity in an algebraic point. Therefore, in such cases, saturation
and Abel density are algebraic numbers. 

Although from the applied perspective issues such as approximating the Dyck
language are meaningful only if probabilities, rather than derivational
multiplicities, are used as weights, the following definitions are provided in
their full generality. Given two weighted languages $f$ and $g$ over the same
alphabet $T$ and a precision $\varepsilon > 0$, we define the {\bf
underestimation error} \index{underestimation error|textbf}
$U(\varepsilon)$ of $g$ with respect to $f$ by

\begin{equation} 
U(\varepsilon)=\sum_{\stackrel{\alpha \in T^*}{g(\alpha) < f(\alpha) -
    \varepsilon}} f(\alpha) -g(\alpha)
\end{equation} 

\noindent
and the {\bf overestimation error} by 
\index{overestimation error|textbf}

\begin{equation} 
T(\varepsilon) = \sum_{\stackrel{\alpha \in T^*}{g(\alpha) > f(\alpha) +
    \varepsilon}} g(\alpha) -f( \alpha)
\end{equation}

\smallskip 
\noindent 

If we start from an unweighted language and use the values of the
characteristic function as weights, (5.22) and (5.23) are often divergent, but
for probabilistic languages they always converge and tell us a great deal
about the structure of the approximation. To apply them to the Dyck case,
where different measures of approximation so far have led to different
conclusions, we need to endow both $D_1$ and $D_1^k$ with a probability
function. We could start by setting all Dyck strings of length $2n$
equiprobable and prescribing a reasonable distribution, such as lognormal, on
overall length. Yet one would inevitably feel that this is more in the nature
of a problem book exercise than something definitive about the way language
works -- what we need is empirical data.

Here we shall briefly consider parenthetical constructions in the Linux
kernel.\index{Linux}\index{parenthetical expression} This has the advantage of
removing performance limitations: on the `hearer' side, the compiler, unlike
humans, is ready to handle parentheticals of large depth, and on the `speaker'
side, the authors write the code with a great deal of attention and using
long-term memory aids rather than relying on short-term memory alone.  Purists
may object that kernel hacking relies on skills very distant from human
language production and comprehension, but in written, let alone spoken,
language we rarely find embedding of depth 5, and depth 7 is completely
unattested, while in the kernel, in just three hundred thousand expressions,
we find over a thousand with depth 7 and dozens with depth 10 (topping out at
depth 13). Restricting our attention to English prose would only serve to
trivialize the issue.

A simple but attractive model of $D_1^k$ has one state for each depth starting
at 0: at each level, we either open another parenthesis with probability $p_i$
or close one with probability $1-p_{i+1}$. At the bottom, we have $p_0=1$ and
at the top we assume $p_k=0$, so as to make the automaton finite. Since the
model only has $k-1$ free parameters and is perfectly symmetrical left to
right, it will fail to account for many observable regularities; e.g. that
back-loaded constructions like (()(())) are more frequent than their
front-loaded counterparts like ((())()) by about half.

\smallskip 
\noindent 
{\bf Exercise 5.7$^\dagger$} Take a large body of code, remove program text,
format strings, and comments, and replace all types of left and right
parentheses by ( and ), respectively. Fit a model with parameters $p_1, \ldots
p_{k-1}$ as above, and compute under- and overestimation errors for different
values of $\varepsilon \approx 1/\sqrt N$, where $N$ is your corpus size.  For
$\varepsilon > 0$ are the under- and overestimation errors always coupled in
the sense that changing one will necessarily change the other as well? Does
the model improve from increased $k$? Can better models be found with the same
number of free parameters?

\subsection{Weighted rules}

The idea that we should investigate weighted languages using weighted grammars
is a natural one, and probabilistic generalizations of the entire Chomsky
hierarchy of grammars were presented early on (for finite automata, see Rabin
1963).\nocite{Rabin:1963} In these systems, we assign some value between 0 and
1 to each production, and require these values to sum to one for all
productions sharing the same left-hand side. Here we depart significantly from
the historical line of development because probabilistic grammars at or above
the CFG level have shown very little ability to characterize the distributions
that occur in practice. The probabilistic generalizations of finite automata
that are of practical and theoretical significance, finite $(k$-$)$transducers
and (hidden) Markov processes, will be discussed in Section~5.5. Here we turn
directly to context-sensitive rules because the practice of associating
probabilities to such rules has long been standard practice within {\it
  sociolinguistics}, the study of language variation. \index{sociolinguistics}

In his study of the speech patterns in New York City, Labov
(1966)\nocite{Labov:1966} noted that contraction of {\it is} to {\it 's} as in
{\it John's going} is almost universal among both white and African-American
speakers when the subject is a pronoun {\it (He's going)}.  When a full noun
ending in a vowel precedes, contraction is more likely than when a full noun
ending in a consonant precedes, $p(\textit{Martha's going}) >
p(\textit{Robert's going})$. When the contracting {\it is} is copulative, as
in {\it John's a good man}, or locative, as in {\it John's in the bathroom},
contraction is less likely than when it appears before an ordinary VP as in
{\it John's going}, and contraction is most likely preceding the future
auxiliary, as in {\it John's gonna go}.

Arranging the factors in three rows by four columns, with $i$ running over the
values {\it pronoun, V-final, C-final} and $j$ running over the values {\it
  copula, locative, ordinary, gonna}, the probabilities (observed frequencies)
of {\it is}-contraction form a $3 \cdot 4$ matrix whose values $p_{ij}$
decrease with $i$ and increase with $j$. The original {\bf additive variable
  rule model} (Cedergren and Sankoff 1974) simply assumed that there are
additive constants $\gamma_1, \gamma_2$ and $\delta_1, \delta_2, \delta_3$
such that $p_{ij} = p_0 + \sum_{k=1}^i \gamma_k + \sum_{l=1}^j
\delta_l$. \index{variable rule model|textbf} Obviously, there is no guarantee
that the observed pattern of frequencies can be fully replicated. The model
assumed finding the parameters by maximum likelihood fit.

When the observed $p_{ij}$ are small (close to 0), often a good fit can be
found. When the $p_{ij}$ are large (close to 1), we can take advantage of the
fact that the same outcome, e.g. deleting a vowel or voicing a consonant, can
be equally well analyzed by the obverse rule (addition of a vowel, devoicing a
consonant), for which the application probability will come out small. For the
case where the $p_{ij}$ are close to 1/2, Cedergren and Sankoff (1974) also
introduced a {\bf multiplicative variable rule model} where the sums are
replaced by products or, what is the same, an additive model of log
probabilities is used. However, it is unclear on what basis we could choose
between the additive and the multiplicative models, and when the probabilities
are outside the critical ranges neither gives satisfactory results. 

As readers familiar with logistic regression will already know, most of these
difficulties disappear when probabilities $p$ are replaced by {\bf odds}
$p/(1-p)$. \index{odds|textbf} We are interested in the effect some factors
$F_1, \ldots , F_k$ have on the odds of some event $H$ like {\it
  is}-contraction. We think of the $F_i$ as possibly causative factors over
which we may have control in an experiment. For example, if the experimenter
supplies the proper name at the beginning of the sentence, she may decide
whether to pick one that ends in a consonant or a vowel. In order to account
for factors that lie outside the experimenter's control (or even awareness), we
add a background factor $F_0$. By Bayes' rule, we have $P(H|F_0 F_1\ldots F_k)
= P(H|F_0)P(F_1\ldots F_k|HF_0)/P(F_1\ldots F_k|F_0)$ and similarly
$P(\overline{H}|F_0 F_1\ldots F_k) = P(\overline{H}|F_0)P(F_1\ldots
F_k|\overline{H}F_0)/P(F_1\ldots F_k|F_0)$ for the complementary event
$\overline{H}$. Dividing the two, we obtain the odds $P(H)/P(\overline{H})$ as

\begin{equation}
O(H)=\frac{P(H|F_0)P(F_1\ldots F_k|HF_0)}{
P(\overline{H}|F_0)P(F_1\ldots F_k|\overline{H}F_0)}
\end{equation}

\noindent
because the terms $P(F_1\ldots F_k|F_0)$ simplify. Taking logarithms in (5.24),
we see that the log odds of $H$ given the factors $F_0\ldots F_k$ are now
obtained as the log odds of $H$ given the background plus $\log(P(F_1\ldots
F_k|HF_0)/P(F_1\ldots F_k|\overline{H}F_0))$. When the $F_1,\ldots,F_k$ are
independent of $F_0$, which lies outside the control of the experimenter, and
of each other (which is expected given that the experimenter can manipulate
each of them separately), we can apply the product rule of conditional
probabilities and obtain

\begin{equation}
e(H|F_0\ldots F_k)=e(H|F_0) + \sum \log \frac{P(F_i|HF_0)}{P(F_i|\overline{H}F_0)}
\end{equation}

\noindent
where we use $e(A|B)$ to denote the log odds of $A$ given $B$. While (5.25) is
exact only as long as the independence assumption is met, we take it to be
indicative of the general class of models even when independence is not fully
assured. Log odds define a surface over the space spanned by the $F_i$, and
(5.25) is a gradient expansion with constant term $e(H|F_0)$ plus some
functions (logs of conditional probability ratios) that we may as well assume
to be linear since they are known to us only at two points, when the trigger
$F_i$ is present and when it is absent.

In the terminology of modern machine learning, the independent
(experimenter-controlled) \index{machine learning features} variables or
triggers $F_i$ are called {\it features} or {\it predicates}.\footnote{This
  terminology is completely foreign to sociolinguistics, where `feature'
  always means distinctive feature in the sense of Section~3.2. Since in
  syntax and semantics `predicate' is always used in the logical sense, there
  is no easy way out.} Generally, features are categorial (present or absent,
on or off), and we do not assume that they are logically or empirically
independent of each other. For example, the feature `preceding word is
vowel-final' is always on when the subject is {\it he} or {\it she}, yet the
presence of {\it he/she} in that position is such a salient trigger of {\it
  is-}contraction that we may want to treat it as a feature on its own.  From
(5.25) we take the lesson that the log odds of the dependent variable are to
be sought in the form $e(H)=\lambda_0 + \sum \lambda_i F_i$, but note that in
sociolinguistics the primary goal is {\it data modeling}, describing the
observed probabilities with the least number of parameters, while in machine
learning the goal is {\it predicting} the value of the dependent variable
given some features $F_i$ (these two goals are not necessarily different).

\section{The regular domain}

Given a system of grammatical description, \index{generative capacity|textbf}
we would like to characterize both the set of grammars that can be written in
this system and the set of languages that can be described by such grammars.
In the combinatorical view, a language is identified as a stringset, and {\bf
  weak generative capacity} refers to the set of formal languages that can be
generated by the permissible grammars, while {\bf strong generative capacity}
is defined as the structure-generating capacity of the system
\cite{Chomsky:1965a}. In some cases, it is possible to investigate issues of
strong generative capacity by linearization, but, on the whole, less
combinatorical views of syntax are not served well by these definitions.
Strong generative capacity needs significant reworking before it can be put to
use over the wider range of grammatical theories considered here (see Rogers
1998, Miller 1999). \nocite{Rogers:1998} \nocite{Miller:1999}

Weak generative capacity is practically meaningless when it comes to
probabilistic versions of the theory: if defined through niveau sets, only the
full stringset $\Sigma^*$ can be obtained in the limit. One could in principle
use $f_0^0$ to encode an arbitrary stringset, but this runs counter to the
probabilistic interpretation, where small differences in weight are considered
empirically undetectable. If defined so as to include the actual numerical
values, the study of weak generative capacity quickly turns into a study of
algebraic independence and transcendence degree: we discuss the reasons for
this in Section~5.5.1, where we treat weighted FSA and transducers. In
Section~5.5.2, we turn to hidden Markov models, which play a critical role in
the applications.

\subsection{Weighted finite state automata}

{\bf Probabilistic finite state automata} (PFSAs) are defined as FSAs
with the additional requirement that the weights associated to all transitions
that can be taken from a given state on scanning a symbol sum to one.
\index{probabilistic finite state automaton, PFSA|textbf} Note that we do not
require the probabilities of all transitions that leave a state to sum to one.
In fact, if the alphabet has $n$ symbols the total weight of such transitions
will be $n$. For a one-letter alphabet already, there is the following theorem.

\smallskip
\noindent
{\bf Theorem 5.5.1} \cite{Ellis:1969} There exist probabilistic languages $f:
\{a\}^* \rightarrow {\Bbb R}^+$ that cannot be characterized by any PFSA.

\smallskip\noindent The original proof by Ellis explicitly constructs an
infinite set of weights as $1/\sqrt{p_i}$, where $p_i$ is the smallest prime
larger than $4^i$ and proceeds by counting degrees of field extensions. In
response, Suppes (1970) \nocite{Suppes:1970} argued that

\begin{quote}
From the empirically oriented standpoint ... Ellis' example, while perfectly
correct mathematically, is conceptually unsatisfactory, because any finite 
sample of $L$ drawn according to the density $p$ could be described also by 
a density taking only rational values. Put another way, algebraic examples of 
Ellis' sort do not settle the representation problem when it is given a
clearly statistical formulation. Here is one such formulation. ...

\smallskip
{\it Let $L$ be a language of type $i$ with probability density $p$. Does
  there always exist a probabilistic grammar $G$ (of type $i$) that generates
  a density $p'$ on $L$ such that for every sample $s$ of $L$ of size less
  than $N$ and with density $p_s$ the null hypothesis that $s$ is drawn from
  $(L,p_s')$ would not be rejected?}

\smallskip
I have deliberately imposed a limit $N$ on the size of the sample in order 
to directly block asymptotic arguments that yield negative results.
\end{quote}

Suppes conjectured that the problem, stated thus, has an affirmative solution.
To approach the issue formally, we first define a {\bf weighted transducer} as
a mapping $f: \Sigma^* \times \Gamma^* \rightarrow R$, where $\Sigma$ and
$\Gamma$ are finite alphabets and $R$ is a semiring.
\index{transducer!weighted} Note that in the general case we do not require
the mapping to be homomorphic in the sense that if $\sigma, \sigma' \in
\Sigma^*$ and $\gamma, \gamma' \in \Gamma^*$, then
$f(\sigma\sigma',\gamma\gamma')= f(\sigma,\gamma) \cdot f(\sigma',\gamma')$
(here $\cdot$ is the product operation of $R$).

To recover our earlier definition of weighted languages as a special case, we
set $\Sigma=\Gamma$ and take the transduction between $\Sigma^*$ and
$\Sigma^*$ to be the identity mapping. (Again, there is no requirement that
weights multiply and $f(\sigma\sigma')= f(\sigma) \cdot f(\sigma')$ -- were
we to impose such a requirement, the only weighted languages would be Bernoulli
languages.) This more complicated definition has the advantage that the
definition of composing weighted transductions given below will extend
smoothly to the case of transducing a weighted language by a weighted
transduction. If $f: \Sigma^* \times \Gamma^* \rightarrow R$ and $g: \Gamma^*
\times \Delta^* \rightarrow R$ are two weighted transductions, their {\bf
  composition} $f \circ g$ is defined to assign $(\sigma,\delta)$ the weight
$\sum_{\gamma \in \Gamma^*} f(\sigma,\gamma) \cdot g(\gamma,\delta)$, where
$\cdot$ refers to the multiplication operation and $\sum$ to the addition
operation of $R$. \index{composition (of weighted transductions)|textbf}
(While the definition is meaningful for all kinds of weighted transductions,
the motivating case is that of probabilities.)

Of central interest are {\bf weighted finite state transducers (WFST)} given
by a finite set $\Sigma$ of states, and a finite set of {\bf weighted
  transitions} \index{transition!weighted} $(b,a,l_1,\cdots,l_n,r)$ where $b$
and $a$ are the states {\it b}efore and {\it a}fter the move, the $l_j$ are
letters scanned on the $j$th tape during the move or $\lambda$, and $r$ is a
weight value.  When for every $b$ and $l_1,\cdots,l_n$ there is at most one $a
\in S$ such that $(b,a,l_1,\cdots,l_n,r) \in T$ with $r \neq 0$, the
transducer is {\bf deterministic}, and when $\lambda$ never appears in any
transition it is called {\bf length-preserving}.
\index{length-preserving FST|textbf}\index{deterministic FST|textbf}\index{finite state transducer, FST!length-preserving|textbf}\index{finite state transducer, FST!deterministic|textbf}\index{finite state transducer, FST!probabilistic|textbf} There are two natural ways to
proceed from weighted transductions to weighted automata: either we can zero
out the input alphabet and obtain a pure generating device, or we can zero out
the output alphabet and obtain a pure accepting device. Either way, to fully
characterize a {\bf weighted FSA} \index{finite state automaton, FSA!weighted|textbf} over a one-letter alphabet $\{a\}$ requires only a set
of states $\Sigma=\{s_1, \ldots ,s_n\}$, for each state $i$ a set of values
$t_{i,j}$ that characterizes the probabilities of moving from state $s_i$ to
$s_j$ upon consuming (emitting) an alphabetic symbol, and a set of values
$l_{i,j}$ that characterizes the probabilities of moving from state $s_i$ to
$s_j$ by lambda-move (i.e. without consuming (emitting) an alphabetic
symbol). For the sake of concreteness, in what follows we will treat PFSA as
generating devices -- the results presented here remain true for acceptors as
well. To simplify the notation, we add a start state $s_0$ that only has
$\lambda$-transitions to $s_i$ for $i >0$ and replace all blocked transitions
by transitions leading to a sink state $s_{n+1}$ that has all (emitting and
nonemitting) transitions looping back to it and has weight 0 in the vector
$\vec{w}$ that encodes the mixture of accepting states. This way, we can
assume that in every state $s_i$ and at every time tick the automaton $A$
will, with probability 1, move on to another state $s_j$ and emit (or not
emit) a symbol during transition with probability $t_{i,j} (l_{i,j})$.

The probability of $A$ emitting $a^k$ is the sum of the probabilities over all
paths that emit $a$ $k$ times. Let us introduce a zero symbol $z$ and the
automaton $A'$ that emits $z$ wherever $A$ made a $\lambda$-transition. This
way, the probability of $A$ emitting $a, P(a|A)$ is the same as $\sum_{k,l
  \geq 0} P(z^k a z^l | A')$, similarly $P(a^2|A) = \sum_{k,l,m \geq 0} P(z^k
a z^l a z^m | A')$, and so forth. To compute the probability over a fixed path
$s_{i_1}, s_{i_2}, \ldots ,s_{i_n}$, we simply multiply the $r_i$ associated to
the transitions. If the transition matrix is $T+L$, where $t_{i,j}$ is the
probability of the emitting and $l_{i,j}$ the probability of the nonemitting
(lambda) transition from $s_i$ to $s_j$, the probability of going from $s_i$
to $s_j$ in exactly $k$ steps is given by the $(i,j)$th element of $(T+L)^k$.
It is convenient to collect all this information in a formal power series
$p(a,z)$ with coefficients in $R$ and noncommuting variables $a$ and $z$: in
matrix notation, $p(a,z)=\sum_{k \geq 0} (aT+zL)^k$. Given a fixed start state
$s_0$ and some weighted combination $\vec{w}$ of accepting states, the
probability of a string $x_1,\ldots,x_n \in \{a,z\}^n$ being generated by $A'$
is obtained as the inner product of the zeroth row of $(T+L)^n$ with the
acceptance vector $\vec{w}$ just as in Theorem 5.4.1. To obtain the probability
of, say, $a$ according to $A$, we need to consider $\sum_{k,l \geq 0} L^k T
L^l$, to obtain $P(a^2|A)$ we need to consider $\sum_{k,l,m \geq 0} L^k T L^l
T L^m$, and so forth. By collecting terms in expressions like these, it is
clear that we are interested in matrix sums of the form $I+L+L^2+L^3+\ldots$.

Notice that the spectral radius of $L$ is less than 1. Since $T+L$ is
stochastic, the rows of $L$ sum to at most 1, and strictly less for all rows
that belong to states that have outgoing emitting transitions with positive
weight. The row norm of the submatrix $R$ obtained by deleting the zeroth rows
and columns will be strictly less than one as long as pure rest states (those
with no emission ever) are eliminated, a trivial task. Therefore the
eigenvalues of $L$ are the eigenvalues of $R$ and zero, and as the eigenvalues
of $R$ are all less than one, the eigenvalues of $L$ are less than one, and
thus the matrix series $I+L+L^2+L^3+\ldots$ converges to $(I-L)^{-1}$. This
gives us a simple formula for $P(a|A)=\vec{e}L(I-L)^{-1}T(I-L)^{-1}\vec{w}$
($\vec{e}$ is the vector that is 1 on the zeroth component and zero elsewhere,
and $\vec{w}$ is the vector encoding the weighting of the final states -- the
initial $L$ is present because the zeroth state by convention has no emitting
transitions), $P(a^2|A)=\vec{e}L(I-L)^{-1}T(I-L)^{-1}T(I-L)^{-1}\vec{w}$, and
in general

\begin{equation}
P(a^k|A)= \vec{e}L((I-L)^{-1}T)^k(I-L)^{-1}\vec{w}
\end{equation}

\smallskip
\noindent
Since the only parts of (5.26) dependent on $k$ are the $k$th powers of a
fixed matrix $(I-L)^{-1}T$, the growth of $P(a^k|A)$ is expressible as a
rational combination of $k$th powers of constants $\lambda_1, \lambda_2, ...,
\lambda_n$ (the eigenvalues of $(I-L)^{-1}T$) with the fixed probabilities
$t_{i,j}$ and $l_{i,j}$. Therefore, the ratios of probabilities $P(a^k|A)$ and
$P(a^{k+i}|A)$ will tend to fixed values for all fixed $i$.  This proves 
the following characterization of PFSA languages over a one-letter alphabet:

\smallskip 
\noindent 
{\bf Theorem 5.5.2} Any PFSA language $p: \{a\}^* \rightarrow {\Bbb R}^+$ is
ultimately periodic in the sense that there exists a fixed $k$ and $l$ such
that for all $0 \leq i <k$ either all weights $p(a^{i+rk})$ are zero once
$i+rk >l$ or none of the weights $p(a^{i+rk})$ are zero for any $r$ such that
$i+rk >l$ and all weight ratios $p(a^{i+rk+k})/p(a^{i+rk})$ tend to a fixed
value $\lambda_1^k < 1$.

\smallskip 
\noindent 
{\bf Discussion} Since both $(I-L)^{-1}$ and $T$ are nonnegative, so is
$(I-L)^{-1}T$. If this is irreducible (every state is reachable from every
state), the Perron-Frobenius theorem guarantees the existence of a unique
greatest eigenvalue $\lambda_1$. If it is reducible, each transitive component
corresponds to an irreducible block on the main diagonal, and will have its
unique largest eigenvalue. When there is a unique largest one among these, it
will dominate the high powers of the matrix, but if different blocks have the
same largest eigenvalue $\lambda_1$, this will appear with multiplicity in the
overall matrix and entries over the main diagonal can contribute
$O(k^c\lambda_1^k)$ where $c$ is the number of components. However, such a
linear factor to the exponentially decreasing main term will be removed by
taking ratios.

\smallskip
\noindent 
{\bf Example 5.5.1} Let $p_0 = 2^{-1}, p_1 = p_2 = p_3 = p_4 = 2^{-2}/2^{2^1},
p_5 = \ldots = p_{20} = 2^{-3}/2^{2^2}$, and in general divide the probability
mass $2^{-n}$ among the next $2^{2^n}$ strings.  By Theorem 5.5.2, this
distribution will differ from any distribution that is obtained from a PFSA by
inspecting frequencies in finite samples, even though all probabilities are
rational, fulfilling Suppes' dictum.

\smallskip
\noindent 
{\bf Discussion} Theorem 5.5.2 provides, and Example 5.5.1 exploits, exactly
the kind of asymptotic characterization that Suppes wanted to avoid by
limiting attention to samples of a fixed size $<N$.  In hindsight, it is easy
to see where the strict empiricism embodied in Suppes' conjecture misses the
mark: with the availability of corpora (samples) with $N > 10^{10}$, it is
evident that our primary goal is not to characterize the underlying
distribution to ten significant digits but rather to characterize the tail,
where probabilities of $10^{-40}$ or many orders of magnitude below are quite
common. Recall from Section~4.4 that perfectly ordinary words often have text
frequencies below $10^{-6}$ or even $10^{-9}$, so sentences like {\it In our
  battalions, dandruffy uniforms will never be tolerated} will have
probability well below $10^{-40}$.  Even if we had corpora with $N > 10^{40}$,
the goal of reproducing the exact measured frequencies would be secondary: the
primary goal is to make reasonable predictions about unattested events {\it
  without} memorizing the details of the corpus.

The entire corpus available to language learners in the course of, say, the
first twenty years of their lives, is much less than $10^{10}$ words, yet they
have a clear sense that some hitherto unseen strings such as {\it furiously
sleep ideas green colorless} are much less likely than other, presumably also
unseen, strings such as {\it colorless green ideas sleep furiously}. By now it
is clear that our interest is precisely with comparing low-probability events,
and the central measure of success is whether, by making good enough
predictions about the high-probability examples that are observable in the
sample, we obtain enough generality to cover the low-probability cases, which
will in general not be observable. In an automaton with $10^6$ states (quite
feasible with today's technology) and $10^2$ letters (well below the size of
commonly used tagsets), we would have over $10^{14}$ free parameters, a huge
number that could only lead to overfitting, were we to follow Suppes' dictum
and restrict ourselves to precisely matching samples of size $10^{12}$. The
key issue, as we shall see in Chapter~8 and beyond, is to reduce the number of
parameters, e.g. by appropriately {\it tying} together as many as possible. 

By homomorphically mapping all tokens on the same token $a$, we obtain the
{\bf length distribution} of the sample.  \index{length distribution|textbf}
If the initial weighted language is regular, its homomorphic image will be
also, which by Theorem 5.5.2 means it must be ultimately periodic.  As the
proof above makes clear, for any $N$ we are at total liberty to prescribe a
probability distribution given by a nonincreasing sequence of the first $N$
values (with judicious use of $\lambda$-moves, nonmonotonic orders can also be
simulated), and in this sense the conjecture proposed by Suppes trivially
holds -- in fact it holds in the stronger form that for any language of type
$i<3$ we can also fit a PFSA (type 3 grammar) to the first $N$ terms of a
distribution to any required precision.  A more interesting question would be
to look at the intrinsic complexity of the parameter space: how many
parameters do we really need to describe the first $N$ length observations. If
the probabilities are $p_1, \ldots , p_r$, we expect $Np_r$ copies of $a^r$ so
the last $r$ to show up in the sample will be $p_r \approx 1/N$.  In the
regular case, $p_r$ is asymptotically $p^r$ for some $p<1$, so we expect
$r\log p \approx -\log N$ or, since $\log p$ is a constant (negative) factor,
$r \approx \log N$. In other words, for corpora with $10^{10}$ strings, a
realistic task is to fit a PFSA with about 23 states $(2*23^2 \approx 10^3$
free parameters) to approximate its length distribution by regular means.

\smallskip
\noindent
{\bf Exercise 5.8$^\dagger$} Obtain two gigaword or larger corpora from the
Linguistic Data Consortium or by crawling the web, and parse them into
sentences along the lines of \newcite{Mikheev:2002}. What is the length
distribution of the first corpus? How many parameters are needed for a PFSA
that provides a good fit? How well does this PFSA describe the length
distribution of the second corpus? 

\smallskip
To get a finer picture than what can be provided by the length distribution,
and also to extend the picture from the regular to the context-free case, let
us briefly consider $t$-letter alphabets, where $t$ is the size of the
category system, by mapping each observed sequence of words on the sequence of
corresponding preterminals.  (As a practical matter, for corpora of the size
discussed here, disambiguating the category can no longer be done manually:
instead of a perfect data set, we have one that may already carry considerable
POS tagging error.) At first, we will ignore word order, and replace strings
over a $t$-letter alphabet by {\bf count vectors} of dimension $t$ whose
$j$th component $c_j$ counts how often the $j$th symbol appeared in the
string. \index{count vector|textbf} A classic theorem of Parikh (see e.g.
Salomaa 1973 Ch. II/7) asserts that for every CFL there is a regular language
with the same count vectors.  However, this theorem does not smoothly extend to
weighted languages where the weight is given by multiplicity (number of
derivations):

\smallskip
\noindent
{\bf Example 5.5.2} Binary trees. The grammar $S \rightarrow SS|a$ generates
all sequences $a^k$ as many times as there are binary trees with $k$ leaves,
i.e. ${{2n}\choose{n}} \frac{1}{n+1}$ times. 

\smallskip\noindent
If we assign probability $p$ to the branching and $q$ to the nonbranching
rule, the probability assigned by an individual derivation of $a^k$ will be
$p^{k-1}q^k$, and it follows from Stirling's formula that the total
probability (taken over all derivations) will also contain a term proportional
to $k^{-3/2}$. So log probabilities would show linearity for individual
derivations but not for the totality of derivations, and therefore by Theorem
5.5.2 the weighted CFL of binary trees cannot be generated by any PFSA. 
\index{Catalan numbers} This is worth stating as the following theorem.

\smallskip
\noindent
{\bf Theorem 5.5.3} PCFGs generate more languages than PFSAs.

\smallskip
\noindent
{\bf Example 5.5.3} Bernoulli languages. These are generated by finite
automata of a very reduced sort: only one accepting state, the initial state,
which has outgoing arcs looping back to itself for each letter $s_i$ of the
alphabet, with the transition assigned log probability $q_i$. To keep the sum
of assigned probabilities convergent, it is necessary only to add a sink state
with a lambda-transition leading to it with any small positive probability $z$
(for convenience, we will use $z=0.5$). In a Bernoulli language, the log
probability of a count vector $\vec{l}=(l_1, \ldots, l_t)$ will be given by the
multinomial formula

\begin{equation}
\log {{l_1+\ldots +l_t}\choose{l_1,\ldots ,l_t}} +(\vec{q},\vec{l})
+\log(1/2)\sum_{i=1}^r l_i
\end{equation}

\noindent
Here the multinomial coefficients again come from the multiple orders in which
different strings with the same count vector could be obtained. If we are
interested in probabilities assigned to strings (with a fixed order of
letters), the first term of (5.27) disappears and only linear terms, namely the
scalar product $(\vec{q},\vec{l})$ of $\vec{q}$ and $\vec{l}$, and the length
normalization term, remain.

\smallskip
\noindent
{\bf Theorem 5.5.4} PCFGs and PCSGs over a one-letter alphabet do not generate
all weighted one-letter languages. 

\smallskip
\noindent
{\bf Proof} 
Define the weights as a set with infinite transcendence degree over ${\Bbb
Q}$.  (That such sets exist follows from the fundamental theorem of algebra
and from the existence of irreducible polynomials of arbitrary degree.  If
$t_1, t_2, t_3, \ldots$ is such a set, so will be $s_1, s_2, s_3, \ldots$
where $s_i=|t_i|/(1+|t_i|)2^i$, and the latter will also sum to $\leq 1$). Now
consider the generating functions which are defined by taking the nonterminals
as unknowns and the terminal $a$ as a variable in the manner of Chomsky and
Sch\"utzenberger (1963), except using the probabilities assigned to the rules
as weights. For example the grammar $S \rightarrow SS | a$ of binary trees
used in Example~5.5.2 yields the functional equation $S=pS^2+qa$.  In the
general case, solving the set of equations for the generating function
associated to the start symbol $S$ is very hard, but \nocite{Chomsky:1963}
over a one-letter alphabet the only variable introduced by CF or CS rules will
of necessity commute with all probabilistically weighted polynomials in the
same variable. Since all defining equations are polynomial, the output
probabilities are algebraically dependent on the rule probability parameters.
Since a CF or CS rule system with $n$ rules can generate at most $n$
algebraically independent values, it follows that $s_1, s_2, s_3, \ldots,
s_{n+1}$ cannot all be obtained from the CFG or CSG in question. $\qed$

These results may leave the reader with some lingering dissatisfaction on two
counts. First, what about Type 0 grammars? Second, there is still the Suppes
objection: what does all of this have to do with transcendence degree? With
Theorem 5.5.2 at hand, we are in a better position to answer these questions.

For Turing machines, an important reduction was presented in de Leeuw et al.
(1956), showing\nocite{Leeuw:1956} that a Turing machine with access to a
random number generator that produces 1s and 0s with some fixed probability
$p$ is equivalent to a standard TM without random components as long as $p$
itself is computable. Any string of 0s and 1s can be uniquely mapped on a
language over a one-letter alphabet: we set $a_i \in L$ if the $i$th digit was
1 and $a_i \not\in L$ if it was 0.  If the string was generated by a random
number generator with a fixed probability $p$ for 1s, the density of the
associated language will be $p$.  It is easy to program a Turing machine that
outputs a string (or language) with no density (see Section~5.4.1 above), but
it is impossible to program a TM that outputs (or accepts) a language with
noncomputable density. In other words, the key issue is not the randomness of
the machinery but rather the complexity of the real numbers that express the
probabilities.

At the bottom of the complexity hierarchy, we find the rationals: every
rational number between zero and 1 (and only these) can be the density of an
unweighted language over a one-letter alphabet generated/accepted by some
FSA. Adding weights $p_1, \ldots, p_k$ to an FS grammar or automaton
accomplishes very little since we will of necessity remain in their rational
closure. However, real numbers are powerful carriers of information: as a
moment of thought will show, all algorithms can be encoded in a single real
number. When we add just one noncomputable $p$ to the weight structure, we
have already stepped out of the TM domain.

At the next level of the hierarchy, we find the algebraic numbers. These
correspond to polynomials and thus, via generating functions, to context-free
grammars and languages. While the relationship is not very visible over the
one-letter alphabet because CFLs have letter-equivalent FSLs, and one needs
to consider the structure to see the difference, algebraic numbers (and only
these) arise naturally at every stage of the analysis of unweighted CFGs --
for the weighted case, the same caveat about introducing arbitrary reals
applies. 

As for Suppes' objection, all algebraic numbers, and some transcendental ones,
are computable. By Liouville's theorem, algebraic numbers of degree $n$ can
only be approximated by rationals $p/q$ to order $q^{-n}$ -- if a number can
be approximated at a faster rate, it must be transcendental. (Of course, it may
still be transcendental even if it has no faster approximation.) Arguments
based on transcendence can be thought of as convenient shorthand for arguments
based on order of growth. The analogy is loose, and we shall not endeavor to
make it more precise here, but Theorem 5.5.2 is a clear instance of replacing
the heavy machinery of transcendence by a more pedestrian reckoning of growth
(in our case, exponential decay).

\smallskip
\noindent
{\bf Exercise 5.9$^*$} Prove Theorem 5.5.4 by direct appeal to order of growth.

\subsection{Hidden Markov models}

A striking consequence of Zipf's law (see Section~4.4) is that in the same
syntactic category we can find words of radically different empirical
frequencies.  Indeed, by Corollary 4.4.1, vocabulary is infinite, and as there
are only finitely many strict lexical categories (see Section~5.1.1), by the
pigeonhole principle there will be at least one category with infinitely many
words, and in such categories, arbitrarily large differences in the log
frequencies of the high- and the low-frequency items must be present. For
example, frequent adjectives such as {\it red} occur over a million times more
often than rarer ones such as {\it dandruffy} in the gigaword corpora in
common use today, and as the corpus size grows we will find even larger
discrepancies.

Here we begin introducing hidden Markov models (HMMs) by using the POS tagging
problem as our example. Formally, a {\bf discrete hidden Markov
  model}\index{hidden Markov model, HMM|textbf} is composed of a finite set
$S$ of {\it hidden states} (in the example, there is one state for each POS
tag), a {\it transition model} that assigns a probability $t_{ij}$ to the
transition from state $s_i$ to state $s_j$ and an {\it emission model} that
assigns, for each state $s$ and for each {\it output symbol} $\vec{w}$, a
probability $E_s(w)$. A {\bf continuous} HMM does not have a discrete set of
output symbols, but rather a continuous set of (multidimensional) output
values $o$, and the emission model is simply a continuous probability
distribution $E_s(o)$ for each hidden state $s$ (see also Section~8.2).  The
{\it hidden} aspect of the model comes from the fact that the same output,
e.g. the word {\it run}, can be emitted from more than one state. In our
example, there is a significant nonzero probability that $s_{\textit{verb}}$
will emit it but also a (smaller, but not zero) probability that it is emitted
by the $s_{\textit{noun}}$ state.  In other words, from seeing the output {\it
  run} we cannot determine which state the model is in; this information is
(at least to some extent) hidden.

Since there are only finitely many states (in the simple POS tagging example
considered here, generally about 200), even the smallest nonzero transition
probability is expected to be rather large, on the order of
$10^{-3}$--$10^{-4}$. Smaller numbers can be used of course, but in any given
model there are only finitely many transitions, so there will be a smallest
nonzero transition probability. This is in sharp contrast to emission
probabilities, where an infinite set of symbols associated to a state will of
necessity give rise to arbitrarily small nonzero values. In POS models,
emission values going down to $10^{-9}$ are quite common, and in the
continuous case (which plays a key role in speech and handwriting recognition;
see Chapters~8 and 9), emission probabilities routinely underflow the 64 bit
floating-point range (which bottoms out at around $10^{-300}$) so that log
probabilities are used instead. Altogether, it is not just the obvious
Markovian aspect that makes HMMs so suitable for language modeling tasks, but
the clear segregation of the transition and emission models also plays a
critical role.

Although typically used for recognition tasks, HMMs are best viewed as
generation devices. In a single {\it run},\index{hidden Markov model, HMM!run}
the model starts at a designated state $s_0$ and, governed by the
transition probabilities $t_{ij}$, moves through a set of hidden states
$s_{i_1}, s_{i_2}, \ldots, s_{i_n}$, emitting an output symbol $w_1, w_2,
\ldots, w_r$ at each state with probability governed by $E_{s_{i_j}}$. We say
that the probability the model assigns to the output string $w_1w_2\ldots w_r$
in a given run is the product of the transition and emission probabilities
over the run and that the probability assigned to the string {\it by the
  model} is the sum of the probabilities assigned in all runs that output
$w_1w_2\ldots w_r$.

HMMs are, at least in the discrete case, clearly finitistic devices that
assign probabilities to strings. Yet they differ from WFSAs and WFSTs in some
ways even if we ignore the possibility of emitting an infinite variety of
symbols. First, WFSAs and WFSTs emit on transitions, while HMMs emit at
states. As with Mealy and Moore machines, this distinction carries no
theoretical significance but implies significant practical differences in the
design of software libraries. Second, older definitions of HMMs may use an
{\it initial probability distribution} to decide in which state the model
starts in a given run. Here, as in the case of WFSTs above, we use a
designated (silent) start state instead -- this again entails no loss of
generality. The main novelty that HMMs bring is thus the segregation of
emissions from transitions: as we shall see, this can be thought of as a form
of {\it parameter tying.}

While in theory it would be possible to abandon the hidden aspect of HMMs and
replace each (state, emission) pair by a dedicated state with only one
(weighted but deterministic) nonsilent emission possibility, this would
complicate the model enormously. For an output inventory of $n$ symbols and a
state space of $k$ states, we would now need $nk$ states, with $(nk)^2$
parameters (transition probabilities) to learn, as opposed to the $k^2+nk$
required by the HMM. Since $k$ is a fixed (and rather small) number, as $n$
grows to realistic values, the system would very soon leave the realm of
practicality. Much of the early criticism leveled at Markov models (Miller and
Chomsky 1963) grew \nocite{Chomsky:1963} \nocite{Miller:1963} out of this
issue. The surprising economy provided by collecting all emissions from a
given underlying state was discovered only later, as attention moved from
ordinary `open' Markov chains to HMMs.

Hiding the states from direct inspection was a move very similar to the one
taken by Chomsky (1957), who introduced {\it underlying} structure different
from the observable {\it surface} structure and used the former to
characterize the latter. In the simple POS tagging model taken as our driving
example here, the mental state of the speaker is characterized as a succession
of POS tags, and the observables are the words. In a speech recognition model,
the underlying states are the phonemes, and the surface forms are acoustic
signals. To recover the underlying states, we need to perform analysis by
synthesis, i.e. to compute the most likely state sequence that could have
emitted the words in the observed sequence. This is done by the {\it Viterbi
  algorithm},\index{Viterbi algorithm} which is based on a data structure
called a trellis: a matrix $M$ with $k$ rows and as many columns as there are
items in the surface string.

The central idea is that to keep track of the sum of the probabilities
generated by every possible run, it is best to group together those runs that
end in the same state. To compute the probability of a string $w_1w_2\ldots
w_r$ in a given run requires $2r$ product operations, and as there are $k^r$
possible runs to consider, a naive algorithm would use $2rk^r$ multiplications
and $k^r$ additions. Using the trellis, we first populate the first column of
$M$ according to the initial transition probabilities times the emission
probabilities from each state -- this takes $2k$ product operations. Once 
column $i$ has been populated, the $j$th element of column $i+1$ is
constructed as $\sum_{l=1}^k t_{lj}M_{li}E_j(w_{i+1})$, which requires $2k$
product and $k$ sum operations per entry, for a total of $2k^2$ products and
$k^2$ sums per column. Over a run of length $r$, this is still only $2rk^2$
products and $rk^2$ sums; i.e. a number of operations {\it linear} in $r$ for
any fixed $k$. Once the trellis is filled, we simply pick the maxima in 
each column to obtain our estimate of the maximum likelihood state sequence.
The number of compare operations required is again linear in $r$. 

A key aspect of hidden Markov modeling is that the HMMs are {\it trainable}:
rather then setting the transition and the emission model parameters manually,
given a set of {\it truthed} data\index{truth!data label} where both the
underlying state and the outputs are known, there exist highly efficient
algorithms to set the model parameters so that the posterior probability of
the truthed observations is maximized. We return to this matter in Chapter~8
-- here we conclude by proving our Theorem 5.4.5 that weighted languages
generated by HMMs have zero density. 

\smallskip
\noindent
{\bf Proof} We use the same trellis structure as above but simplify the
calculation by exploiting the fact that for each state both the emission
probabilities and the outgoing transition probabilities sum to one. Starting
with the $0$th state and summing over all possible outputs $\vec{w}$, we see
that $\sum_{i=1}^k \sum_{j=1}^n P(w_j|s_i)t_{0i}=\sum_{i=1}^k t_{0i}=1$;
i.e. that the total weight assigned to strings of length 1 by the HMM is 1 (or
less if silent states are permitted). Similarly, in filling out the second
column, by keeping the first output $w_1$ fixed, the probability mass assigned
to all strings of length two that begin with $w_1$ is at most 1, and therefore
the total weight of strings of length two is again at most 1. This means that
the coefficients $r_i$ of the generating function $d(z)$ are all $\leq 1$, and
therefore $d(z)\leq 1/(1-z)$. By (5.20), we have $\rho(L) = \lim_{z
  \rightarrow 1} (1-z)d(z/n) \leq \lim_{z \rightarrow 1} (1-z)/(1-(z/n))=
(1-1)/(1-(1/n))=0$.

\smallskip
\noindent
{\bf Discussion} The same result is obtained by noting the elementary fact
that the HMM assigns a weight of at most $r$ to strings of length at most $r$,
and there are $n+n^2+\ldots+n^r$ such strings. This is not a bad result
since natural languages, conceived as regular stringsets, also have
density zero by Theorem 5.4.3.

Using the finer measures introduced for zero-density languages in
Section~5.4.2 above, the saturation $\sigma$ of the weighted language
generated by an HMM is at most $1/n$, where $n$ is the size of the output
alphabet. Note that, for any corpus, the generating function is a polynomial,
which has saturation 0 (polynomials converge on the whole plane). To model a
corpus of size $N$, we need, by (4.7), about $n=cN^{1/B}$ words. As $N$ tends
to infinity, $1/n$ will tend to zero.

The Bernoulli density $\delta$ of HMM languages is $\leq 1$ by the first step
of the inductive proof of Theorem 5.4.5 given above. As for natural languages,
the Bernoulli density is hard to estimate, but clearly not every sentence is a
prefix, so we have $\delta(L) < 1$.

Finally, for combinatorial density $\kappa$ in HMM languages, we have $\kappa
\leq \zeta{n}$, where $n$ is the size of the vocabulary (output alphabet),
which again tends to 0 as $n$ tends to infinity. Whether this is the right
prediction for natural languages is hard to say. As long as we treat
vocabulary size as finite (a rather dubious assumption in light of Corollary
4.4.1), combinatorial density will be positive since at least the words that
are suitable for isolated utterances such as {\it Ouch!}  or {\it Yes} will
contribute to $\kappa$. Since the Zipf constant $B$ is close to 1, the
contribution of two-word or longer utterances to $\kappa$ will vanish as we
increase the corpus size, but it is possible that the probability of one-word
utterances remains above some positive lower bound. We are far from being able
to settle the issue: the total probability mass of one-word utterances is
clearly below 0.001, while the best language models we have today have a total
of over- and underestimation errors on the order of 0.2. To put the issue in
perspective, note that for a fixed HMM with fixed vocabulary it is quite
trivial to adjust the probabilities of emitting a sentence boundary so that
interjections and similar material are modeled as one-word sentences with a
positive probability -- in fact, we expect our models to automatically train to
such values without making any manual adjustments toward this goal.


\section{External evidence}

Just as rational numbers provide a sufficient foundation for all numerical
work in computer science, regular languages with rational weights are
sufficient for all empirical work in linguistics (though in practice
irrational numbers in the form of log probabilities are used quite often).
Our interest in models of higher intrinsic complexity comes entirely from
their capability to assign relevant structures to widely attested
constructions, not from their improved weak generative capacity, which, at any
rate, comes into play only on increasingly marginal examples.  In the standard
unweighted setup there are already reasons to prefer the rational models over
the more complex ones, and these are summarized briefly here.

One key issue in selecting the right model is the amount of resources it takes
to compute the structure it assigns. Without some special effort, even
innocent-looking grammatical frameworks such as CG require time exponential in
the size of input: as we discussed above, if the average ambiguity of a word
is $d$ (in practice $d \approx 1.5)$, brute force algorithms require the
inspection of $d^n$ combinations to find all structures associated to a string
of length $n$. This sets the outer limits of weak generative capacity at or
below mild context-sensitivity. Such formalisms are by definition polynomially
parsable (see Kracht 2003 Ch. 5 for further discussion). In particular,
polynomial (typically $n^6$) parsing algorithms are known for all widely used
grammar formalisms such as CCGs or TAGs. For the average case, requiring a
polynomial bound is not very ambitious, inasmuch as humans obviously perform
the task in real time on ordinary sentences, but we have little knowledge of
how the human parser is implemented. Also, by carefully constructed {\it
garden path} sentences such as\index{garden path} 

\begin{equation}
\text{Fat people eat accumulates} 
\end{equation}

\smallskip\noindent it is quite possible to trip up the human parser, so a
worst-case polynomial bound may still make sense.

Another key issue is learnability: it is clear that humans can learn any human
language they are exposed to at an early age. Ideally, we are looking for a
class of grammars such that the correct one can be selected based on exposure
to positive examples only. This puts much more severe constraints on the weak
generative capacity of the system. For example, context-free grammars cannot
be learned this way \cite{Gold:1967}. The best lower bound on this problem is
perhaps\nocite{Kanazawa:1996} Kanazawa's (1996) result that CGs in which the
degree of ambiguity for preterminals is limited to some fixed $k$ are still
identifiable in the limit from positive data. For probabilistic CFGs, the
situation is better: algorithms that converge to a probabilistic CFL are
widely used. Since language learning takes several years in humans, the known
complexity bounds (e.g.  the fact that learning regular expressions is NP-hard;
see\nocite{Angluin:1980}\nocite{Angluin:1982} Angluin 1980, 1982) are
harder to interpret as imposing external constraints on the class of human
grammars.

The structuralist program of defining a {\it discovery
  procedure}\index{discovery procedure} whereby the grammarian (as opposed to
the child learning the language) can systematically extract a grammar by
selectively testing for distributional similarities has been at least
partially realized in the work of \nocite{Clark:2005} Clark and Eyraud (2005),
who prove that it is possible to discover CFGs for those CFLs that are {\bf
  substitutable} in the sense that positive context sharing between any two
$\gamma$ and $\delta$ (the existence of $\alpha\gamma\beta, \alpha\delta\beta
\in L$) implies full distributional equivalence (5.5). To the extent that we
have already seen examples such as {\it slept} and {\it opened Parliament}
that share a positive context without being fully distributionally equivalent
(cf. {\it *a bed rarely opened Parliament in}), one may rush to the conclusion
that the Kanazawa result cited above is more readily applicable to natural
language, especially as we have not seen examples of arbitrarily ambiguous
words. But in modern versions of CG arbitrary {\bf type lifting}\index{type lifting} (automatic assignment of higher types) is often present, and a
realistic theory of {\sl grammar induction} remains one of the central
unsolved problems of mathematical linguistics.

Part of the interest in the regular domain comes from the fact that there
grammar induction is feasible, and part comes from the fact that once we add
memory limitations, theories with richer structural descriptions such as CFGs
or LTAGs also reduce to finite automata. As we shall see in Section~6.3.2,
memory limitations, whether imposed externally on a richer theory or inherent
in the system as assumed here, can be used in a direct fashion to characterize
the syntactic congruence by means of investigating the number of words that
are required to complete a partial sentence. One striking observation is that
natural language syntax seems to be enjoying the following {\bf noncounting
  property} $Q_4$.\index{language!noncounting|textbf}\index{noncounting|textbf}

\begin{equation}
ax^4b \in L \Leftrightarrow ax^5b \in L
\end{equation}

\noindent In general, we say that if a language satisfies $ax^kb \in L
\Leftrightarrow ax^{k+1}b \in L$ it is noncounting with {\it threshold} $k$
(has property $Q_k$). Obviously the complement of a noncounting language is
also noncounting, with the same threshold.  In (5.29) the number 4 cannot be
further reduced because of agreement phenomena. Some languages, such as
Gunwinggu [GUP], distinguish singular, dual, trial, and plural, so $x^3$ and
$x^4$ may require different $a$ or $b$.  However, to make the performance
limitations truly felt, it can be increased: we could say e.g.  that no
construction distinguishes between $x^{13}$ and $x^{14}$. As an example,
consider expressions such as {\it missile, anti missile missile, anti anti
  missile missile missile} etc.  According to Carden (1983), these provide a
simple way to demonstrate that word formation is not finite state: to get from
the $n$-degree weapon from $n-1$, we need to prefix {\it anti} and suffix {\it
  missile}, which would yield the language {\it anti}$^n$ {\it
  missile}$^{n+1}$.  But in reality, the enterprise falls apart at around
three: people will accept {\it anti anti missile, anti anti missile missile},
and other forms about as well as those where the number or {\it anti}s and
{\it missile}s around the first {\it missile} is properly
balanced.\nocite{Carden:1983}

Observation (5.29) has far-reaching consequences for the structure of the
formal apparatus. For an $n$ element set $\Sigma$, the set of $\Sigma
\rightarrow \Sigma$ functions is a monoid, with the role of multiplication
played by function composition and the role of identity by the identity
function.  Subsets of this set are called {\bf transformation semigroups},
{\bf transformation monoids}, and {\bf transformation groups}, provided they
are closed under multiplication, contain the identity, and are also closed
under inverse, respectively.\index{transformation semigroup}\index{transformation monoid}\index{transformation group} A
classic theorem of Cayley asserts that any group is isomorphic to a
transformation group. The proof extends to monoids and semigroups as well.
Given an arbitrary semigroup (monoid) $S$, we can take its members to be the
set to be transformed and define the transformation effected by a given
semigroup (monoid) element $s$ (also called the {\bf
  action}\index{action|textbf} of $s$ on $S$) as the result of multiplying
(from the right) by $s$. Unlike in groups, where each transformation is a
permutation of the elements, in semigroups multiplying two different elements
$r$ and $r'$ by $s$ may lead to the same result $rs=r's$.  The number of
elements in the image of $S$ under the transformation is called the {\bf
  rank}\index{rank|textbf} of the transformation -- this equals $|S|$ only if
the transformation is injective.

The importance of transformation semigroups in linguistics comes from the fact
that each element of the alphabet $\Sigma$ induces a transformation on the
syntactic congruence. Since automata states are in one-to-one relation with
classes of the right congruence, it is natural to identify the semigroup
generators with the transformations of the state space of the automaton
induced by the letters of the alphabet. If all elements of a semigroup $S$
except the identity transform the base set into a singleton set, $S$ is
called\index{reset semigroup|textbf} a {\bf reset semigroup (monoid)}. To see
the utility of this notion, recall that every word (more precisely, the
lexical category of the word) is a member of the syntactic monoid of the
language and acts as a transformation on it. By picking several words at
random, one is extremely likely to introduce unrecoverable ungrammaticality,
i.e. the combined action of these words is to reset the monoid to a sink
state. This is not to say that every word (lexical category) performs a reset,
but they all narrow down what can come next. 

The fundamental theorem of semigroup decomposition (Krohn and Rhodes 1965)
asserts that every semigroup is a divisor of some semigroup that can be built
up as a {\it wreath product} of those semigroups in which every element has
extreme low rank (reset semigroups) with those where every element has extreme
high rank, namely groups. Fortunately, we do not need to consider Krohn-Rhodes
theory in its full generality because Sch\"{u}tzenberger (1965) proved that
the transformation semigroup associated to a language is {\bf group-free}
\index{group-free} (has no nontrivial subgroups) if and only if the language
is noncounting. Combining these results gives the following.
\nocite{Krohn:1965} \nocite{Schu2tzenberge:1965}
%\nocite{Maler:1994} 

\medskip
\noindent
{\bf Theorem 5.6.1} (McNaughton and Papert 1968) The semiautomaton associated
with a noncounting language is a divisor of a cascade product of reset
monoids $U_2$.\nocite{McNaughton:1968}

\smallskip\noindent {\bf Discussion} Cascade products $A \circ B$, defined for
semiautomata, and the analogous wreath products $A \wr B$, which are defined
directly on semigroups, are somewhat difficult to grasp because the role of
the two terms $A$ and $B$ is asymmetrical: once $A$ is given, $B$ must take a
certain form for their cascade or wreath product to be defined. This is
unusual, but not unheard of. For example, if $A$ were an $n$ row by $k$
column matrix, the only $B$s that could participate in a matrix product $AB$
would be those with $k$ rows. Specifically, if $A$ has state space $Q_A$ and
input alphabet $\Sigma$, $B$ must have state space $Q_B$ and input alphabet
$Q_A \times \Sigma$ -- the construction is called a {\it cascade} because the
higher automaton $A$ can transmit its state as part of the input to $B$, but
the lower automaton $B$ does not have the means to inform $A$ of its own state.

One way of thinking about this construction is to reproduce the lower
automaton in as many copies as there are states in $Q_A$ and associate one
copy to each state of $Q_A$. During this process, the state space $Q_B$ is
preserved exactly, but the transition function of the copies is made dependent
on the state to which they are associated. We can take the $A$ to be the prime
minister and the $Q_B$ clones as the cabinet. In each state, the prime
minister listens only to the cabinet member associated to that state: the
higher automaton $Q_A$ takes transition according to its own transition
function, and the $Q_B$ copies must all fall in line and move to the same
state that was dictated by the transition function of the copy that was
distinguished before the transition. More formally, we have the following
definition.

\medskip\noindent {\bf Definition 5.6.1} Given two semiautomata $A$ and $B$
with state spaces $Q_A$ and $Q_B$, input alphabets $\Sigma$ and $Q_A \times
\Sigma$, and transition functions $\delta_A: Q_A \times \Sigma \rightarrow
Q_A$ and $\delta_B: Q_B \times Q_A \times \Sigma \rightarrow Q_B$, their {\bf
  cascade product}\index{cascade product|textbf} $A \circ B$ has as its state
space the direct product $Q_A \times Q_B$ and has the input alphabet $\Sigma$
as its input alphabet. From the product state $(q_1,q_2)$, upon input of
$\sigma \in \Sigma$ the cascade product automaton will transition to
$(\delta_A(q_1,\sigma),\delta_B(q_2,q_1,\sigma))$. By definition, cascade
products $A_1 \circ A_2 \circ \ldots \circ A_n$ are associating $ ( (\ldots
(A_1 \circ A_2) \circ A_3) \ldots \circ A_n)$.

\smallskip\noindent The cardinal building block used in Theorem 5.6.1 is the
reset monoid $U_2$, which acts on two elements $u$ and $v$ as a transformation
semigroup and has only three elements: the transformation $C$ that maps
everything to $u$ (i.e. the constant function whose only value is $u$), the
transformation $D$ that maps everything to $v$, and the identity
transformation $I$. If we take $u$ and $v$ to be the open and closed parens $a$
and $b$ of the bounded counter of depth 2, it is easy to see that the
semigroup of the language associated to this automaton is exactly $U_2$. 

\smallskip\noindent {\bf Exercise 5.10} Consider the $k$-term cascade product
$U_2^k = U_2 \circ \ldots \circ U_2$ over a two-letter alphabet $\Sigma =
\{a,b\}$. What is the largest $n$ for which $U_2^k$ is a cover for the bounded
counter of depth $n$?

\smallskip\noindent From the linguistic standpoint, this is closest to a
single binary feature, which can take the value `$+$', the value `$-$', or be
left {\it underspecified}.\index{underspecification} Note that the fourth
possible action $P$, mapping $u$ on $v$ and $v$ on $u$, is missing from $U_2$.
Were we to add this (a mapping that would correspond to `negating' the value
of a binary feature), we would still not obtain a group, since the constant
functions $C$ and $D$ have no inverse, but we would destroy the group-free
property since $P$ is its own inverse and the subset $\{I,P\}$ would be a
group. In other words, Theorem 5.6.1 asserts that the syntax of those regular
languages that enjoy the noncounting property (5.29) can be handled by relying
on binary features alone, a result that goes some way toward explaining the
ubiquity of these features in linguistics.

To see this in more detail, we will first consider a toy language $T$ based on
the example {\it Which books did your friends say your parents thought your
  neighbor complained were/*was too expensive?} The transformational
description of sentences like this involves an underlying structure {\it Your
  friends say ... the books were too expensive} from which the NP {\it the
  books} gets moved to the front, so that the agreement in number between {\it
  books ... were expensive} and {\it book ... was expensive} requires no
further stipulation beyond the known rule of subject-predicate agreement that
obtains already in the simple sentences {\it The book was expensive} and {\it
  The books were expensive}. This is easily handled by extending rule (5.7) 
with an agreement feature: 

\begin {equation}
\mbox{S} \rightarrow \text{NP}\langle \alpha \text{PL}\rangle \text{ VP}\langle
\alpha \text{PL}\rangle
\end{equation} 

\noindent 
where $\alpha$ is a variable that can take the values `+' or `$-$'. In other
words, we replace (5.7) with a rule schema containing two rules: the
assumption (further discussed in Section~7.3) is that such schemas are no more
expensive in terms of the simplicity measure of grammars than the original
(5.7), unadorned with agreement features, would be. Here we assume similarly
abbreviated regular expressions: our language $T$ is defined as the union of
$T_{sg}$ given by {\it which N}$\langle -\text{PL}\rangle$ {\it did}
$\text{Z}^*$ VP$\langle -\text{PL}\rangle$ and $T_{pl}$ given by {\it which
  N}$\langle +\text{PL}\rangle$ {\it did} $\text{Z}^*$ VP$\langle
+\text{PL}\rangle$. Here $Z$ stands for sentences missing an object: {\it your
  friends say $\underline{\ \ }$, your parents thought $\underline{\ \ }$,
  your neighbor complained $\underline{\ \ }$}, and the Kleene $^*$ operator
means any number of these can intervene between the extracted subject {\it
  book(s)} and the predicate {\it was/were}. The regular expression {\it
  which} N {\it did} $\text{Z}^*$ VP, or what is the same, the automaton

\xyoption{curve}
\begin{equation}
\xymatrix{0\ar[r]^{\textit{which}} & 1\ar[r]^{\text{N}} &
2\ar[r]^{\textit{did}}\ar@(ur,ul)_{\text{Z}} & 3\ar[r]^{\text{VP}}& 4}
\end{equation}

\noindent
will accept the desired language {\it without} proper agreement. To get the
agreement right, we take the cascade product with the depth 2 bounded counter
whose transition is defined for all clones as the identity mapping for all
states and all symbols, except N$\langle +$PL$\rangle$ will move clone 1
from the initial (accepting) state to the other (warning) state, and for clone 
3 the input VP$\langle +$PL$\rangle$ will force a move back to the 
initial state. 

What this simple treatment of $T$ shows is that cascade multiplication with a
small reset automaton is sufficient to keep one bit of information around even
though an arbitrary amount of material may intervene before we make use of
this bit.  If we measure the complexity of the automaton by the number of
states it has, the cascade product is very expensive (multiplying with $U_2$
will double the size of the state space), but by the same general principle
that treats (5.7) and (5.30) as having equal or nearly equal cost, a more
proper measure of automaton complexity is the number of states in its cascade
decomposition, which increases only linearly in the number of bits kept
around.

\section{Further reading}

Distributional criteria for lexical categorization were advocated by
Bloomfield (1926, 1933) \nocite{Bloomfield:1926} \nocite{Bloomfield:1933} and
elaborated further by Bloch and Trager (1942) and Harris (1951). Definition
(1) is known as the {\it Myhill-Nerode equivalence} or the {\bf syntactic
  congruence} associated with a language $L$ as it was \newcite{Myhill:1957}
and \newcite{Nerode:1958} who proved, independently of one another, the key
theorem that this equivalence, extended to $\Sigma^*$, has only finitely many
classes iff $L$ is regular. \index{syntactic congruence|textbf}
\nocite{Bloch:1942} For a modern survey, see \newcite{Pin:1997}. For the use of
noncounting in practical grammar design, see Yli-Jyr\"{a} (2003,2005) and
Yli-Jyr\"{a} and Koskenniemi (2006).\nocite{Yli-Jyra2:2006}

The classical works on categorial grammars are \newcite{Ajdukiewicz:1935},
where the group is taken to be Abelian, \newcite{Bar-Hillel:1953}, and
\newcite{Lambek:1958}. Some of the interest in the area was lost when
Bar-Hillel, Gaifman, and Shamir (1960) proved the equivalence of CFGs to one
form of categorial grammar, but the field has largely revived owing to its
strong relation to semantics. The equivalence between CFGs and Lambek grammars
has been proven by Pentus (1997) -- a more accessible proof is given in
Chapter~3 of Kracht (2003), which presents categorial grammars in greater
depth.  \nocite{Pentus:1997} \nocite{Bar-Hillel:1960}

Although X-bar theory clearly originates with Harris (1951), the name itself is
from Chomsky (1970) and subsequent work, especially Jackendoff (1977). For a
more detailed discussion, see Kornai and Pullum (1990). A good survey of the
linguistic motivation for going beyond the CF domain is Baltin and Kroch
(1989).  \nocite{Baltin:1989} The equivalence, both weak and strong, between
alternative formulations of extended phrase structure and categorial systems
has been proven in a series of papers by Joshi and his students, of which we
single out here Vijay-Shanker et al. (1987) and Weir (1992).
\nocite{Kornai:1990} \nocite{Weir:1992} \nocite{Vijay-Shanker:1987}
\nocite{Moravcsik:1980}\nocite{Chomsky:1970}\nocite{Jackendoff:1977}

Moravcsik and Wirth (1980) presents the analysis of (5.8) and (5.9) in a
variety of syntactic frameworks. Some of these frameworks are still in use in
essentially unchanged format; in particular {\it tagmemics}, being the
standard theory at the Summer Institute of Linguistics is in wide use, and
covers an immense variety of languages, many of which have no analyses in any
other framework.  Other frameworks, such as {\it Montague grammar}, {\it
  relational grammar}, and {\it role and reference grammar}, have changed
mildly, but the reader interested in a quick overview can still get the basic
facts from the respective articles in the Moravcsik-Wirth volume. Yet others,
in particular mainstream transformational theory, have changed so radically
that the presentation of {\it trace theory} in that volume is of historical
interest only. The reader interested in more current developments should
consult \newcite{Chomsky:1995} and subsequent work.\nocite{Moravcsik:1980}

The first proposal within generative linguistics to use case as the driving
mechanism for syntax was by Fillmore (1968, 1977). The standard introduction
to ergativity is\newcite{Dixon:1994}. Dependency grammars
\nocite{Fillmore:1968}\nocite{Fillmore:1977}\nocite{Tesnie1re:1959} are due to
Tesni\`ere (1959). Other modern formulations include \newcite{Sgall:1986},
\newcite{Melcuk:1988}, \newcite{Hudson:1990}, and \newcite{McCord:1990}.
Again, an important early result that served to deflect interest from the area
was by \newcite{Gaifman:1965}, who proved the equivalence of one formulation
of dependency grammar to CFGs, and again the area revived largely because of
its strong connections to key notions of grammar. A variety of dependency and
valency models are surveyed in \newcite{Somers:1987} and
\nocite{Tapanainen:1997} Tapanainen and J\"arvinen (1997). A modern survey of
argument linking theories is Levin and Rappaport (2005).\nocite{Levin:2005}

For linearization of Hayes-Gaifman-style DGs, see Yli-Jyr\"{a} (2005) and for
linking a different formulation of DGs to mildly context sensitive grammars
and languages see \newcite{Kuhlmann:2005}. The MIL formalization of DG is from
Kornai (1987), except there it was given as an equational system in the sense
of \nocite{Curry:1958} Curry and Feys (1958 Ch. 1E) rather than stated in the
more widely used language of universal algebra. \nocite{Kornai:1987} The ID/LP
analysis of the Dutch crossed dependency is due to Ojeda (1988). The valence
(clause) reduction analysis of {\it hate to smoke} and similar constructions
originates with \newcite{Aissen:1983}; see also Dowty (1985) and Jacobson
(1990).\nocite{Ojeda:1988}\nocite{Dowty:1985}\nocite{Jacobson:1990}\nocite{Yli-Jyra2:2005}

The method of mapping linguistic expressions to algebraic structures in order
to capture some of their significant properties is not at all restricted to
the simple examples of checking agreement or slot/filler relations discussed
here. The modern theory of using types as a means of checking correctness
begins with \newcite{Henkin:1950}. In {\it type-logical
  grammar}\index{type-logical grammar} we can use proofnets to check the
syntactic correctness of strings based on their types (Carpenter and Morrill
2005). Another theory of note, {\it pregroup grammar},\index{pregroup grammar}
uses left and right adjoints to avoid the issues of group elements commuting
with their inverses.  Lambek (2004) defines pregroups as partially ordered
monoids where the partial order is compatible with the monoid multiplication,
and multiplication with every element has a left and a right adjoint. His
example is the set of ${\Bbb Z} \rightarrow {\Bbb Z}$ functions that are
unbounded in both directions, with function composition as the monoid
operation. The relationship of pregroups to categorial grammar and (bi)linear
logic is discussed in Casadio et al. (2005).
\nocite{Lambek:2004}\nocite{Casadio:2005}\nocite{Carpenter:2005}

In Minsky's (1975) theory, the slots are endowed with ranges (used for error
checking) and often with explicit algorithms that compute the slot value or
update other values on an as-needed basis, but there are no clear natural
language phenomena that would serve to motivate them. Altogether, the
relationship between AI/KR and syntax is far more tenuous today than it was in
the 1970s: on the one hand, AI/KR has largely given up on syntax as too hard,
and on the other, its center of gravity has moved to machine learning, a field
more immediately concerned with linguistic pattern recognition in speech,
handwriting, and machine print recognition (see Chapters 8 and 9) than in
syntax proper. For default inheritance and defeasible reasoning, see
\newcite{Ginsberg:1986a}.  The best introduction to thematic roles remains
\newcite{Dowty:1991}.  \nocite{Minsky:1975}

The complex and often acrimonious discussion that followed Chomsky's (1957)
introduction of arbitrary depth center-embedded sentences led to important
methodological advances, in particular the introduction of the distinction
between {\it competence} and {\it performance} \cite{Chomsky:1965} discussed
in Section~3.2. For the debate surrounding generative semantics, see
\newcite{Newmeyer:1980}, \nocite{Huck:1995}\newcite{Harris:1995}, and Huck
and Goldsmith (1995).

For the linguistic use of arbitrary semirings for weighted languages,
automata, and transducers see Mohri et al (1996), \newcite{Eisner:2001}.  The
interpretation of the weights as degrees of\nocite{Mohri:1996} grammaticality
is due to \newcite{Chomsky:1967}.  The density of languages was first
discussed in Berstel (1973), who considered pairs of languages $L$ and
mappings $f$ from $\Sigma^*$ to ${\Bbb R}^+$. He also considered the ratio of
the summed weights (summed for strings of length $\leq n$ in $L$ in the
numerator and for all strings in $\Sigma^n$ in the denominator), while our
definition (see Kornai 1998)\nocite{Kornai:1998} uses the differences,
segregated by sign.  We believe that our choice better reflects the practice
of computational language modeling, since in these models both underestimation
and overestimation errors are present, and their overall effects are seldom
determined in the limit (incorrect estimates for high-frequency strings are
far more important than incorrect estimates for low-frequency strings).  For
natural numbers represented in binary, Minsky and Papert (1966) use density
arguments to prove that e.g. the primes in base two are not regular; see also
Cobham (1969) and Chapter~5 of Eilenberg (1974).\nocite{Minsky:1966}
\nocite{Cobham:1969}\nocite{Eilenberg:1974}

The early work on probabilistic FS and CF grammars is summarized in Levelt
(1974 Ch.~3).\nocite{Levelt:1974} Outside sociolinguistics, probabilistic
theories of grammar had little influence because Chomsky (1957) put forward
the influential argument that {\it colorless green ideas sleep furiously} is
grammatical and {\it *furiously sleep ideas green colorless} is ungrammatical,
yet both have frequency zero, so probability has no traction in the domain of
grammar (for a modern assessment of this argumentation, see Pereira
2000).\nocite{Pereira:2000} Within sociolinguistics, however, the variability
of the data is so overwhelming that there was never a question of abandoning
the probabilistic method. The standard introduction to variable rules is
Cedergren and Sankoff (1974). The free choice between the additive and the
multiplicative models has been cogently criticized by Kay and McDaniel (1979).
\nocite{Kay:1979} Logistic models were introduced to sociolinguistics by
Rousseau and Sankoff (1978).\nocite{Rousseau:1978} HMMs as formal models were
introduced by Baum and Petrie (1966) and Baum et al. (1970),\nocite{Baum:1966}
\nocite{Baum:1970} and first put to significant use in speech recognition by
Baker, Jelinek, and their colleagues at IBM. The standard tutorial
introduction is Rabiner (1989);\nocite{Rabiner:1989} for a more extensive and
deeper treatment, see \newcite{Jelinek:1997}.  For a detailed discussion of
probabilistic CFGs, see \newcite{Charniak:1993}.

The use of formal power series in noncommuting variables to describe ambiguity
in CFLs and the concomitant use of semirings originates with
\nocite{Schu2tzenberge:1960} Sch\"{u}tzenberger (1960).  Early developments
are summarized in \nocite{Chomsky:1963} Chomsky and Sch\"{u}tzenberger (1963).
For the rational case, see Eilenberg (1974), and for general development of
the semiring-oriented approach, see Kuich and Salomaa (1986).
\nocite{Kuich:1986} For further discussion of Parikh mappings in the weighted
case, see \newcite{Petre:1999}. For initial estimates of state space size, see
Kornai (1985) and Kornai and Tuza (1992) -- the estimate we shall derive in
Section~6.3.2 also takes ambiguity into account. \nocite{Kornai:1985}
\nocite{Kornai:1992}

For a simple proof of Theorem 5.6.1 see Section 7.12 of
\newcite{Ginzburg:1968}, written by Albert Meyer -- the same ideas are
presented in \newcite{Meyer:1969}.  The classical study of this area is
McNaughton and Papert (1971), which discusses both the well-known equivalence
of noncounting and star height zero and its relation to temporal logic, also
discussed in Maler and Pnueli (1994). For a modern treatment, see
\newcite{Thomas:2003}.  \nocite{Maler:1994}\nocite{Bergeron:2004} The idea of
a prime minister and a cabinet is based on Bergeron and Hamel (2004), where
the important relation between bit-vector operations and noncounting languages
is explored. For closure of noncounting languages under a relaxed alternative
of Karttunen's (1995) replacement operator, see Yli-Jyr\"{a} and Koskenniemi
(2006).\nocite{Karttunen:1995}\nocite{Yli-Jyra2:2006}\nocite{McNaughton:1971}

A significant source of external evidence, not discussed in the main text as
there is no mathematical treatment in existence, is language pathology. For
the pioneering effort in this direction, see \newcite{Jakobson:1941}.  Chomsky
(1965) has distinguished the internal combinatorical evidence from the
broader external considerations discussed here under the heading of {\it
  descriptive} vs. {\it explanatory adequacy.} \index{descriptive adequacy}
\index{explanatory adequacy} The pivotal study linking the extensive
literature of child language acquisition to formal theories of inductive
learning is Wexler and Culicover (1980);\nocite{Wexler:1980} see also Cussens
and Pulman (2001).\nocite{Cussens:2001} We shall return to inductive
inference in Chapter~7.

As discussed in Chapter~2, grammarians before de Saussure and structuralism
did not hesitate to consider evidence from another source: dialectal and
historical variation of the language.  Under the influence of de Saussure,
such evidence has largely fallen into disfavor, since it is clear that people
can, and do, learn language without access to such data. The same can be
said of evidence from translation: though many language learners benefit from
learning two or more languages in early childhood, monolingual language
acquisition is obviously possible. Evidence from translation is generally
accepted in the form of {\it paraphrases}, a method we will discuss 
further in Chapter~6. 

\endinput

% Large-scale grammar writing is a timeconsuming process. 

substitution tests show that what we counted as complement 4 is not at all a
true complement to the verb but rather a free sentence adverbial {\it
adjunct:} such elements are called free because they can be attached to 
virtually every sentence as in {\it For a barbershop John was willing to 
suffer any indignity}

whether the semantic relation of this argument to the event described by the
verb is active (agent), passive (experiencer), or something 
undergoer/experiencer, or something more nebulous such as the theme. 





